# ""Start of script""
# --- Core Libraries ---
import sys
import os
import re
import pandas as pd
import numpy as np
import datetime # Ensure this import is present

# --- Plotting Libraries ---
import matplotlib
matplotlib.use('Agg') # FORCE Agg backend BEFORE importing pyplot
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
from matplotlib.ticker import FuncFormatter
import matplotlib.lines as mlines
import seaborn as sns

# --- Panel Model Imports --- #
from statsmodels.discrete.conditional_models import ConditionalLogit
import statsmodels.regression.mixed_linear_model as mlm
import statsmodels.genmod.generalized_estimating_equations as gee
import statsmodels.genmod.cov_struct as sm_cov_struct
from statsmodels.genmod.families import Binomial
from statsmodels.iolib.summary2 import summary_col
import statsmodels.api as sm # Already have this

# --- Machine Learning Libraries (sklearn) ---
from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_curve,
    silhouette_score, davies_bouldin_score, calinski_harabasz_score
)

# --- Other Libraries ---
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Additional style package for scientific plots
try:
    import scienceplots
    use_scienceplots = True # Flag to indicate if scienceplots is available
except ImportError:
    use_scienceplots = False
    print("SciencePlots not found, using fallback style.")

# Environment setup
os.environ['OMP_NUM_THREADS'] = '1'

# --- Generate Timestamp/Date and Create Output Directory ---
now = datetime.datetime.now()
timestamp_str = now.strftime("%Y%m%d_%H%M%S") # For filenames: YYYYMMDD_HHMMSS
date_str = now.strftime("%Y%m%d")             # For directory name: YYYYMMDD

# Define the output directory path (subdirectory named with date)
# os.getcwd() gets the current working directory where the script is run
output_dir_path = os.path.join(os.getcwd(), date_str)

# Create the directory if it doesn't exist
try:
    os.makedirs(output_dir_path, exist_ok=True) # exist_ok=True prevents error if dir exists
    print(f"Timestamp for output files: {timestamp_str}")
    print(f"Output directory: {output_dir_path}")
except OSError as error:
    print(f"Error creating output directory {output_dir_path}: {error}")
    # Decide if you want to exit or try saving in the current directory
    # output_dir_path = os.getcwd() # Fallback to current directory
    # print(f"Warning: Saving files to current directory instead: {output_dir_path}")
    sys.exit(f"Could not create output directory: {output_dir_path}") # Or exit
# --- End Timestamp/Directory Setup ---

# --- Set this at the beginning ---
desired_linewidth = 1.5 # Choose your desired thickness
plt.rcParams['axes.linewidth'] = desired_linewidth

# ==============================================================
# === Publication Style Guide Dictionary ===
# ==============================================================
PUBLICATION_NAMES = {
    # --- Variables (Raw & Engineered) ---
    "Age": "Firm Age (Years)",
    "Total assets th EUR": "Total Assets",
    "EBITDA margin": "EBITDA Margin (%)", # Assuming it's a percentage
    "ROA using Net income": "Return on Assets (%)",
    "Current ratio": "Current Ratio",
    "Total Leverage": "Total Leverage Ratio", # Be more specific if possible (e.g., Debt/Assets, Debt/Equity?)
    "Interest coverage": "Interest Coverage Ratio",
    "DebtToAssetsRatio": "Debt-to-Assets Ratio",
    "Log_Age_Plus_1": "Log(Firm Age + 1)",
    "Log_Total_Assets": "Log(Total Assets)", # Consider adding units if needed: "Log(Total Assets, th EUR)"

    # --- Target & Concepts ---
    "Zombie": "Zombie Status", # Or "Zombie Firm Indicator"
    "Zombie_Prop": "Zombie Proportion", # For legends/labels if needed
    "Zombie_Count": "Zombie Count", # For legends/labels if needed
    "Firm-Year": "Firm-Year Observation",

    # --- Sector Names ---
    "Mfg": "Manufacturing",
    "NonMfg": "Non-Manufacturing",
    "BvD sectors": "Industry Sector", # Or just "Industry"
    "Zombie_Mfg": "Manufacturing Zombie Firms", # FOR PART E.2 RESULTS 
    "Zombie_NonMfg": "Non-Manufacturing Zombie Firms", # FOR PART E.2 RESULTS 
    "Zombie_Full_Fallback": "Zombie Firms (Full Sample)", # FOR PART E.2 FALLBACK 

    # --- Clustering Algorithms ---
    "KMeans": "K-Means",
    "Hierarchical": "Agglomerative Clustering", # Or "Hierarchical Clustering"
    "DBSCAN": "DBSCAN",
    "DBSCAN (Best Eps)": "DBSCAN", # Keep simple for titles, explain 'Best Eps' in caption

    # --- Plotting Terms ---
    "Cluster": "Cluster",
    "Prop.": "Proportion",
    "Share": "Share",
    "Count": "Count",
    "PC1": "Principal Component 1",
    "PC2": "Principal Component 2",
    "t-SNE1": "t-SNE Dimension 1",
    "t-SNE2": "t-SNE Dimension 2",
    "ROC": "Receiver Operating Characteristic",
    "AUC": "Area Under Curve",

    # --- Subsets ---
    "All_Full_Fallback": "Full Sample", # Or just "Full Sample" if fallback is the main run
    "Zombie_Full_Fallback": "Zombie Firms (Full Sample)",
    "Zombie_Mfg": "Manufacturing Zombie Firms",
    "Zombie_NonMfg": "Non-Manufacturing Zombie Firms",

    # Add any other specific terms or abbreviations you use
}
print("Defined PUBLICATION_NAMES dictionary for consistent plot labeling.")
# ==============================================================
###############################################################################
# PART 0: MATPLOTLIB CONFIGURATION
###############################################################################
# Note: Backend ('Agg') is forced at the top before plt import.
print("Configuring Matplotlib...")

# --- Font & Text Settings ---
# Ensure LaTeX is definitely off to prevent errors
matplotlib.rcParams['text.usetex'] = False
matplotlib.rcParams['pgf.rcfonts'] = False # Ensure PGF doesn't try to handle fonts

# Tell Matplotlib to use Serif fonts and prefer CMU Serif if installed
matplotlib.rcParams['font.family'] = 'serif'
matplotlib.rcParams['font.serif'] = ['CMU Serif', 'DejaVu Serif', 'STIXGeneral', 'Times New Roman', 'serif']

# Force Matplotlib to use the standard hyphen for minus signs (avoids missing glyph issues)
matplotlib.rcParams['axes.unicode_minus'] = False

# --- Default Size Settings ---
matplotlib.rcParams['font.size'] = 9      # Default for text elements not otherwise specified
matplotlib.rcParams['axes.labelsize'] = 10 # Size for x and y labels
matplotlib.rcParams['xtick.labelsize'] = 8 # Size for x-axis tick labels
matplotlib.rcParams['ytick.labelsize'] = 8 # Size for y-axis tick labels
matplotlib.rcParams['legend.fontsize'] = 8 # Default legend font size

# --- TITLE SIZES --- <<< ADDED/UPDATED
matplotlib.rcParams['figure.titlesize'] = 14 # Larger size for main figure titles (set with fig.suptitle)
matplotlib.rcParams['axes.titlesize'] = 12   # Slightly smaller size for subplot titles (set with ax.set_title)

# --- Other Plotting Defaults (Optional but good practice) ---
matplotlib.rcParams['lines.linewidth'] = 1.0 # Default line width
matplotlib.rcParams['axes.grid'] = False     # Default grid state (can be overridden per plot)
matplotlib.rcParams['savefig.dpi'] = 600     # Default save resolution
matplotlib.rcParams['savefig.bbox'] = 'tight'# Default save bounding box

# --- Style Application ---
# Apply the chosen style sheet AFTER setting rcParams defaults
if use_scienceplots:
    # Apply science style, which might override some rcParams above,
    # so we re-assert critical ones afterwards.
    plt.style.use(['science'])
    print("Using SciencePlots style (requesting CMU Serif system font, no LaTeX).")
    # Re-ensure LaTeX is off and desired font settings persist after style application
    matplotlib.rcParams['text.usetex'] = False
    matplotlib.rcParams['pgf.rcfonts'] = False
    matplotlib.rcParams['axes.unicode_minus'] = False
    # SciencePlots might change font family, force it back if needed, or accept its choice
    # matplotlib.rcParams['font.family'] = 'serif' # Re-assert if style overrides
    # matplotlib.rcParams['font.serif'] = ['CMU Serif', 'DejaVu Serif', 'STIXGeneral', 'Times New Roman', 'serif'] # Re-assert if style overrides
    # Ensure title sizes stick (scienceplots might have its own defaults)
    matplotlib.rcParams['figure.titlesize'] = 14
    matplotlib.rcParams['axes.titlesize'] = 12
else:
    # Fallback style if scienceplots is not available
    plt.style.use('seaborn-v0_8-paper')
    # Update the fallback style with our specific size preferences
    plt.rcParams.update({
        "font.family": 'serif', # Ensure serif here too
        "font.serif": ['CMU Serif', 'DejaVu Serif', 'STIXGeneral', 'Times New Roman', 'serif'],
        "font.size": 9,
        "axes.labelsize": 10,
        "xtick.labelsize": 8,
        "ytick.labelsize": 8,
        "legend.fontsize": 8,
        "figure.titlesize": 14, # Add title sizes to fallback
        "axes.titlesize": 12,   # Add title sizes to fallback
        "axes.unicode_minus": False # Ensure minus sign fix applies here too
    })
    print("Using seaborn-v0_8-paper style (requesting CMU Serif system font).")

# --- Figure Size Configuration ---
textwidth = 3.31314 # Example width in inches (adjust as needed)
aspect_ratio = 6 / 8
width = textwidth
height = width * aspect_ratio
print(f"Matplotlib configuration complete. Base figure width: {width:.2f}in, height: {height:.2f}in")


###############################################################################
# PART A: LOAD & PREPARE DATA (REWORKED FOR PARQUET - Zombie Calc Added)
###############################################################################
print("\n=== Loading and Preparing Data (from Parquet) ===")
"""
1. Loads Parquet.
2. Reshapes variables using appropriate separators (space vs none).
3. Merges reshaped data.
4. Performs Feature Engineering (Age calculation, Log transforms, Zombie Status Calculation). # <<< MODIFIED
5. Defines final feature set.
6. Creates initial analysis DataFrames (df_analysis, X, y).
7. Applies Trimming.
8. Computes VIF.
"""
# --- Configuration ---
base_path = r'C:\Users\arthu\OneDrive - Università Commerciale Luigi Bocconi\Bocconi\4. Semester\Thesis\Data\5Combined Data Parquet'
file_name_base = 'Combined_Data_Filtered_AvgRevMicro_20250411_144402_StatusOVERWRITTEN_20250427_125825'
file_path = os.path.join(base_path, f"{file_name_base}.parquet")

years = list(range(2002, 2025)) # Years to keep after reshaping
zombie_age_threshold = 10 # <<< ADDED Zombie parameter

# --- Define base vars for reshaping based on separator type ---
# Variables with a space before the year (after cleaning \n to space)
base_vars_space_sep = [
    "EBITDA margin",
    "ROA using Net income",
    "Current ratio",
    "Total Leverage",
    "Total assets th EUR",
    "Interest coverage" # <<< ADDED Interest Coverage here
]
# Variables with NO space before the year
base_vars_no_sep = [
    "Age",
    "DebtToAssetsRatio"
    # "Zombie" # <<< REMOVED Zombie from reshape list
]
print(f"Variables to reshape with sep=' ': {base_vars_space_sep}")
print(f"Variables to reshape with sep='': {base_vars_no_sep}")

# --- Define other configuration variables ---
industry_col_name = "BvD sectors"
id_vars = ["Company name Latin alphabet", "VAT/Tax number"]
target_col = "Zombie" # Final name of the target column (will be CALCULATED)

# --- Define Sector Groupings ---
mfg_sectors = [
    "Printing & Publishing", "Chemicals, Petroleum, Rubber & Plastic",
    "Leather, Stone, Clay & Glass products", "Metals & Metal Products",
    "Industrial, Electric & Electronic Machinery", "Computer Hardware",
    "Transport Manufacturing", "Miscellaneous Manufacturing",
    "Biotechnology and Life Sciences", "Wood, Furniture & Paper Manufacturing",
    "Food & Tobacco Manufacturing", "Textiles & Clothing Manufacturing" # Added missing from initial list
]
nonmfg_sectors = [
    "Communications", "Wholesale", "Retail", "Transport, Freight & Storage",
    "Travel, Personal & Leisure", "Computer Software", "Media & Broadcasting",
    "Property Services", "Business Services", "Information Services",
    "Waste Management & Treatment", "Construction" # Added missing from initial list
    # Add any other sectors from your data explicitly here or handle as 'Other'
]
print(f"Defined {len(mfg_sectors)} Manufacturing sectors.")
print(f"Defined {len(nonmfg_sectors)} Non-Manufacturing sectors.")
# --- End Sector Groupings ---

# --- Define RAW feature columns names (as they appear AFTER reshaping) ---
# These are used as input for Feature Engineering below
raw_feature_col_names = [
     "Age", "EBITDA margin", "ROA using Net income",
     "Current ratio", "Total Leverage", "DebtToAssetsRatio", "Total assets th EUR",
     "Interest coverage" # <<< ADDED Interest Coverage
 ]
print(f"Raw features expected after reshape: {raw_feature_col_names}")

# FINAL feature_cols (for analysis) will be defined AFTER Feature Engineering block below
# --- End Configuration ---

# Read Parquet file
try:
    df = pd.read_parquet(file_path)
    print(f"Successfully loaded Parquet file: {file_path}")

    # Clean column names IMMEDIATELY after loading
    df.columns = df.columns.str.strip().str.replace('\n', ' ', regex=False) # Use regex=False for literal replace
    print("Cleaned column names (replaced newline with space).")

    # Check if ID variables exist
    missing_ids = [v for v in id_vars if v not in df.columns]
    if missing_ids:
        print(f"Error: id_vars {missing_ids} not found.")
        sys.exit()

    # Check for industry column
    if industry_col_name not in df.columns:
        print(f"Warning: Industry col '{industry_col_name}' not found.")
        industry_col_available = False
    else:
        industry_col_available = True
except FileNotFoundError:
    print(f"Error: Parquet file not found at {file_path}")
    sys.exit()
except ImportError:
    print("Error: Ensure 'pyarrow' or 'fastparquet' is installed.")
    sys.exit()
except Exception as e:
    print(f"Error reading Parquet file: {e}")
    sys.exit()

# --- Check for necessary columns BEFORE reshaping ---
# Function to check if stub exists with a given separator and year suffix
def check_stub(df_cols, stub, sep, suffix_pattern=r'\d{4}$'):
    # Use re from global import
    pattern = f"^{re.escape(stub)}{re.escape(sep)}{suffix_pattern}"
    if not sep: # sep=''
        pattern = f"^{re.escape(stub)}{suffix_pattern}"
    found = any(re.match(pattern, col) for col in df_cols)
    # print(f"Checking stub='{stub}', sep='{sep}', pattern='{pattern}' -> Found={found}") # Debug check
    return found

# Check stubs with space separator
print("\nChecking stubs with space separator (' ')...")
missing_stubs_space = [s for s in base_vars_space_sep if not check_stub(df.columns, s, ' ')]
if missing_stubs_space:
    print(f"Error: Missing base variables for reshaping with space separator: {missing_stubs_space}")
    # Debug: Print columns containing parts of the missing stubs
    for missing_stub in missing_stubs_space:
         print(f"Columns potentially related to '{missing_stub}':")
         print([col for col in df.columns if missing_stub in col])
    sys.exit()
else:
    print("Space-separated stubs found.")

# Check stubs with no separator
print("Checking stubs with no separator ('')...")
missing_stubs_no_sep = [s for s in base_vars_no_sep if not check_stub(df.columns, s, '')]
if missing_stubs_no_sep:
    print(f"Error: Missing base variables for reshaping with no separator: {missing_stubs_no_sep}")
    sys.exit()
else:
    print("No-separator stubs found.")

# Check for duplicates based on id_vars BEFORE reshaping
if df.duplicated(subset=id_vars).any():
    print(f"Error: Duplicate rows found based on id_vars {id_vars}.")
    sys.exit()

# --- Reshape variables with SPACE separator ---
try:
    print(f"\nReshaping variables with space separator: {base_vars_space_sep}")
    # Define pattern carefully using regex escapes and matching digits
    cols_space_sep_pattern = f"^({'|'.join([re.escape(s) for s in base_vars_space_sep])}) \\d{{4}}$" # Match exactly 4 digits for year
    cols_to_select_space = id_vars + [col for col in df.columns if re.match(cols_space_sep_pattern, col)]
    if industry_col_available and industry_col_name not in cols_to_select_space:
        cols_to_select_space.append(industry_col_name)
    #print("DEBUG: Columns selected for space sep reshape:", cols_to_select_space) # Added debug

    # Check if selection is empty
    if len(cols_to_select_space) == len(id_vars) + (1 if industry_col_available and industry_col_name not in id_vars else 0):
         print(f"Warning: No columns found matching the pattern for space-separated variables: {cols_space_sep_pattern}")
         # Decide how to handle: exit or create empty df? Creating empty for now.
         df_long_space = pd.DataFrame(columns = id_vars + ['year'] + base_vars_space_sep + ([industry_col_name] if industry_col_available else []))

    else:
        df_space_sep_wide = df[cols_to_select_space].copy()
        df_long_space = pd.wide_to_long(df_space_sep_wide,
                                        stubnames=base_vars_space_sep,
                                        i=id_vars,
                                        j='year',
                                        sep=' ',
                                        suffix='\\d+' # Keep suffix generic if years aren't always 4 digits, or use '\\d{4}'
                                        ).reset_index()
    print("Reshaping variables with space sep successful.")
    # print(df_long_space.head()) # Optional debug
except ValueError as ve:
    print(f"Error during wide_to_long for space-separated variables: {ve}")
    sys.exit()
except Exception as e:
   print(f"An unexpected error occurred during space-separator reshape: {e}")
   sys.exit()

# --- Reshape variables with NO separator ---
try:
    print(f"\nReshaping variables with no separator: {base_vars_no_sep}")
    # Select only needed columns (IDs + columns matching the stubs)
    cols_no_sep_pattern = f"^({'|'.join([re.escape(s) for s in base_vars_no_sep])})\\d{{4}}$" # Match exactly 4 digits
    cols_to_select_no_sep = id_vars + [col for col in df.columns if re.match(cols_no_sep_pattern, col)]

    if len(cols_to_select_no_sep) == len(id_vars):
         print(f"Warning: No columns found matching the pattern for no-separator variables: {cols_no_sep_pattern}")
         df_long_no_sep = pd.DataFrame(columns = id_vars + ['year'] + base_vars_no_sep)
    else:
        df_no_sep_wide = df[cols_to_select_no_sep].copy()
        df_long_no_sep = pd.wide_to_long(df_no_sep_wide,
                                       stubnames=base_vars_no_sep,
                                       i=id_vars,
                                       j='year',
                                       sep='',
                                       suffix='\\d+' # Or '\\d{4}'
                                       ).reset_index()
    print("Reshaping variables with no sep successful.")
    # print(df_long_no_sep.head()) # Optional debug

    # --- IMPORTANT: No Zombie column renaming needed here anymore ---
    # The Zombie column will be calculated later

except ValueError as ve:
    print(f"Error during wide_to_long for no-separator variables: {ve}")
    sys.exit()
except Exception as e:
   print(f"An unexpected error occurred during no-separator reshape: {e}")
   sys.exit()

# --- Merge the two reshaped DataFrames ---
print("\nMerging reshaped data...")
merge_on_cols = id_vars + ['year']

# Identify columns to merge from df_long_no_sep (IDs, year, and the reshaped vars)
# Only merge the columns that were actually reshaped (excluding Zombie now)
cols_to_merge_no_sep = merge_on_cols + [col for col in base_vars_no_sep if col in df_long_no_sep.columns]
cols_to_merge_no_sep = list(set(cols_to_merge_no_sep)) # Ensure unique columns

# --- NO check needed for target_col here anymore ---

# Check if all expected NON-ZOMBIE columns are present
expected_cols_no_sep_after_reshape = base_vars_no_sep # Zombie removed earlier
missing_after_reshape = [col for col in expected_cols_no_sep_after_reshape if col not in df_long_no_sep.columns]
if missing_after_reshape:
    print(f"Warning: Columns missing after no-sep reshape: {missing_after_reshape}")
    # Adjust columns to merge if some are missing
    cols_to_merge_no_sep = [col for col in cols_to_merge_no_sep if col not in missing_after_reshape]

# Perform merge only if both DataFrames have data
if df_long_space.empty or df_long_no_sep.empty:
    print("Warning: One or both reshaped DataFrames are empty before merge. Resulting df_long might be empty.")
    # Create an empty df_long with expected columns if merge is impossible
    all_expected_cols = id_vars + ['year'] + base_vars_space_sep + base_vars_no_sep
    if industry_col_available: all_expected_cols.append(industry_col_name)
    df_long = pd.DataFrame(columns=all_expected_cols)
else:
    df_long = pd.merge(df_long_space,
                       df_long_no_sep[cols_to_merge_no_sep],
                       on=merge_on_cols,
                       how='inner') # Use 'inner' to keep only company-years present in both

if df_long.empty:
   print("Error: Merging space-separated and no-separator data resulted in an empty DataFrame.")
   # sys.exit() # Decide whether to exit or continue with empty df
print(f"Merging complete. Resulting shape: {df_long.shape}")
print("DEBUG: Columns in df_long after merge:", df_long.columns.tolist()) # Added debug

# --- Merge industry column back (if needed) ---
# (This section remains the same, operating on the merged df_long)
if industry_col_available and industry_col_name not in df_long.columns:
    if 'df' in locals() and set(id_vars).issubset(df.columns) and industry_col_name in df.columns:
        print("Attempting to merge industry column from original wide data...")
        industry_data = df[id_vars + [industry_col_name]].drop_duplicates(subset=id_vars, keep='first')
        df_long = pd.merge(df_long, industry_data, on=id_vars, how='left', suffixes=('', '_drop'))
        # Drop duplicated columns if merge created suffixes
        df_long = df_long[[col for col in df_long.columns if not col.endswith('_drop')]]
        if industry_col_name not in df_long.columns:
            print("Warning: Failed to merge industry column.")
            industry_col_available = False
        else:
            print("Industry column merged successfully.")
            print("DEBUG: Columns in df_long after fallback merge:", df_long.columns.tolist()) # Added debug
    else:
        print(f"Warning: Cannot merge industry column (Original df missing, IDs missing, or '{industry_col_name}' missing in original data).")
        industry_col_available = False
elif industry_col_available and industry_col_name in df_long.columns:
   print(f"Note: '{industry_col_name}' already present after merging.")


# --- Convert year ---
# (Convert year column to numeric *before* FE needs it, but DON'T filter by analysis years yet)
if 'year' not in df_long.columns:
    print("Error: 'year' column not found in df_long after merging.")
    sys.exit()
df_long['year'] = pd.to_numeric(df_long['year'], errors='coerce')
df_long.dropna(subset=['year'], inplace=True) # Keep dropna for year conversion errors itself
df_long['year'] = df_long['year'].astype(int)
print("\nLong-format data sample after merging (BEFORE year filtering):\n", df_long.head())
# Note: rows_after_reshape_filter variable might not be needed or should be redefined later

# --- FIX: Remove duplicate columns from df_long BEFORE Feature Engineering ---
if df_long.columns.has_duplicates:
    print("\nWarning: Duplicate columns found in df_long after merging. Removing duplicates, keeping first instance.")
    original_cols = df_long.columns.tolist()
    df_long = df_long.loc[:, ~df_long.columns.duplicated(keep='first')]
    new_cols = df_long.columns.tolist()
    print(f"   df_long columns after removing duplicates: {new_cols}")
    if len(new_cols) < len(original_cols):
        print(f"   Removed {len(original_cols) - len(new_cols)} duplicate column instance(s).")
else:
    print("\nNo duplicate columns found in df_long after merging.")
# --- END FIX ---

# ============================================================================
# === FEATURE ENGINEERING BLOCK STARTS HERE ===
# (Operates on df_long containing ALL available years from reshape)
# ============================================================================
print("\n--- Feature Engineering ---")

# --- Convert necessary columns to numeric BEFORE calculations ---
print("Converting columns to numeric before FE calculations...")
cols_to_convert = ['Age', 'Total assets th EUR', 'Interest coverage'] # Add any others needed
for col in cols_to_convert:
    if col in df_long.columns:
        print(f"   Converting '{col}'...")
        df_long[col] = pd.to_numeric(df_long[col], errors='coerce')
    else:
        print(f"   Warning: Column '{col}' not found for numeric conversion.")

# --- Calculate Zombie Status (Post-Reshape) ---
print("\nCalculating Zombie status post-reshape...")
ic_col = 'Interest coverage'
age_col = 'Age'
required_zombie_cols = [ic_col, age_col]
prereqs_met = True
for req_col in required_zombie_cols:
    if req_col not in df_long.columns:
        print(f"   ERROR: Required column '{req_col}' for Zombie calculation is missing.")
        prereqs_met = False
    elif not pd.api.types.is_numeric_dtype(df_long[req_col].dtype):
       print(f"   Warning: Column '{req_col}' is not numeric ({df_long[req_col].dtype}). Zombie calc may fail.")
    elif df_long[req_col].isna().all():
       print(f"   Warning: Column '{req_col}' contains only NaN values.")

if not prereqs_met:
    print("   Skipping Zombie calculation due to missing prerequisite columns.")
    df_long[target_col] = np.nan
else:
    print(f"   Calculating lagged '{ic_col}'...")
    df_long.sort_values(id_vars + ['year'], inplace=True) # Sort for lagging
    df_long['ic_lag1'] = df_long.groupby(id_vars)[ic_col].shift(1)
    df_long['ic_lag2'] = df_long.groupby(id_vars)[ic_col].shift(2)
    print(f"   Applying Zombie definition (Age >= {zombie_age_threshold}, IC < 1 for 3 years)...")
    cond_ic_y  = df_long[ic_col] < 1
    cond_ic_y1 = df_long['ic_lag1'] < 1
    cond_ic_y2 = df_long['ic_lag2'] < 1
    cond_age   = df_long[age_col] >= zombie_age_threshold
    is_any_input_nan = df_long[[age_col, ic_col, 'ic_lag1', 'ic_lag2']].isna().any(axis=1)
    is_zombie_candidate = cond_ic_y & cond_ic_y1 & cond_ic_y2 & cond_age
    df_long[target_col] = np.select(
        condlist=[is_any_input_nan, is_zombie_candidate],
        choicelist=[np.nan, 1],
        default=0
    )
    df_long[target_col] = df_long[target_col].astype(float)
    print(f"   Calculated '{target_col}' column post-reshape.")
    print(f"   Value counts for '{target_col}':\n{df_long[target_col].value_counts(dropna=False)}")
    try:
        df_long.drop(columns=['ic_lag1', 'ic_lag2'], inplace=True)
        print("   Dropped temporary lag columns.")
    except KeyError:
        print("   Lag columns not found to drop (might have failed creation).")
# --- End of Zombie Calculation ---

# --- Log Transform Total Assets ---
asset_col_name = 'Total assets th EUR'
if asset_col_name in df_long.columns:
    print(f"\nProcessing '{asset_col_name}' for logging...")
    df_long.replace([np.inf, -np.inf], np.nan, inplace=True)
    non_positive_assets = df_long[asset_col_name] <= 0
    if non_positive_assets.any():
        print(f"Warning: Found {non_positive_assets.sum()} non-positive '{asset_col_name}' values. Setting them to NaN before logging.")
        df_long.loc[non_positive_assets, asset_col_name] = np.nan
    df_long['Log_Total_Assets'] = np.log(df_long[asset_col_name])
    print("Created 'Log_Total_Assets'.")
else:
    print(f"\nWarning: '{asset_col_name}' column not found for logging.")

# --- Log Transform Age ---
if age_col in df_long.columns:
    print("\nProcessing 'Age' for logging...")
    non_negative_age = df_long[age_col] >= 0
    if (~non_negative_age).any():
        print(f"Warning: Found {(~non_negative_age).sum()} negative 'Age' values. Setting them to NaN before logging.")
        df_long.loc[~non_negative_age, age_col] = np.nan
    df_long['Log_Age_Plus_1'] = np.log1p(df_long[age_col])
    df_long['Log_Age_Plus_1'] = df_long['Log_Age_Plus_1'].replace([np.inf, -np.inf], np.nan)
    print("Created 'Log_Age_Plus_1'.")
else:
    print("\nWarning: 'Age' column not found for logging.")

# --- NO LONGER DROPPING ROWS FROM df_long based on FE NaNs ---
print("\nChecking for NaNs in engineered columns (rows will NOT be dropped from df_long)...")
fe_cols_to_check = []
if 'Log_Total_Assets' in df_long.columns: fe_cols_to_check.append('Log_Total_Assets')
if 'Log_Age_Plus_1' in df_long.columns: fe_cols_to_check.append('Log_Age_Plus_1')
if fe_cols_to_check:
    for col_check in fe_cols_to_check:
         if col_check in df_long.columns: print(f"   NaNs in '{col_check}': {df_long[col_check].isna().sum()}")
         else: print(f"   Column '{col_check}' not found for NaN check.")
    potential_rows_to_drop = df_long[fe_cols_to_check].isna().any(axis=1).sum()
    if potential_rows_to_drop > 0: print(f"(Info: {potential_rows_to_drop} rows have NaNs in these specific columns)")
else: print("   No specific engineered columns identified to check for NaNs.")
print("Filtering for analysis based on final features will occur when creating df_analysis.")

# --- Debug Checks ---
if target_col not in df_long.columns: print(f"DEBUG: Target column '{target_col}' MISSING after FE")
if 'Log_Age_Plus_1' not in df_long.columns: print("DEBUG: 'Log_Age_Plus_1' MISSING")
if 'Log_Total_Assets' not in df_long.columns: print("DEBUG: 'Log_Total_Assets' MISSING")
# --- End Debug ---

# --- End Feature Engineering ---
# ============================================================================
# === FEATURE ENGINEERING BLOCK ENDS HERE ===
# ============================================================================


# ============================================================================
# === DEFINE FINAL feature_cols LIST HERE ===
# ============================================================================
feature_cols = [
    "Log_Age_Plus_1", "Log_Total_Assets", "EBITDA margin",
    "ROA using Net income", "Current ratio", "Total Leverage",
    "DebtToAssetsRatio", "Interest coverage"
]
existing_cols_in_df = df_long.columns.tolist()
feature_cols = [col for col in feature_cols if col in existing_cols_in_df]
feature_cols = sorted(list(set(feature_cols))) # Ensure unique

if target_col not in existing_cols_in_df:
    print(f"CRITICAL ERROR: Target column '{target_col}' is missing from df_long before year filtering.")
    sys.exit()

print(f"\nFinal feature_cols defined (unique & existing): {feature_cols}")


# ============================================================================
# === Filter df_long for Analysis Years === <<< MOVED HERE >>>
# ============================================================================
print(f"\nFiltering df_long to keep only analysis years: {years}...")
initial_rows_before_year_filter = len(df_long)
df_long = df_long[df_long['year'].isin(years)] # Apply year filter HERE
rows_after_year_filter = len(df_long)
print(f"   Rows remaining after year filter: {rows_after_year_filter}/{initial_rows_before_year_filter}")

if df_long.empty:
    print(f"Error: df_long is empty after filtering for years {years}. Cannot proceed.")
    sys.exit()
# ============================================================================


# ============================================================================
# === Create df_analysis / X / y ===
# ============================================================================
print("\nCreating df_analysis, X, y by filtering for non-missing required columns...")
# Ensure all feature columns and the target column exist *in the year-filtered df_long*
missing_analysis_cols = [c for c in feature_cols + [target_col] if c not in df_long.columns]
if missing_analysis_cols:
    print(f"Error: Required columns for analysis missing after year filter: {missing_analysis_cols}")
    sys.exit()

# --- Convert AGAIN just before masking to be absolutely sure ---
print("   Converting analysis columns to numeric before masking...")
X_temp = df_long[feature_cols].apply(pd.to_numeric, errors='coerce')
y_temp = pd.to_numeric(df_long[target_col], errors='coerce')

if X_temp.isnull().all().all() or y_temp.isnull().all():
   print("Error: All values in features or target became NaN after conversion for masking.")
   sys.exit()

# Create mask based on non-NaN in final features AND target
print("   Creating mask based on non-NaN features and target...")
mask = X_temp.notna().all(axis=1) & y_temp.notna()
num_passing_mask = mask.sum()
print(f"   Number of rows passing the non-NaN mask: {num_passing_mask}")

if num_passing_mask == 0:
    print("Error: No rows have complete data. Cannot create df_analysis.")
    sys.exit()

# --- Define cols_to_keep carefully to avoid duplicates ---
# --- Include industry column IF available ---
cols_to_keep = id_vars + ['year'] + feature_cols + [target_col]
if industry_col_available and industry_col_name in df_long.columns: # Check df_long now
     cols_to_keep.append(industry_col_name)
# --- Add optional raw columns ---
optional_raw_cols = {}
if 'Age' in df_long.columns and 'Age' not in feature_cols: optional_raw_cols['Age'] = None
if 'Total assets th EUR' in df_long.columns and 'Total assets th EUR' not in feature_cols: optional_raw_cols['Total assets th EUR'] = None
if 'Interest coverage' in df_long.columns and 'Interest coverage' not in feature_cols: optional_raw_cols['Interest coverage'] = None
cols_to_keep.extend(optional_raw_cols.keys())
cols_to_keep = sorted(list(set(cols_to_keep))) # Ensure unique and sorted
# --- Ensure all selected columns actually exist in df_long ---
existing_cols_to_keep = [col for col in cols_to_keep if col in df_long.columns]
existing_cols_to_keep = sorted(list(set(existing_cols_to_keep))) # Final sort
print(f"   Final unique columns selected for df_analysis: {existing_cols_to_keep}")
# --- END FIX for cols_to_keep ---


# Create df_analysis using the unique list of columns that exist
df_analysis = df_long.loc[mask, existing_cols_to_keep].copy()

# <<< --- START: ADD firm_id CREATION HERE --- >>>
print("\nAttempting to add 'firm_id' to df_analysis...")
if all(v in df_analysis.columns for v in id_vars): # Check if id_vars are in df_analysis
    try:
        # Create firm_id by combining the specified ID variables
        df_analysis['firm_id'] = df_analysis[id_vars[0]].astype(str) + '_' + df_analysis[id_vars[1]].astype(str)
        if 'firm_id' in df_analysis.columns:
             print("Successfully created 'firm_id' column in df_analysis.")
        else:
             # This case is unlikely if the line above doesn't raise an error, but good to check
             print("Warning: 'firm_id' creation failed silently in df_analysis.")
    except Exception as e_fid:
        print(f"Warning: Could not create 'firm_id' in df_analysis: {e_fid}")
else:
    # This error message is crucial if the required id_vars were somehow lost
    print(f"CRITICAL WARNING: Cannot create 'firm_id' in df_analysis. Required id_vars ({id_vars}) not found in its columns.")
    print(" -> Check the 'existing_cols_to_keep' definition and filtering steps.")
# <<< --- END: ADD firm_id CREATION HERE --- >>>

# --- Add check for duplicates immediately after creation ---
if df_analysis.columns.has_duplicates:
    print("ERROR TRACE: Duplicates STILL exist in df_analysis immediately after creation! Applying fix.")
    df_analysis = df_analysis.loc[:, ~df_analysis.columns.duplicated(keep='first')]
    feature_cols = [col for col in feature_cols if col in df_analysis.columns] # Re-filter feature_cols
    print(f"   df_analysis columns after MANUAL duplicate removal: {df_analysis.columns.tolist()}")
    print(f"   feature_cols possibly updated: {feature_cols}")
# --- End check ---

# --- Define X and y initially ---
if not df_analysis.empty:
    feature_cols = [col for col in feature_cols if col in df_analysis.columns] # Ensure feature_cols are still valid
    X = df_analysis[feature_cols].copy()
    y = df_analysis[target_col].copy()
    print(f"Successfully created de-duplicated df_analysis ({len(df_analysis)} rows), X, and y.")
    if X.columns.has_duplicates: print("FATAL ERROR: Duplicates STILL exist in X!")
else:
    print("Warning: df_analysis is empty after applying the non-NaN mask.")
    X = pd.DataFrame(columns=feature_cols)
    y = pd.Series(dtype=float, name=target_col)

# Report counts AFTER filtering into df_analysis
# Use rows_after_year_filter as the starting point for this filtering step
initial_rows_for_masking = rows_after_year_filter
rows_after_masking = len(df_analysis)
nan_dropped_filter = initial_rows_for_masking - rows_after_masking
print("\nData Filtering Summary:")
# print(f"   Started with {initial_rows_before_year_filter} rows after reshape/merge.") # Maybe remove this intermediate count
print(f"   Started with {initial_rows_for_masking} rows after filtering for years {years}.")
print(f"   Filtered out {nan_dropped_filter} rows due to NaNs in required features/target ({feature_cols + [target_col]}) when creating analysis subset.")
print(f"   Kept {rows_after_masking}/{initial_rows_for_masking} rows for initial df_analysis, X, y.")

# --- End of section creating initial df_analysis, X, y ---

# --- Rigorous Final Numeric Conversion and Cleaning ---
# This ensures the selected features in X and the target y are truly numeric
# before trimming/modeling, handling any issues from logs/ratios/coercion.

final_rows = 0 # Initialize final row count
if not X.empty:
    print("\nRunning final cleanup on selected features before Trimming...")
    print("X dtypes before final conversion:\n", X.dtypes)
    # Convert selected feature columns in X again robustly
    # Use final feature_cols list defined previously
    X = X[feature_cols].apply(pd.to_numeric, errors='coerce')
    # Replace inf/-inf which might result from calculations (e.g., logs, ratios)
    X.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Drop rows with NaNs introduced during final conversion/inf replacement IN FEATURES
    rows_before_final_drop = len(X)
    na_mask_X = X.notna().all(axis=1)
    X = X[na_mask_X]

    if len(X) < rows_before_final_drop:
        print(f"Note: Dropped {rows_before_final_drop - len(X)} rows with non-numeric/inf values detected in final features.")

    # Realign y and df_analysis to match the cleaned X index
    if not X.empty:
        # Ensure indices are alignable
        common_index = X.index.intersection(y.index).intersection(df_analysis.index)
        if len(common_index) < len(X.index):
             print("Warning: Index mismatch during final X cleanup realignment.")
        X = X.loc[common_index] # Keep only rows present in all three
        y = y.loc[common_index]
        # Ensure df_analysis keeps only relevant rows AND potentially the industry column
        df_analysis = df_analysis.loc[common_index].copy()
    else:
        print("Warning: X became empty during final feature cleanup.")
        y = pd.Series(dtype=float, name=target_col)
        # Ensure df_analysis schema matches expectation even if empty
        df_analysis = pd.DataFrame(columns=existing_cols_to_keep) # Use list from earlier


    print(f"Dataset size after final feature cleanup: {len(X)} rows.")
    if not X.empty:
        print("\nX dtypes after final conversion:\n", X.dtypes)

    # Final check and cleanup for y (target variable)
    if not y.empty:
        # Check if y is already numeric
        if not pd.api.types.is_numeric_dtype(y.dtype):
             print(f"Warning: Target variable '{target_col}' is not numeric. Attempting conversion...")
             rows_before_y_drop = len(y)
             y_numeric = pd.to_numeric(y, errors='coerce')
             na_mask_y = y_numeric.notna()
             y = y_numeric[na_mask_y] # Keep only non-NaN numeric values

             if y.empty or not pd.api.types.is_numeric_dtype(y.dtype):
                 print(f"Error: Could not convert target '{target_col}' to numeric or it became empty.")
                 X = pd.DataFrame(columns=feature_cols)
                 df_analysis = pd.DataFrame(columns=existing_cols_to_keep)
             elif len(y) < rows_before_y_drop:
                 print(f"Note: Dropped {rows_before_y_drop - len(y)} rows due to non-numeric/NaN target.")
                 # Realign X and df_analysis again
                 if not y.empty:
                      common_index_y = X.index.intersection(y.index).intersection(df_analysis.index) #<<< Added df_analysis index
                      if len(common_index_y) < len(y.index):
                           print("Warning: Index mismatch after target cleanup.")
                      y = y.loc[common_index_y]
                      X = X.loc[common_index_y]
                      df_analysis = df_analysis.loc[common_index_y].copy()
                 else:
                      print("Warning: Target variable y became empty after cleanup.")
                      X = pd.DataFrame(columns=feature_cols)
                      df_analysis = pd.DataFrame(columns=existing_cols_to_keep)
        # Ensure y is integer type if it's the target 'Zombie'
        if target_col == "Zombie" and not y.empty and pd.api.types.is_numeric_dtype(y.dtype):
             y = y.astype(int)

        print(f"Dataset size after final y cleanup: {len(X)} rows.")

    final_rows = len(X) # Final count after all cleaning *before* trimming

else: # Corresponds to 'if not X.empty:' at the start of this cleanup block
    print("Skipping final cleanup and trimming because X was empty after initial NaN drop.")
    final_rows = 0

# --- Trimming Outliers (0.5%/99.5%) --- # Corrected comment
# Applied to the cleaned X based on the final feature_cols

if not X.empty and not y.empty and not df_analysis.empty:
    print("\nApplying Trimming (0.5%/99.5% percentile) to final feature columns...") # Corrected comment
    initial_rows_before_trim = len(X)
    print(f"Dataset size before trimming: {initial_rows_before_trim} rows.")

    # Use the final feature_cols list defined after FE
    cols_to_trim = feature_cols
    print(f"Trimming based on columns: {cols_to_trim}")

    rows_to_keep_mask = pd.Series(True, index=X.index)

    for col in cols_to_trim:
        if col in X.columns and pd.api.types.is_numeric_dtype(X[col]):
             if X[col].nunique(dropna=True) > 1: # Check for variance ignoring NaNs
                 q_low = X[col].quantile(0.005)
                 q_high = X[col].quantile(0.995)
                 print(f"  Column '{col}': 0.5%ile={q_low:.4f}, 99.5%ile={q_high:.4f}")

                 # Keep rows within bounds OR if the value is NaN (shouldn't be NaNs here, but safer)
                 rows_to_keep_mask &= (X[col].isna() | ((X[col] >= q_low) & (X[col] <= q_high)))

                 if q_low == q_high:
                      print(f"  Warning: 0.5% and 99.5% percentile are the same for column '{col}'. Check data variance.") # Corrected comment
             else:
                 print(f"  Skipping trimming for column '{col}': Column has no variance (or only NaNs).")
        else:
             print(f"  Skipping trimming for column '{col}' (not found or not numeric).")

    # Apply the combined mask
    X_trimmed = X[rows_to_keep_mask]
    y_trimmed = y[rows_to_keep_mask]
    # --- Ensure df_analysis is also trimmed using the SAME mask ---
    df_analysis_trimmed = df_analysis[rows_to_keep_mask].copy() # Mask applied directly, use copy

    final_rows_after_trim = len(X_trimmed)
    rows_trimmed_count = initial_rows_before_trim - final_rows_after_trim

    print(f"\nTrimming complete. Removed {rows_trimmed_count} rows.")
    print(f"Dataset size after trimming: {final_rows_after_trim} rows.")

    # Overwrite X, y, df_analysis with trimmed versions
    X = X_trimmed
    y = y_trimmed
    df_analysis = df_analysis_trimmed # <<< ENSURE df_analysis is updated
    final_rows = final_rows_after_trim # UPDATE final_rows count again

    if X.empty: # Check if trimming removed all data
        print("Error: Trimming removed all data.")
        # sys.exit() # Optional: exit if needed

else:
    print("\nSkipping Trimming (X, y, or df_analysis was empty before trimming).")
# --- End of Trimming Block ---

# --- Prepare for VIF and Statsmodels ---
if not X.empty:
    # Ensure X is purely numeric before adding constant
    if X.select_dtypes(include=np.number).shape[1] == X.shape[1]:
        try:
            X_const = sm.add_constant(X, has_constant='add') # Use trimmed X
            print("\nFinal check of X_const dtypes (Before VIF on trimmed data):\n", X_const.dtypes)
        except Exception as e_const:
            print(f"Error adding constant to trimmed X: {e_const}")
            X_const = pd.DataFrame()
    else:
        print("Error: Non-numeric columns detected in trimmed X before adding constant.")
        X_const = pd.DataFrame()
else:
    X_const = pd.DataFrame(columns=['const'] + feature_cols) # Use final feature_cols
    print("Warning: X is empty after trimming.")

# Check row count AFTER trimming
if final_rows == 0:
    print("Error: No valid data remaining after cleaning and trimming.")

# --- Calculate Variance Inflation Factors (VIF) ---
# On the final, trimmed data
if 'X_const' in locals() and not X_const.empty and 'const' in X_const.columns and X_const.shape[0] > X_const.shape[1]:
    print("\nCalculating VIF (on trimmed data)...")
    try:
        numeric_X_const = X_const.select_dtypes(include=np.number)
        if numeric_X_const.shape[1] == X_const.shape[1]:
             if not numeric_X_const.isnull().values.any() and not np.isinf(numeric_X_const.values).any():
                 vif_data = pd.DataFrame({
                     'feature': numeric_X_const.columns,
                     'VIF': [variance_inflation_factor(numeric_X_const.values, i) for i in range(numeric_X_const.shape[1])]
                 })
                 print("\nVIF:\n", vif_data)
             else:
                 print("Error calculating VIF: Trimmed X_const contains NaNs or Infs.")
        else:
             print("Error calculating VIF: Trimmed X_const contains non-numeric columns.")
             print(X_const.dtypes)
    except Exception as e:
        print(f"Could not calculate VIF: {e}")
else:
    print("\nSkipping VIF calculation (not enough data or invalid X_const after trimming).")

# --- End of Rigorous Cleaning / Trimming / VIF Section ---
# --- End of Part A ---

# <<< --- START: ADDED BLOCK FOR STATIC INFO MERGE (ExitStatus) --- >>>
print("\n--- Merging Static Company Info (Exit Status) into df_analysis (End of Part A) ---")

# <<< Initialize df_static_info_for_merge definitively in this scope >>>
# This DataFrame will hold UNIQUE company info for lookups later (e.g., in summary function)
df_static_info_for_merge = pd.DataFrame()
exit_status_merged = False # Flag to track if merge into df_analysis happened
STATUS_COL_SOURCE = "Status" # Column name for status in original wide 'df'
# STATUS_DATE_COL_SOURCE = "Status date" # Uncomment if needed elsewhere
ID_VARS_STATIC = id_vars             # Use same ID vars as defined earlier

# Check if prerequisite DataFrames exist
prereqs_met_static = (
    'df' in locals() and isinstance(df, pd.DataFrame) and not df.empty and
    'df_analysis' in locals() and isinstance(df_analysis, pd.DataFrame) and not df_analysis.empty
)

if prereqs_met_static:
    print("  Prerequisite DataFrames ('df', 'df_analysis') found.")
    # Check if essential source columns (IDs, Status) exist in the original wide df
    required_src_cols_for_merge = ID_VARS_STATIC + [STATUS_COL_SOURCE]
    missing_src_cols = [col for col in required_src_cols_for_merge if col not in df.columns]

    if missing_src_cols:
        print(f"  WARNING: Required source columns missing from original 'df' for static info ({missing_src_cols}). Cannot extract/merge ExitStatus reliably.")
        # df_static_info_for_merge remains empty as initialized above
    else:
        print("  Required source columns found in 'df'. Attempting static info extraction.")
        try:
            # --- Extract and prepare static info (Unique Companies) ---
            # This creates the DataFrame needed for the cluster summary helper function
            df_static_extract = df[required_src_cols_for_merge].drop_duplicates(subset=ID_VARS_STATIC, keep='first').copy()
            df_static_extract.rename(columns={STATUS_COL_SOURCE: "ExitStatus"}, inplace=True)
            print(f"  Extracted static info (ExitStatus) for {len(df_static_extract)} unique companies.")

            # <<< Assign to the main variable ONLY on success >>>
            # Select only needed columns for the static lookup table
            df_static_info_for_merge = df_static_extract[[col for col in ID_VARS_STATIC + ['ExitStatus'] if col in df_static_extract.columns]].copy()

            # --- Merge ExitStatus into the main df_analysis (Firm-Year Level) ---
            # This adds the column to the main analysis df for potential direct use
            print("  Attempting to merge 'ExitStatus' into 'df_analysis'...")
            df_to_merge = df_static_info_for_merge # Use the prepared static df
            initial_rows_analysis = len(df_analysis)
            original_analysis_index = df_analysis.index.copy()
            # Reset index for merge, keeping original index name
            original_index_name = df_analysis.index.name if df_analysis.index.name else 'index'
            df_analysis_reset = df_analysis.reset_index()

            # Use outer merge to keep all analysis rows, even if no static match
            df_merged = pd.merge(df_analysis_reset, df_to_merge, on=ID_VARS_STATIC, how='left', suffixes=('', '_from_static')) # Add suffixes defensively

            # Drop any duplicated columns potentially created by merge suffixes (if IDs were somehow already there)
            cols_to_drop_merge = [col for col in df_merged.columns if col.endswith('_from_static')]
            if cols_to_drop_merge:
                print(f"  Dropping merge suffix columns: {cols_to_drop_merge}")
                df_merged.drop(columns=cols_to_drop_merge, inplace=True)

            # Restore index if possible
            if len(df_merged) != initial_rows_analysis:
                 print("  WARNING: Row count changed unexpectedly after merging ExitStatus. Using merged result as is (index order may change).")
                 df_analysis = df_merged.copy() # Accept the merge result with potentially new index order
            else:
                 try:
                     index_name_to_use = original_index_name # Use stored name
                     if index_name_to_use in df_merged.columns:
                        df_merged = df_merged.set_index(index_name_to_use)
                        # Ensure all original indices are present before reindexing
                        missing_indices = original_analysis_index.difference(df_merged.index)
                        if not missing_indices.empty:
                            print(f"  WARNING: {len(missing_indices)} original indices missing after merge. Re-aligning...")
                            # Reindex to ensure all original rows are present, fill missing statuses later
                            df_analysis = df_merged.reindex(original_analysis_index)
                        else:
                            df_analysis = df_merged.loc[original_analysis_index]
                            print("  Successfully restored original index order after merge.")
                     else:
                        print(f"  WARNING: Original index column '{index_name_to_use}' not found after reset/merge. Using merged result as is.")
                        df_analysis = df_merged.copy() # Fallback
                 except Exception as e_setidx:
                     print(f"  WARNING: Could not restore index after merge ({e_setidx}). Using merged result as is.")
                     df_analysis = df_merged.copy()

            # Ensure ExitStatus column exists and fill NaNs in df_analysis
            if 'ExitStatus' not in df_analysis.columns:
                 print("  INFO: 'ExitStatus' column not present after merge, adding with 'Unknown'.")
                 df_analysis['ExitStatus'] = 'Unknown'
            df_analysis['ExitStatus'] = df_analysis['ExitStatus'].fillna('Unknown').astype(str)
            print(f"  ExitStatus column processed in df_analysis. Non-unknown count: {(df_analysis['ExitStatus'] != 'Unknown').sum()}")
            exit_status_merged = True # Mark merge attempt as done

        except Exception as e_status_prep_merge:
            print(f"  ERROR processing or merging static info: {e_status_prep_merge}")
            import traceback
            print(traceback.format_exc()) # Print full traceback for debugging
            # df_static_info_for_merge remains as initialized (likely empty)
            # Ensure ExitStatus exists in df_analysis even on error
            if 'ExitStatus' not in df_analysis.columns:
                 df_analysis['ExitStatus'] = 'Unknown'
                 df_analysis['ExitStatus'] = df_analysis['ExitStatus'].astype(str)

else: # Prereqs not met
     print("  Skipping ExitStatus extraction/merge: Original 'df' or 'df_analysis' not available or empty.")
     # df_static_info_for_merge remains empty as initialized above
     # Ensure ExitStatus column exists in df_analysis if df_analysis itself exists
     if 'df_analysis' in locals() and isinstance(df_analysis, pd.DataFrame) and not df_analysis.empty and 'ExitStatus' not in df_analysis.columns:
          df_analysis['ExitStatus'] = 'Unknown'
          df_analysis['ExitStatus'] = df_analysis['ExitStatus'].astype(str)


# --- Final Check on df_static_info_for_merge before Part E ---
print(f"\nStatus of df_static_info_for_merge before Part E:")
if isinstance(df_static_info_for_merge, pd.DataFrame) and not df_static_info_for_merge.empty:
    print(f"  Shape: {df_static_info_for_merge.shape}")
    print(f"  Columns: {df_static_info_for_merge.columns.tolist()}")
    print(f"  Sample Head:\n{df_static_info_for_merge.head(2)}")
elif isinstance(df_static_info_for_merge, pd.DataFrame):
    print("  DataFrame exists but is EMPTY. Check warnings above for reasons (e.g., missing source columns, errors).")
else:
    # This case should be less likely now but is a critical error indicator
    print(f"  CRITICAL WARNING: Variable 'df_static_info_for_merge' is NOT a DataFrame (Type: {type(df_static_info_for_merge)}). Check initialization and error handling above.")

# <<< --- END: REVISED BLOCK FOR STATIC INFO MERGE --- >>>

# --- Add Manufacturing Dummy Variable --- # <<< INSERT BEFORE THIS LINE >>>

# --- Add Manufacturing Dummy Variable ---
print("\nAdding 'Is_Manufacturing' dummy variable to df_analysis...")
if 'df_analysis' in locals() and not df_analysis.empty:
    if industry_col_available and industry_col_name in df_analysis.columns:
        # Define a function for clarity
        def assign_mfg_dummy(sector):
            if sector in mfg_sectors:
                return 1
            elif sector in nonmfg_sectors:
                return 0
            else: # Treat 'Other' or unexpected sectors as NonMfg (0) for the dummy
                return 0

        df_analysis['Is_Manufacturing'] = df_analysis[industry_col_name].apply(assign_mfg_dummy)
        # Ensure it's integer type after creation
        df_analysis['Is_Manufacturing'] = df_analysis['Is_Manufacturing'].astype(int)

        print("Successfully added 'Is_Manufacturing' column.")
        print("Value counts for 'Is_Manufacturing':\n", df_analysis['Is_Manufacturing'].value_counts())
    else:
        print(f"Warning: Cannot add 'Is_Manufacturing' dummy. Industry column '{industry_col_name}' not available or missing in df_analysis.")
        # Add a column of NaNs or zeros if you need the column structure regardless
        df_analysis['Is_Manufacturing'] = np.nan # Or 0 if preferred default
else:
    print("Skipping 'Is_Manufacturing' dummy creation (df_analysis is empty).")
# --- End Manufacturing Dummy ---

print("\n--- DEBUG: Industry Column Check in df_analysis AFTER TRIMMING ---")
if not df_analysis.empty:
    if industry_col_name in df_analysis.columns:
        print(f"DEBUG: '{industry_col_name}' IS present in df_analysis AFTER trimming.")
        print(f"  Non-NaN count: {df_analysis[industry_col_name].notna().sum()} / {len(df_analysis)} rows")
    else:
        print(f"DEBUG: '{industry_col_name}' IS MISSING from df_analysis AFTER trimming.")
else:
    print("DEBUG: df_analysis is empty AFTER trimming.")
print("--- END DEBUG ---")


###############################################################################
# PART A2: EXPLORATORY ANALYSIS (Stats based on Trimmed data)
###############################################################################
print("\n=== Exploratory Analysis: Summary Statistics ===")
# Uses df_analysis which has been realigned and potentially trimmed based on X.
# The descriptive stats calculated will reflect trimmed values. 
if 'df_analysis' in locals() and not df_analysis.empty and final_rows > 0:
    numeric_cols_desc = [col for col in feature_cols if col in df_analysis.columns] # Ensure columns exist
    target_col_exists = target_col in df_analysis.columns

    if numeric_cols_desc and target_col_exists:
        try:
            print("Checking dtypes in df_analysis for describe:\n", df_analysis[numeric_cols_desc + [target_col]].dtypes)
            # df_analysis_desc = df_analysis.loc[X.index].copy() # Re-align might not be needed if X index hasn't changed further
            df_analysis_desc = df_analysis.copy()

            # Ensure cols are numeric for describe (should be, but safeguard)
            df_analysis_desc[numeric_cols_desc] = df_analysis_desc[numeric_cols_desc].apply(pd.to_numeric, errors='coerce')
            df_analysis_desc[target_col] = pd.to_numeric(df_analysis_desc[target_col], errors='coerce')

            # Drop rows if coercion failed for describe step
            rows_before_desc_drop = len(df_analysis_desc)
            df_analysis_desc.dropna(subset=numeric_cols_desc + [target_col], inplace=True)
            if len(df_analysis_desc) < rows_before_desc_drop:
                 print(f"Note: Dropped {rows_before_desc_drop - len(df_analysis_desc)} rows during describe cleanup due to non-numeric coercion.")


            # <<< --- START OF REPLACEMENT (for granular sector/zombie stats) --- >>>
            if not df_analysis_desc.empty:
                # Describe using the cleaned-up subset for ALL firms
                desc_all = df_analysis_desc[numeric_cols_desc].describe(percentiles=[0.5])
                print("\nCalculated overall summary statistics.")

                # --- MODIFICATION: Describe by Sector AND Zombie Status ---
                desc_mfg_non_zombie = pd.DataFrame()
                desc_mfg_zombie = pd.DataFrame()
                desc_nonmfg_non_zombie = pd.DataFrame()
                desc_nonmfg_zombie = pd.DataFrame()
                summary_stats = pd.DataFrame() # Initialize final summary

                # Check if prerequisites for splitting are met
                industry_col_in_desc = industry_col_name in df_analysis_desc.columns
                target_col_in_desc = target_col in df_analysis_desc.columns # Already checked earlier, but good practice
                can_split_granular = (
                    industry_col_in_desc and
                    target_col_in_desc and
                    'mfg_sectors' in globals() and isinstance(mfg_sectors, list) and
                    'nonmfg_sectors' in globals() and isinstance(nonmfg_sectors, list) and
                    df_analysis_desc[industry_col_name].notna().any()
                )

                if can_split_granular:
                    print(f"\nSplitting summary statistics by sector and zombie status using columns: '{industry_col_name}', '{target_col}'")

                    # --- Manufacturing Subsets ---
                    mfg_mask = df_analysis_desc[industry_col_name].isin(mfg_sectors)
                    # Mfg - Non-Zombie (target == 0)
                    mfg_non_zombie_mask = mfg_mask & (df_analysis_desc[target_col] == 0)
                    if mfg_non_zombie_mask.any():
                        desc_mfg_non_zombie = df_analysis_desc.loc[mfg_non_zombie_mask, numeric_cols_desc].describe(percentiles=[0.5])
                        print(f"  -> Calculated stats for {mfg_non_zombie_mask.sum()} Mfg / Non-Zombie firm-years.")
                    else:
                        print("  -> Warning: No Mfg / Non-Zombie firms found.")
                    # Mfg - Zombie (target == 1)
                    mfg_zombie_mask = mfg_mask & (df_analysis_desc[target_col] == 1)
                    if mfg_zombie_mask.any():
                        desc_mfg_zombie = df_analysis_desc.loc[mfg_zombie_mask, numeric_cols_desc].describe(percentiles=[0.5])
                        print(f"  -> Calculated stats for {mfg_zombie_mask.sum()} Mfg / Zombie firm-years.")
                    else:
                        print("  -> Warning: No Mfg / Zombie firms found.")

                    # --- Non-Manufacturing Subsets ---
                    nonmfg_mask = df_analysis_desc[industry_col_name].isin(nonmfg_sectors)
                     # Includes 'Other' if not explicitly filtered out earlier
                    # NonMfg - Non-Zombie (target == 0)
                    nonmfg_non_zombie_mask = nonmfg_mask & (df_analysis_desc[target_col] == 0)
                    if nonmfg_non_zombie_mask.any():
                        desc_nonmfg_non_zombie = df_analysis_desc.loc[nonmfg_non_zombie_mask, numeric_cols_desc].describe(percentiles=[0.5])
                        print(f"  -> Calculated stats for {nonmfg_non_zombie_mask.sum()} NonMfg / Non-Zombie firm-years.")
                    else:
                         print("  -> Warning: No NonMfg / Non-Zombie firms found.")
                    # NonMfg - Zombie (target == 1)
                    nonmfg_zombie_mask = nonmfg_mask & (df_analysis_desc[target_col] == 1)
                    if nonmfg_zombie_mask.any():
                        desc_nonmfg_zombie = df_analysis_desc.loc[nonmfg_zombie_mask, numeric_cols_desc].describe(percentiles=[0.5])
                        print(f"  -> Calculated stats for {nonmfg_zombie_mask.sum()} NonMfg / Zombie firm-years.")
                    else:
                         print("  -> Warning: No NonMfg / Zombie firms found.")

                    # --- Combine results dynamically ---
                    dfs_to_concat = [desc_all]
                    keys_for_concat = ["All"]

                    # Add Mfg groups if data exists
                    if not desc_mfg_non_zombie.empty:
                        dfs_to_concat.append(desc_mfg_non_zombie)
                        keys_for_concat.append(f"{PUBLICATION_NAMES.get('Mfg','Mfg')} - Non-Zombie")
                    if not desc_mfg_zombie.empty:
                        dfs_to_concat.append(desc_mfg_zombie)
                        keys_for_concat.append(f"{PUBLICATION_NAMES.get('Mfg','Mfg')} - Zombie")

                    # Add NonMfg groups if data exists
                    if not desc_nonmfg_non_zombie.empty:
                        dfs_to_concat.append(desc_nonmfg_non_zombie)
                        keys_for_concat.append(f"{PUBLICATION_NAMES.get('NonMfg','NonMfg')} - Non-Zombie")
                    if not desc_nonmfg_zombie.empty:
                        dfs_to_concat.append(desc_nonmfg_zombie)
                        keys_for_concat.append(f"{PUBLICATION_NAMES.get('NonMfg','NonMfg')} - Zombie")

                    # Concatenate if we have more than just 'All'
                    if len(dfs_to_concat) > 1:
                        summary_stats = pd.concat(dfs_to_concat, axis=1, keys=keys_for_concat)
                        print("\nSummary Statistics (Split by Sector & Zombie Status):")
                    else:
                        summary_stats = desc_all # Fallback if no subgroups had data
                        print("\nSummary Statistics (Overall Only - Granular split failed):")

                else: # If cannot split by sector/zombie
                     print(f"\nWarning: Cannot perform granular split for summary statistics. Required columns ('{industry_col_name}', '{target_col}') missing, empty, or sector lists undefined.")
                     summary_stats = desc_all # Default to only the 'All' summary
                     print("\nSummary Statistics (Overall Only):")
                # --- END Granular MODIFICATION ---

                # Print the final summary_stats
                # Use options to display all rows/columns if needed for checking
                with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                    print(summary_stats)

                # Save the resulting summary_stats DataFrame to Excel
                # Note: The filename reflects the previous request, but the content is now more granular
                # You might want to change the filename to e.g., "summary_stats_granular_{timestamp_str}.xlsx"
                try:
                    excel_filename = f"summary_stats_by_sector_granular_{timestamp_str}.xlsx" # MODIFIED Filename
                    full_excel_path = os.path.join(output_dir_path, excel_filename)
                    summary_stats.to_excel(full_excel_path)
                    print(f"\nSummary statistics (granular split) saved to {full_excel_path}")
                except Exception as e_save_excel:
                    print(f"Warning: Could not save granular summary stats to Excel: {e_save_excel}")

            else: # This else corresponds to 'if not df_analysis_desc.empty:'
                print("No valid numeric data remaining for summary statistics after describe cleanup.")
           # <<< --- END OF REPLACEMENT --- >>>

        except Exception as e: # This except corresponds to the 'try:' block starting above the user code
            print(f"Could not generate summary stats: {e}")
    else: # This else corresponds to 'if numeric_cols_desc and target_col_exists:'
        print("Skipping summary statistics: Required numeric or target columns not found in df_analysis.")
else: # This else corresponds to 'if 'df_analysis' in locals()...'
    print("Skipping summary statistics (df_analysis is empty or no valid rows).")

# --- Data Prep for Panel Models ---
# This section prepares df_analysis_panel and defines formulas needed for Part B.

# Default values as a safety net
panel_data_available = False
df_analysis_panel = None
# gee_panel_formula and fe_panel_formula are only defined on success path below

# Check if there is valid data from Part A to proceed
if 'df_analysis' in locals() and not df_analysis.empty and 'final_rows' in locals() and final_rows > 0:
    print("\n--- Preparing data for Panel Models ---")
    try:
        # --- Start: Code inside the try block ---
        df_analysis_panel = df_analysis.copy() # Use df_analysis (aligned w/ final X, y)

        # <<< --- START: ADD MANUFACTURING DUMMY HERE --- >>>
        if 'Is_Manufacturing' in df_analysis_panel.columns:
             print("Found 'Is_Manufacturing' dummy in df_analysis, using it for panel data.")
             # Ensure it's integer type if copied over
             if not pd.api.types.is_integer_dtype(df_analysis_panel['Is_Manufacturing']):
                 # Attempt conversion, handle potential NaNs introduced if original had issues
                 df_analysis_panel['Is_Manufacturing'] = pd.to_numeric(df_analysis_panel['Is_Manufacturing'], errors='coerce').fillna(0).astype(int) # Coerce, fill NaN with 0, convert to int
                 print("  Converted 'Is_Manufacturing' to integer type in panel data.")
        elif industry_col_available and industry_col_name in df_analysis_panel.columns:
            print("Adding 'Is_Manufacturing' dummy variable to df_analysis_panel...")
            def assign_mfg_dummy(sector):
                if sector in mfg_sectors: return 1
                elif sector in nonmfg_sectors: return 0
                else: return 0 # Treat 'Other' as 0
            df_analysis_panel['Is_Manufacturing'] = df_analysis_panel[industry_col_name].apply(assign_mfg_dummy).astype(int)
            print("  Successfully added 'Is_Manufacturing' column to panel data.")
        else:
            print("Warning: Cannot add 'Is_Manufacturing' dummy to panel data (source column missing). Adding column with 0.")
            df_analysis_panel['Is_Manufacturing'] = 0 # Default to 0 if cannot create
        # <<< --- END: ADD MANUFACTURING DUMMY HERE --- >>>
        
        # Clean feature names for use in formulas (replace space, %, /, &)
        cleaned_feature_cols_map = {}
        panel_feature_names_unscaled = [] # Store cleaned unscaled names
        for col in feature_cols: # feature_cols defined in Part A config
            clean_name = col.replace(' ', '_').replace('%', 'Perc').replace('/', '_').replace('&', 'and')
            # Add further replacements if needed based on your column names
            cleaned_feature_cols_map[col] = clean_name
            panel_feature_names_unscaled.append(clean_name)
        df_analysis_panel.rename(columns=cleaned_feature_cols_map, inplace=True)

        # --- ADD SCALING FOR GEE ---
        # Check if there's data in the columns to be scaled
        if not df_analysis_panel[panel_feature_names_unscaled].empty:
              scaler_gee = StandardScaler()
              scaled_cols = [f"{name}_scaled" for name in panel_feature_names_unscaled]
              # Apply scaling
              df_analysis_panel[scaled_cols] = scaler_gee.fit_transform(df_analysis_panel[panel_feature_names_unscaled])
              print(f"Added scaled columns for GEE: {scaled_cols}")
              # --- END SCALING ---

              # --- Create firm_id ---
              # --- Check for firm_id ---
              # firm_id should now exist as it was created in df_analysis earlier
              if 'firm_id' not in df_analysis_panel.columns:
                    # If it's missing here, something went wrong earlier, but don't try to create it again.
                    raise ValueError("'firm_id' column is unexpectedly missing from df_analysis_panel before panel model prep.")
              else:
                    print("Verified 'firm_id' column is present in df_analysis_panel.")

              # --- Ensure target column is integer type ---
              # Ensure target_col (defined in Part A config) exists
              if target_col in df_analysis_panel.columns:
                   # Convert target to numeric first (if it's not already), coercing errors
                   df_analysis_panel[target_col] = pd.to_numeric(df_analysis_panel[target_col], errors='coerce')
                   # Drop rows where target became NaN after coercion
                   initial_len = len(df_analysis_panel)
                   df_analysis_panel.dropna(subset=[target_col], inplace=True)
                   if len(df_analysis_panel) < initial_len:
                        print(f"Warning: Dropped {initial_len - len(df_analysis_panel)} rows due to non-numeric target during panel prep.")
                   # Convert to integer if not empty
                   if not df_analysis_panel.empty:
                        df_analysis_panel[target_col] = df_analysis_panel[target_col].astype(int)
                        print(f"Ensured target column '{target_col}' is integer.")
                   else:
                        raise ValueError("Target column conversion resulted in empty DataFrame.")
              else:
                    raise ValueError(f"Target column '{target_col}' not found in df_analysis_panel.")

# <<< --- START: REPLACE original formula definitions with this block --- >>>

            # --- Define Formulas for Statsmodels (with Main Effect + Interactions) ---

            # --- GEE Formula (Interact ALL) ---
              print("\nDefining GEE formula with Is_Manufacturing and ALL interactions...")
              if 'scaled_cols' in locals() and isinstance(scaled_cols, list):
                # Create interaction term for *every* variable in scaled_cols
                interaction_terms_gee = [f"{col_name} * Is_Manufacturing" for col_name in scaled_cols]
            
                # Construct the final GEE formula string
                gee_panel_formula = f"{target_col} ~ {' + '.join(scaled_cols)} + Is_Manufacturing"
                if interaction_terms_gee:
                    gee_panel_formula += " + " + " + ".join(interaction_terms_gee)
                print(f"--> Using formula for GEE model (ALL interactions): {gee_panel_formula}")
              else:
                print("ERROR: 'scaled_cols' not found. Cannot define GEE formula.")
                gee_panel_formula = None # Handle error state
                
              # --- FE Formula (Interact ALL - using UNscaled names) ---
              print("\nDefining FE formula with Is_Manufacturing and ALL interactions...")
              if 'panel_feature_names_unscaled' in locals() and isinstance(panel_feature_names_unscaled, list):
                 # Create interaction term for *every* unscaled variable name
                interaction_terms_fe = [f"{col_name} * Is_Manufacturing" for col_name in panel_feature_names_unscaled]
            
                # Construct the final FE formula string
                fe_panel_formula = f"{target_col} ~ {' + '.join(panel_feature_names_unscaled)} + Is_Manufacturing"
                if interaction_terms_fe:
                     fe_panel_formula += " + " + " + ".join(interaction_terms_fe)
                fe_panel_formula += " - 1"
                print(f"--> Using formula for FE model (ALL interactions): {fe_panel_formula}")
              else:    
                print("ERROR: 'panel_feature_names_unscaled' not found. Cannot define FE formula.")
                fe_panel_formula = None # Handle error state

            # Check if formulas were created successfully before setting the flag
              if not (gee_panel_formula and fe_panel_formula):
                 # Raise error or handle if formulas failed
                 raise ValueError("Failed to define necessary formulas for panel models.")

            # <<< --- END: REPLACE original formula definitions with this block --- >>>

              # --- If ALL steps above succeed, set flag to True ---
              print("Panel data preparation successful.")
              panel_data_available = True # DEFINED HERE (indicates success)
              # --- End: Code inside the try block ---
        else:
             print("Warning: No data available in feature columns to scale for GEE model. Panel prep failed.")
             panel_data_available = False # Ensure False if scaling failed

    except Exception as e_prep:
        print(f"ERROR during panel data preparation: {e_prep}")
        # import traceback # Uncomment for detailed debugging if needed
        # print(traceback.format_exc()) # Uncomment for detailed debugging
        panel_data_available = False # DEFINED HERE (indicates failure)
        df_analysis_panel = None # DEFINED HERE (reset dataframe on failure)
else:
    # This block runs if df_analysis was empty or missing, or final_rows was 0
    print("\nSkipping Panel Models Data Prep (no valid data from Part A).")
    panel_data_available = False # DEFINED HERE (indicates skipped)
    df_analysis_panel = None # DEFINED HERE (dataframe is None if skipped)
# --- End Data Prep ---

# Ensure the helper functions are defined before they are called

###############################################################################
# HELPER FUNCTION(S)
###############################################################################
print("\n--- Defining Helper Functions ---")

# --- Significance Code Function ---
def significance_code(p):
    """Returns significance stars based on p-value."""
    if pd.isna(p): return '' # Handle potential NaN p-values
    if p < 0.01: return '***'
    if p < 0.05: return '**'
    if p < 0.1: return '*'
    return ''

# --- Formatter for ASCII Minus Sign ---
def ascii_minus_formatter(x, pos):
    """
    Formatter function that formats the tick value using a general format
    and then replaces any Unicode minus signs with a standard hyphen.
    NOTE: matplotlib.rcParams['axes.unicode_minus'] = False should achieve
          this globally for default formatters. This provides explicit control.
    """
    # Use a general format specifier 'g' which is often adaptive
    s = "{:g}".format(x)
    # Replace Unicode minus with ASCII hyphen
    return s.replace('\u2212', '-')

# --- Helper Function for Detailed Cluster Summary --- ## VERSION WITH YEARLY ZOMBIE %, UNIQUE COMPANY COUNT & EXIT STATUS ##
def generate_cluster_summary_text(df_with_all_vars, labels, algorithm_name,
                                  numeric_cols, categorical_cols,
                                  target_col,
                                  id_vars, # <<< ADDED ID Vars argument
                                  company_static_info_df, # <<< ADDED Static Info DF argument
                                  exit_status_col='ExitStatus', # <<< ADDED Exit Status col name
                                  baseline_col=None, baseline_dist_series=None):
    """
    Generates a formatted text summary for each cluster, including descriptive stats
    for numeric variables, distribution for categorical variables, yearly zombie counts,
    count of unique companies, and exit status distribution for those unique companies.

    Args:
        df_with_all_vars (pd.DataFrame): DataFrame with firm-year observations for the subset.
                                         Must contain id_vars, year, target_col.
        labels (np.array or pd.Series): Cluster labels corresponding to df_with_all_vars index.
                                        Can be numeric or string type (mapped labels).
        algorithm_name (str): Name of the algorithm for the title.
        numeric_cols (list): List of numeric column names to summarize.
        categorical_cols (list): List of categorical column names to summarize (should NOT include exit_status_col).
        target_col (str): Name of the target variable column (e.g., "Zombie").
        id_vars (list): List of column names uniquely identifying a company.
        company_static_info_df (pd.DataFrame): DataFrame with unique company IDs (id_vars)
                                               and their final ExitStatus (exit_status_col).
        exit_status_col (str, optional): Name of the exit status column in company_static_info_df. Defaults to 'ExitStatus'.
        baseline_col (str, optional): Name of the categorical column for baseline comparison. Defaults to None.
        baseline_dist_series (pd.Series, optional): Overall distribution for baseline_col. Defaults to None.

    Returns:
        str: A formatted string containing the summary for all clusters.
    """
    report_lines = []
    report_lines.append(f"===== {algorithm_name} Cluster Summary =====")

    # --- Input validation ---
    if not isinstance(df_with_all_vars, pd.DataFrame):
        report_lines.append("Error: Invalid input DataFrame.")
        return "\n".join(report_lines)
    labels_array = labels.values if isinstance(labels, pd.Series) else labels # Ensure numpy array for length check
    if labels_array is None or len(labels_array) != len(df_with_all_vars):
        labels_len = len(labels_array) if labels_array is not None else 0
        report_lines.append(f"Error: Mismatch data({len(df_with_all_vars)})/labels({labels_len}).")
        return "\n".join(report_lines)
    if 'year' not in df_with_all_vars.columns:
        report_lines.append("Error: 'year' column missing from input df_with_all_vars. Cannot calculate yearly counts.")
    if target_col not in df_with_all_vars.columns:
        report_lines.append(f"Error: Target column '{target_col}' missing from input df_with_all_vars. Cannot calculate yearly zombie counts.")
    if not id_vars or not all(col in df_with_all_vars.columns for col in id_vars):
         report_lines.append(f"Error: id_vars {id_vars} not found in df_with_all_vars.")
         return "\n".join(report_lines) # Exit if IDs missing in main data

    # --- Validate Static Info DataFrame ---
    static_info_valid = False
    static_info_indexed = None # To hold the indexed version for lookup
    if isinstance(company_static_info_df, pd.DataFrame) and not company_static_info_df.empty:
        required_static_cols = id_vars + [exit_status_col]
        if all(col in company_static_info_df.columns for col in required_static_cols):
            try:
                # Ensure static info has unique IDs for reliable lookup
                if company_static_info_df.duplicated(subset=id_vars).any():
                     print(f"  (Warning: Duplicate companies found in company_static_info_df for {algorithm_name}. Using first entry for ExitStatus lookup.)")
                     company_static_info_df_unique = company_static_info_df.drop_duplicates(subset=id_vars, keep='first')
                else:
                     company_static_info_df_unique = company_static_info_df
                # Set index for faster lookup - crucial for performance
                static_info_indexed = company_static_info_df_unique.set_index(id_vars)
                static_info_valid = True # Mark as valid only if indexing succeeds
                print(f"  (Static info indexed successfully for {algorithm_name} summary)")
            except Exception as e_index:
                 report_lines.append(f"  (Warning: Could not set index on company_static_info_df: {e_index}. ExitStatus lookup might fail or be slow.)")
                 # Don't exit, but lookup might fail later
        else:
            missing_static = [col for col in required_static_cols if col not in company_static_info_df.columns]
            report_lines.append(f"  (Warning: company_static_info_df missing required columns: {missing_static}. Cannot report ExitStatus.)")
    else:
        report_lines.append(f"  (Warning: company_static_info_df is not a valid DataFrame or is empty. Cannot report ExitStatus.)")
    # --- End Static Info Validation ---


    all_vars_to_describe = numeric_cols + categorical_cols
    missing_vars = [v for v in all_vars_to_describe if v not in df_with_all_vars.columns]
    if missing_vars:
        report_lines.append(f"Warning: Variables {missing_vars} not found in provided DataFrame for summary stats.")
        numeric_cols = [c for c in numeric_cols if c not in missing_vars]
        categorical_cols = [c for c in categorical_cols if c not in missing_vars] # Ensure exit_status_col is not in this list

    # Baseline comparison validation (remains same)
    if baseline_col and baseline_col not in df_with_all_vars.columns: baseline_col = None
    if baseline_col and baseline_col not in categorical_cols: baseline_col = None
    if baseline_col and not isinstance(baseline_dist_series, pd.Series): baseline_col = None


    # --- Data processing ---
    cols_needed = list(set(numeric_cols + categorical_cols + ['year', target_col] + id_vars)) # Ensure id_vars included
    existing_cols_needed = [v for v in cols_needed if v in df_with_all_vars.columns]
    if not existing_cols_needed:
        report_lines.append("Error: No valid columns found for description.")
        return "\n".join(report_lines)

    df_temp = df_with_all_vars[existing_cols_needed].copy()
    try:
        # Assign cluster labels (could be numeric or string) using index alignment
        df_temp['cluster'] = pd.Series(labels_array, index=df_with_all_vars.index).reindex(df_temp.index)
        if 'cluster' not in df_temp.columns or df_temp['cluster'].isna().all():
             raise ValueError("Cluster column assignment failed or resulted in all NaNs.")
    except ValueError as e:
        report_lines.append(f"Error assigning cluster labels: {e}")
        return "\n".join(report_lines)

    # Drop rows where the assigned cluster label is NA for reliable grouping
    df_temp.dropna(subset=['cluster'], inplace=True)
    if df_temp.empty:
         report_lines.append("No non-NA cluster labels found. Cannot generate summary.")
         return "\n".join(report_lines)


    report_lines.append(f"\nValue Ranges & Distributions ({algorithm_name}):")

    # Pre-calculate yearly zombie counts (remains same)
    total_zombies_by_year_subset = pd.Series(dtype=int)
    if target_col in df_with_all_vars.columns and 'year' in df_with_all_vars.columns:
        try:
            subset_zombies = df_with_all_vars[df_with_all_vars[target_col] == 1]
            if not subset_zombies.empty: total_zombies_by_year_subset = subset_zombies.groupby('year').size()
        except Exception as e_totals: print(f"  (Warning: Failed to calculate total yearly zombies for subset {algorithm_name}: {e_totals})")


    # Determine unique cluster labels present in the data *after* NA drop
    try:
        unique_clusters_in_data = df_temp['cluster'].unique()
        # Attempt to sort numerically if possible, otherwise sort as strings
        try:
            numeric_clusters = pd.to_numeric(unique_clusters_in_data, errors='coerce')
            if not np.isnan(numeric_clusters).all(): # If at least one is numeric
                # Sort numerically, handle potential NaNs if conversion failed for some
                unique_clusters = sorted([c for c in unique_clusters_in_data if pd.notna(pd.to_numeric(c, errors='coerce'))], key=float) + \
                                  sorted([str(c) for c in unique_clusters_in_data if pd.isna(pd.to_numeric(c, errors='coerce'))]) # Convert non-numeric to string for sort
            else: # All are non-numeric strings
                 unique_clusters = sorted(str(c) for c in unique_clusters_in_data) # Ensure all are strings for sorting
        except: # Fallback to simple sort if complex sort fails
             unique_clusters = sorted(str(c) for c in unique_clusters_in_data)
    except Exception as e_sort:
        print(f"  Warning: Could not determine order for clusters ({e_sort}), using unique values as found.")
        unique_clusters = df_temp['cluster'].dropna().unique()

    if len(unique_clusters) == 0:
        report_lines.append("  No valid clusters found after dropping NA labels.")
        return "\n".join(report_lines)


    # --- Loop through each unique cluster label found in the data ---
    for cl in unique_clusters:
        # Format cluster label for display (same logic as before)
        try:
            numeric_cl = float(cl) # Attempt conversion
            # Use specific label for DBSCAN noise, otherwise generic 'Cluster #'
            if algorithm_name.startswith("DBSCAN") and int(numeric_cl) == -1:
                 cluster_label_str = "Noise"
            else:
                 cluster_label_str = f"Cluster {cl}" # Use original label (numeric or mapped string)
        except (ValueError, TypeError):
             cluster_label_str = str(cl) # Use the string label directly if conversion fails

        # Filter group based on the current cluster label
        group = df_temp[df_temp['cluster'] == cl] # Direct comparison works for both numeric and string labels
        count = len(group)

        report_lines.append(f"\n--- {cluster_label_str} ---")
        report_lines.append(f"  Firm-Year Count: {count}")

        # Overall share calculation (remains same)
        total_subset_count = len(df_with_all_vars) # Use original subset size for context
        if total_subset_count > 0:
            cluster_percentage_overall = (count / total_subset_count) * 100
            share_label = PUBLICATION_NAMES.get("Share", "Share")
            report_lines.append(f"  Share of Subset: {cluster_percentage_overall:.1f}% ({count}/{total_subset_count})")

        if count == 0: continue # Should not happen after dropping NAs, but safe check

        # Yearly Zombie Counts (remains same)
        if 'year' in group.columns and target_col in group.columns:
             try:
                  cluster_zombies_per_year = group[group[target_col] == 1].groupby('year').size()
                  cluster_zombies_per_year = cluster_zombies_per_year[cluster_zombies_per_year > 0].sort_index()
                  if not cluster_zombies_per_year.empty:
                       yearly_summary_dict = {}
                       for year, cluster_count in cluster_zombies_per_year.items():
                            total_count_for_year = total_zombies_by_year_subset.get(year, 0)
                            percentage = (cluster_count / total_count_for_year * 100) if total_count_for_year > 0 else np.nan
                            yearly_summary_dict[year] = f"{cluster_count} ({percentage:.1f}%)" if pd.notna(percentage) else f"{cluster_count} (N/A %)"
                       report_lines.append(f"  Yearly Zombie Counts (% of Year's Total Zombies in Subset): {yearly_summary_dict}")
                  else:
                       report_lines.append("  Yearly Zombie Counts (% of Year's Total Zombies in Subset): (None in this cluster)")
             except Exception as e_year_summary:
                  report_lines.append(f"  Yearly Zombie Counts (% of Year's Total Zombies in Subset): (Error: {e_year_summary})")
        else:
             report_lines.append(f"  Yearly Zombie Counts (% of Year's Total Zombies in Subset): (Required columns missing)")

        # <<< --- START: UNIQUE COMPANY COUNT & EXIT STATUS (Revised) --- >>>
        if static_info_valid and static_info_indexed is not None: # Check if static info DF is valid and indexed
            try:
                # Identify unique companies in this cluster group
                # Ensure we drop duplicates based on the actual id_vars list
                unique_company_ids_in_group_df = group[id_vars].drop_duplicates()
                num_unique_companies = len(unique_company_ids_in_group_df)
                report_lines.append(f"  Unique Companies in Cluster: {num_unique_companies}") # <<< ADDED Count

                if num_unique_companies > 0:
                    # Create multi-index from unique company IDs DataFrame for lookup
                    if len(id_vars) > 1:
                        lookup_index = pd.MultiIndex.from_frame(unique_company_ids_in_group_df)
                    else: # Single ID variable
                        lookup_index = unique_company_ids_in_group_df[id_vars[0]].tolist()

                    # Look up statuses from the indexed static info DataFrame
                    # Use reindex + .loc to handle potential index mismatches gracefully
                    # Check if index exists before trying to access
                    valid_lookup_index = static_info_indexed.index.intersection(lookup_index)

                    if not valid_lookup_index.empty:
                         company_statuses = static_info_indexed.loc[valid_lookup_index, exit_status_col]
                    else:
                         print(f"  (Warning: No matching company IDs found in static info index for cluster '{cl}'.)")
                         company_statuses = pd.Series(dtype='object') # Empty series if no matches

                    report_lines.append(f"  Exit Status Distribution ({num_unique_companies} Unique Companies):")
                    # Ensure company_statuses is a Series before value_counts
                    if isinstance(company_statuses, pd.Series) and not company_statuses.empty:
                        status_counts = company_statuses.fillna('Unknown/Missing').value_counts(normalize=False).sort_values(ascending=False) # Use absolute counts

                        if not status_counts.empty:
                            max_statuses_to_show = 7 # Show a few more statuses
                            statuses_shown = 0
                            other_count = 0
                            for status_value, cnt in status_counts.items():
                                perc = (cnt / num_unique_companies) * 100 # Calculate percentage based on unique companies in cluster
                                if statuses_shown < max_statuses_to_show:
                                    report_lines.append(f"    - {status_value}: {int(cnt)} ({perc:.1f}%)") # Show count and percentage
                                    statuses_shown += 1
                                else:
                                    other_count += cnt

                            if other_count > 0:
                                other_perc = (other_count / num_unique_companies) * 100
                                report_lines.append(f"    - Other Statuses: {int(other_count)} ({other_perc:.1f}%)")
                        else:
                            report_lines.append("    (Could not retrieve valid statuses for companies in this cluster)")
                    elif company_statuses.empty and not valid_lookup_index.empty:
                        # This case means IDs were found, but the status column had only NaNs for them
                         report_lines.append("    (All looked-up statuses were NaN)")
                    else: # Handle case where lookup might not return a Series or no IDs matched
                         report_lines.append(f"    (Status lookup failed or yielded no results)")

                else: # num_unique_companies == 0
                     report_lines.append("  Exit Status Distribution (Unique Companies): (No unique companies identified)")

            except Exception as e_comp_status:
                 report_lines.append(f"  Unique Companies in Cluster: (Error: {e_comp_status})")
                 report_lines.append(f"  Exit Status Distribution (Unique Companies): (Error calculating: {e_comp_status})")
        else:
             report_lines.append(f"  Unique Companies in Cluster: (Static info invalid/unavailable)")
             report_lines.append(f"  Exit Status Distribution (Unique Companies): (Static info invalid/unavailable)")
        # <<< --- END: UNIQUE COMPANY COUNT & EXIT STATUS --- >>>


        # --- Numeric Variables ---
        if numeric_cols:
            report_lines.append("\n  Numeric Variables:")
            valid_numeric_cols = [nc for nc in numeric_cols if nc in group.columns]
            if not valid_numeric_cols:
                report_lines.append("    (No valid numeric columns found in group)")
            else:
                for col in valid_numeric_cols:
                    numeric_col = pd.to_numeric(group[col], errors='coerce')
                    if numeric_col.notna().sum() == 0:
                        report_lines.append(f"    {col}: (No valid numeric data)")
                        continue
                    # ... (rest of numeric summary code remains the same) ...
                    desc = numeric_col.describe(percentiles=[.25, .5, .75])
                    q1 = desc.get('25%', np.nan); median = desc.get('50%', np.nan); q3 = desc.get('75%', np.nan)
                    min_val = desc.get('min', np.nan); max_val = desc.get('max', np.nan); iqr = q3 - q1 if pd.notna(q1) and pd.notna(q3) else np.nan
                    if pd.notna(iqr) and iqr > 0: whisker_low_limit = q1 - 1.5 * iqr; whisker_high_limit = q3 + 1.5 * iqr; actual_whisker_low = numeric_col[numeric_col >= whisker_low_limit].min() if not numeric_col[numeric_col >= whisker_low_limit].empty else min_val; actual_whisker_high = numeric_col[numeric_col <= whisker_high_limit].max() if not numeric_col[numeric_col <= whisker_high_limit].empty else max_val
                    else: whisker_low_limit = np.nan; whisker_high_limit = np.nan; actual_whisker_low = min_val; actual_whisker_high = max_val
                    report_lines.append(f"    {col}:")
                    if pd.notna(min_val): report_lines.append(f"      Min:           {min_val:.2f}")
                    if pd.notna(actual_whisker_low): report_lines.append(f"      Lower Whisker: {actual_whisker_low:.2f}")
                    if pd.notna(q1): report_lines.append(f"      Q1 (25%):      {q1:.2f}")
                    if pd.notna(median): report_lines.append(f"      Median (50%):  {median:.2f}")
                    if pd.notna(q3): report_lines.append(f"      Q3 (75%):      {q3:.2f}")
                    if pd.notna(actual_whisker_high): report_lines.append(f"      Upper Whisker: {actual_whisker_high:.2f}")
                    if pd.notna(max_val): report_lines.append(f"      Max:           {max_val:.2f}")


        # --- Other Categorical Variables ---
        # Filter out only exit_status_col, keep baseline_col for the loop
        categorical_cols_to_plot = [c for c in categorical_cols if c != exit_status_col] # <<< MODIFIED LINE: Keep baseline_col
        if categorical_cols_to_plot:
            report_lines.append("\n  Categorical Variable Distributions:") # Changed section title slightly
            valid_categorical_cols = [cc for cc in categorical_cols_to_plot if cc in group.columns]
            if not valid_categorical_cols:
                report_lines.append("    (No valid categorical columns found in group for distribution)")
            else:
                for col in valid_categorical_cols: # This loop will now include the baseline_col if it was in categorical_cols
                    # ... (code to calculate proportions - remains the same) ...
                    proportions = group[col].value_counts(normalize=True, dropna=False)
                    if not proportions.empty:
                        report_lines.append(f"    {col} Distribution:")
                        proportions = proportions.sort_values(ascending=False)
                        for item_value, prop in proportions.items():
                            item_label = "Missing/NaN" if pd.isna(item_value) else str(item_value)
                            if prop > 0:
                                diff_str = ""
                                # This comparison logic will now execute when col is the baseline_col
                                if baseline_col == col and baseline_dist_series is not None:
                                    try:
                                        baseline_prop = baseline_dist_series.get(item_value, 0)
                                        diff_pp = (prop - baseline_prop) * 100
                                        diff_str = f" ({diff_pp:+.1f} pp vs Baseline)"
                                    except Exception as e_baseline:
                                        diff_str = f" (Baseline comparison failed: {e_baseline})"
                                report_lines.append(f"        - {item_label}: {prop:.1%}{diff_str}") # Appends baseline diff if calculated
                    elif group[col].isna().all(): report_lines.append(f"    {col}: (All values are NaN)")
                    else: report_lines.append(f"    {col}: (No non-NaN values found)")

        # --- End Cluster Loop ---

    return "\n".join(report_lines)
# --- End of generate_cluster_summary_text Function ---

# --- Helper Function for Plotting Box Plot Grid for ALL Variables (ADAPTED FOR STRINGS & LONG LABELS) ---
def plot_subset_boxplot_grid(X_data_subset, labels_dict, cluster_counts, clustering_vars,
                             subset_suffix, fig_width, fig_height,
                             output_dir_path, timestamp_str):
    """Generates a single figure containing a grid of box plots (handles string labels, adapted for long labels)."""


    # Filter for algorithms that have valid label Series matching the data subset length
    valid_algorithms = {}
    if isinstance(labels_dict, dict):
        for name, labels_series in labels_dict.items(): # labels_dict now contains Series
            # Check if it's a Series, matches length, and has non-NA values
            if isinstance(labels_series, pd.Series) and \
               len(labels_series) == len(X_data_subset) and \
               labels_series.notna().any():
                valid_algorithms[name] = labels_series
            else:
                print(f"WARN: Labels for algorithm '{name}' are invalid, size mismatch, or all NA for subset '{subset_suffix}'. Skipping.")

    num_algos = len(valid_algorithms)
    num_vars = len(clustering_vars)

    if num_algos == 0 or num_vars == 0 or X_data_subset.empty:
        print(f"Skipping boxplot grid for {subset_suffix}: No valid algorithms, variables, or data.")
        return

    # --- Adjust figsize for potentially more vertical space ---
    # Increase the height multiplier slightly per variable row
    figsize = (fig_width * num_algos * 0.9, fig_height * num_vars * 0.9) # Increased height multiplier
    fig, axes = plt.subplots(num_vars, num_algos, figsize=figsize,
                             sharex='col', # Share x-axis within a column (algorithm)
                             sharey='row',
                             squeeze=False)

    subset_display_name = PUBLICATION_NAMES.get(subset_suffix, subset_suffix)
    #fig.suptitle(f'Variable Distributions by Cluster and Algorithm ({subset_display_name})',
                 #fontsize=matplotlib.rcParams['figure.titlesize'])

    boxplot_args = { # Keep consistent styling
        'linewidth': 0.75,
        'flierprops': dict(marker='.', mfc='black', ms=2, ls='none', mec='none', alpha=0.4),
        'boxprops': dict(edgecolor='black', facecolor=(0,0,0,0)), # Transparent boxes
        'whiskerprops': dict(color='black', linestyle='-'),
        'capprops': dict(color='black'),
        'medianprops': dict(color='red', linewidth=1)
    }

    algo_names = list(valid_algorithms.keys()) # Get ordered list of algorithm names

    for row_idx, var_name in enumerate(clustering_vars):
        display_var_name = PUBLICATION_NAMES.get(var_name, var_name)

        for col_idx, algo_name in enumerate(algo_names):
            ax = axes[row_idx, col_idx]
            labels_series = valid_algorithms[algo_name] # This is the FINAL STRING label Series
            algo_display_name = PUBLICATION_NAMES.get(algo_name, algo_name)
            num_unique_clusters = cluster_counts.get(algo_name, 0) # Count of unique final labels

            # Prepare data for this specific subplot
            try:
                temp_df = pd.DataFrame({
                    var_name: X_data_subset[var_name].values,
                    'Cluster': labels_series.values # Use the Series values (strings or NA)
                })
                # Drop rows where EITHER the feature OR the FINAL cluster label is NA
                temp_df.dropna(subset=[var_name, 'Cluster'], inplace=True)

                if temp_df.empty:
                    ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes, fontsize=7)
                    ax.set_title(f"{algo_display_name} (No Data)") # Update title if no data
                    continue

                # Ensure Cluster column is string for sorting/plotting
                temp_df['Cluster'] = temp_df['Cluster'].astype(str)

                # --- Determine Expected Cluster Order (Sorted Strings) ---
                # Get unique string labels present in this specific subplot's data
                expected_clusters_str = sorted(temp_df['Cluster'].unique())
                # --- End Determine Order ---

                if not expected_clusters_str: # Handle case where all labels were NA after dropna
                     ax.text(0.5, 0.5, 'No Valid Clusters', ha='center', va='center', transform=ax.transAxes, fontsize=7)
                     ax.set_title(f"{algo_display_name} (No Clusters)") # Update title
                     continue
                else:
                    # --- Create the boxplot using explicit string order ---
                    sns.boxplot(data=temp_df, x='Cluster', y=var_name, ax=ax,
                                palette='viridis',
                                order=expected_clusters_str, # <<< USE SORTED STRING ORDER
                                **boxplot_args)

                    # Set x-ticks based on the string order
                    ax.set_xticks(range(len(expected_clusters_str))) # Set ticks based on position
                    ax.set_xticklabels(expected_clusters_str) # Label with cluster STRINGS

                    # --- *** ADJUSTED ROTATION LOGIC *** ---
                    # Rotate labels 90 degrees if they are numerous or long
                    max_label_len = 0
                    if expected_clusters_str: # Check list is not empty
                        max_label_len = max(len(str(lab)) for lab in expected_clusters_str)

                    # Increase rotation to 90 if labels are long or numerous
                    if len(expected_clusters_str) > 4 or max_label_len > 15: # Adjust thresholds as needed
                        ax.tick_params(axis='x', rotation=90, ha='center', # Use center align for 90 deg
                                       labelsize=max(5, matplotlib.rcParams['xtick.labelsize'] * 0.75)) # Slightly smaller font
                    elif len(expected_clusters_str) > 3 or max_label_len > 10: # Use 45 for moderately long/many
                         ax.tick_params(axis='x', rotation=45, ha='right',
                                       labelsize=max(5, matplotlib.rcParams['xtick.labelsize'] * 0.85))
                    else: # Keep horizontal for few/short labels
                        ax.tick_params(axis='x', rotation=0, ha='center')
                    # --- *** END ADJUSTED ROTATION LOGIC *** ---

                    # --- APPLY ASCII MINUS FORMATTER (Boxplot Y-Axis) ---
                    if 'ascii_minus_formatter' in globals():
                        ax.yaxis.set_major_formatter(FuncFormatter(ascii_minus_formatter))

            except Exception as e_sns:
                print(f"Error during sns.boxplot: {e_sns}")
                #ax.text(0.5, 0.5, 'Plot Err', ha='center', va='center', transform=ax.transAxes, fontsize=7)

            # --- Set Titles and Labels ---
            if row_idx == 0: # Column Titles (Algorithm Names) only on the top row
                title_suffix = f"(Clusters={num_unique_clusters})" # Use unique count from final labels
                ax.set_title(f"{algo_display_name} {title_suffix}", fontsize=matplotlib.rcParams['axes.titlesize'])

            if col_idx == 0: ax.set_ylabel(display_var_name, fontsize=matplotlib.rcParams['axes.labelsize'])
            else: ax.set_ylabel("")

            # Only set X label on the *bottom* row
            if row_idx == num_vars - 1:
                 ax.set_xlabel(PUBLICATION_NAMES.get("Cluster", "Cluster"), fontsize=matplotlib.rcParams['axes.labelsize'])
            else:
                 ax.set_xlabel("") # Remove x-label from non-bottom rows

            ax.tick_params(axis='y', which='major', labelsize=matplotlib.rcParams['ytick.labelsize']) # Ensure y tick size is applied
            ax.grid(False) # Turn off grid for boxplots

    # Adjust layout - increase vertical spacing if needed
    try:
        # Add more vertical space between subplots using hspace
        fig.tight_layout(rect=[0, 0.03, 1, 0.96], h_pad=1.5) # Increased h_pad slightly
    except Exception as e_tl:
        print(f"Warning: tight_layout failed for boxplot grid {subset_suffix}: {e_tl}")
        # Try subplots_adjust as a fallback if tight_layout fails badly
        try:
             fig.subplots_adjust(hspace=0.4, wspace=0.15, top=0.95, bottom=0.1) # Manually adjust spacing
        except Exception as e_sa:
              print(f"Warning: subplots_adjust also failed for boxplot grid {subset_suffix}: {e_sa}")


    # --- Saving ---
    safe_subset = subset_suffix.replace(' ', '_').replace('/', '_').replace('\\', '_')
    base_filename = f"boxplot_grid_{safe_subset}_FINAL_{timestamp_str}.png" # Add FINAL
    full_save_path = os.path.join(output_dir_path, base_filename)
    try:
        plt.savefig(full_save_path, dpi=600)
        print(f"Saved Boxplot Grid: {full_save_path}")
    except Exception as e_save:
        print(f"Error saving plot {full_save_path}: {e_save}")
    plt.close(fig)

# --- End of plot_subset_boxplot_grid Helper Function (ADAPTED FOR STRINGS & LONG LABELS) ---

# --- Helper Function for Plotting Time Evolution Bars (ADAPTED FOR STRINGS) ---
def plot_cluster_evolution_bars(df, year_col, cluster_col, k_clusters, algo_name, ax):
    """Plots time evolution of cluster proportions (can handle string labels)."""
    # Style is now set globally, no plt.style.use here needed

    if cluster_col not in df.columns:
        print(f"Warning in plot_cluster_evolution_bars: Col '{cluster_col}' missing for {algo_name}. Skipping subplot.")
        ax.text(0.5, 0.5, 'Data Err', ha='center', va='center', transform=ax.transAxes) # Fontsize from rcParams
        ax.set_title(f"{algo_name} (Error)") # Fontsize from rcParams
        return

    plot_df = df[[year_col, cluster_col]].dropna(subset=[year_col, cluster_col]).copy() # Drop NA in relevant cols

    # Ensure cluster_col is treated as object/string type for grouping
    plot_df[cluster_col] = plot_df[cluster_col].astype(str) # Convert to string for robust grouping

    # Group by year and string cluster label
    cluster_ev = plot_df.groupby([year_col, cluster_col]).size().unstack(fill_value=0)
    # Calculate proportions
    cluster_ev_prop = cluster_ev.apply(lambda x: x / x.sum() if x.sum() > 0 else x, axis=1)

    # Sort columns (string labels) alphabetically for consistent color mapping
    cluster_ev_prop = cluster_ev_prop.sort_index(axis=1)
    unique_string_labels = cluster_ev_prop.columns.tolist() # Get sorted string labels

    actual_k_for_plot = len(unique_string_labels) # Number of unique string labels found

    if cluster_ev_prop.empty or actual_k_for_plot == 0:
        print(f"Warning in plot_cluster_evolution_bars: No valid cluster data for {algo_name} to plot bars.")
        ax.text(0.5, 0.5, 'No Cl.', ha='center', va='center', transform=ax.transAxes) # Fontsize from rcParams
        # Determine title based on whether input 'k_clusters' was intended for DBSCAN or others
        is_dbscan_intent = 'DBSCAN' in algo_name # Check original algo name intent
        title_suffix = "(No non-noise data)" if is_dbscan_intent else "(No data)"
        ax.set_title(f"{PUBLICATION_NAMES.get(algo_name, algo_name)} {title_suffix}") # Fontsize from rcParams
        return

    # Update title (don't need k if using strings)
    algo_display_name = PUBLICATION_NAMES.get(algo_name, algo_name)
    title = f"{algo_display_name}" # Simpler title using mapped names
    ax.set_title(title)

    # Use number of unique string labels for colormap
    num_colors_needed = actual_k_for_plot
    try:
        colors = matplotlib.colormaps['viridis'](np.linspace(0, 1, max(1, num_colors_needed)))
    except Exception: colors = matplotlib.colormaps['gray'](np.linspace(0, 1, max(1, num_colors_needed))) # Fallback

    # Plotting using the sorted string columns
    cluster_ev_prop.plot(kind='bar', stacked=True, ax=ax, color=colors, width=0.8, alpha=0.8, legend=False)

    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))
    ax.set_ylim(0, 1)
    # Rotate x-ticks if many years
    if len(cluster_ev_prop.index) > 10:
        ax.tick_params(axis='x', rotation=45, labelsize=matplotlib.rcParams['xtick.labelsize'] * 0.9) # Slightly smaller if rotated
    else:
        ax.tick_params(axis='x', rotation=0)

    # Grid styling specific to this plot
    ax.grid(False) # Turn off default grid
    ax.yaxis.grid(True, linestyle='--', alpha=0.7, linewidth=0.5) # Subtle horizontal lines only


    # --- ADJUSTED Legend Logic (Frameless, Title Left-Aligned, Using String Labels) ---
    handles = [plt.Rectangle((0,0),1,1, color=colors[min(i, len(colors)-1)]) for i in range(num_colors_needed)]
    # Use the unique string labels directly for the legend
    labels_leg = unique_string_labels

    legend_obj = None # Initialize legend object

    if len(labels_leg) > 1:
        # Create legend WITHOUT frame
        legend_obj = ax.legend(handles, labels_leg,
                               loc='center left',
                               bbox_to_anchor=(1.02, 0.5), # Position outside right
                               title=PUBLICATION_NAMES.get("Cluster", "Cluster") + "s", # Generic title
                               title_fontsize=8,
                               fontsize=7,
                               frameon=False # <<< SET TO FALSE to remove frame/background
                              )
        # Align title AFTER legend creation
        if legend_obj and legend_obj.get_title():
            legend_obj.get_title().set_horizontalalignment('left')

    elif len(labels_leg) == 1: # Single cluster case (no legend box)
        ax.text(0.95, 0.95, labels_leg[0],
                transform=ax.transAxes, ha='right', va='top',
                fontsize=matplotlib.rcParams['legend.fontsize'])

    # Ensure axis labels are set (using existing settings if any)
    ax.set_xlabel(ax.get_xlabel())
    ax.set_ylabel(ax.get_ylabel())
    # --- END ADJUSTED Legend Logic ---
# --- End of plot_cluster_evolution_bars Helper Function (ADAPTED FOR STRINGS) ---

def run_clustering_analysis(df_subset, X_subset, subset_suffix, # Core inputs
                            clustering_vars, feature_cols, target_col, industry_col_name,
                            industry_col_available,
                            id_vars,                 # Non-default config - NEW
                            company_static_info_df,  # Non-default config - NEW
                            output_dir_path,         # Non-default config
                            timestamp_str,           # Non-default config
                            width,                   # Non-default config
                            height,                  # Non-default config
                            # --- Start Default Arguments ---
                            exit_status_col='ExitStatus', # NEW Optional arg
                            dbscan_eps_range="auto",
                            dbscan_min_samples=16,
                            perform_tsne=True,
                            include_zombie_overlay=False,
                            kmeans_k_min=2,
                            kmeans_k_max=5,
                            ):
    """
    Performs scaling, clustering (KMeans, Hierarchical, DBSCAN), applies mapping
    from global USER_CLUSTER_MAPPINGS (if available), generates plots and summaries
    using the mapped labels. CORRECTED FOR DUPLICATION.

    Args:
        # ... (args remain the same, except user_label_mapping removed) ...

    Returns:
        pd.DataFrame: The input df_subset with added RAW cluster label columns
                      (e.g., 'kmeans_cluster_raw') and FINAL mapped string
                      cluster label columns (e.g., 'kmeans_cluster').
                      Returns the original df_subset if clustering fails early.
    """
    print(f"\n\n====== Running Clustering for Subset: {subset_suffix} ======")
    print(f"Input df shape: {df_subset.shape}, Input X shape: {X_subset.shape}")
    print(f"Using clustering variables: {clustering_vars}")
    print(f"Requested K-range for KMeans/Hierarchical: {kmeans_k_min} to {kmeans_k_max}")

    # Initialize output DataFrame with potential NaN columns if they don't exist
    df_subset_out = df_subset.copy() # Start fresh
    # Ensure raw columns exist, initialize final columns later if mapping occurs
    for raw_col_base in ['kmeans_cluster', 'hier_cluster', 'dbscan_cluster']:
        raw_col = f"{raw_col_base}_raw"
        final_col = raw_col_base # e.g., 'kmeans_cluster'
        if raw_col not in df_subset_out.columns:
            # Use pandas nullable integer type for raw labels
            df_subset_out[raw_col] = pd.NA
            df_subset_out[raw_col] = df_subset_out[raw_col].astype('Int64')
        if final_col not in df_subset_out.columns:
            df_subset_out[final_col] = pd.NA
            df_subset_out[final_col] = df_subset_out[final_col].astype('string')


    # Initial checks for sufficient data
    min_required_samples = max(3, dbscan_min_samples) # Need >=3 for most clustering, >= min_samples for DBSCAN
    if X_subset.empty or X_subset.shape[0] < min_required_samples:
        print(f"Skipping clustering for {subset_suffix}: Not enough samples ({X_subset.shape[0]} < {min_required_samples}).")
        return df_subset_out # Return original df

    # --- Scale Subset Data ---
    print(f"\nScaling {subset_suffix} dataset...")
    scaler_subset = RobustScaler()
    X_subset_scaled_df = pd.DataFrame() # Initialize

    try:
        missing_cvars = list(set(clustering_vars) - set(X_subset.columns))
        if missing_cvars:
            print(f"ERROR: Clustering variables missing from X_subset ({subset_suffix}): {missing_cvars}")
            return df_subset_out

        X_to_scale = X_subset[clustering_vars]
        # Ensure data is numeric before scaling
        X_to_scale = X_to_scale.apply(pd.to_numeric, errors='coerce')
        if X_to_scale.isnull().any().any():
            nan_cols_before_scale = X_to_scale.columns[X_to_scale.isnull().any()].tolist()
            print(f"WARN: NaN values found in columns before scaling ({subset_suffix}): {nan_cols_before_scale}. RobustScaler might handle some, but check inputs.")
            # Optional: Impute NaNs before scaling if RobustScaler doesn't handle them as desired
            # X_to_scale = X_to_scale.fillna(X_to_scale.median()) # Example imputation

        X_subset_scaled = scaler_subset.fit_transform(X_to_scale)
        X_subset_scaled_df = pd.DataFrame(X_subset_scaled, columns=clustering_vars, index=X_subset.index)
        print(f"Scaled {subset_suffix} data shape: {X_subset_scaled_df.shape}")
        n_samples_subset = X_subset_scaled_df.shape[0]

        if np.isinf(X_subset_scaled_df.values).any():
            print(f"ERROR: Infinite values detected after scaling {subset_suffix} dataset. Cannot proceed.")
            return df_subset_out # Return original df
        if X_subset_scaled_df.isnull().values.any():
            nan_cols_after_scale = X_subset_scaled_df.columns[X_subset_scaled_df.isnull().any()].tolist()
            print(f"WARN: NaN values detected after scaling ({subset_suffix}): {nan_cols_after_scale}. Clustering might be affected.")

    except Exception as e_scale:
        print(f"ERROR: Failed to scale {subset_suffix} dataset: {e_scale}")
        import traceback
        print(traceback.format_exc())
        return df_subset_out # Return original df

    if X_subset_scaled_df.empty:
        print(f"ERROR: Scaled data is empty for {subset_suffix}. Cannot proceed.")
        return df_subset_out

    # --- Initialize RAW results holders ---
    labels_kmeans_raw = np.array([])
    best_k = None
    labels_hier_raw = np.array([])
    labels_dbscan_raw = np.array([])
    n_clusters_dbscan = 0 # Stores number of non-noise clusters found by DBSCAN
    dbscan_best_eps = -1 # Store the best eps found
    k_distances = None # To store distances for k-distance plot

    # --- Run Clustering Algorithms (get RAW labels) ---

    # --- K-Means (Get Raw Labels) ---
    print(f"\n--- K-Means ({subset_suffix} - Getting Raw Labels) ---")
    effective_max_k = min(kmeans_k_max, n_samples_subset - 1)
    effective_min_k = max(2, kmeans_k_min)
    range_n_clusters_subset = []
    if effective_max_k >= effective_min_k:
        range_n_clusters_subset = list(range(effective_min_k, effective_max_k + 1))
        print(f"Effective K-range after sample size check: {effective_min_k} to {effective_max_k}")
    else:
        print(f"Skipping K-Means range search: Cannot form valid range (Min={effective_min_k}, Max={effective_max_k}, N={n_samples_subset})")

    kmeans_results = {}
    if len(range_n_clusters_subset) > 0 :
        print(f"Testing K from {min(range_n_clusters_subset)} to {max(range_n_clusters_subset)}")
        for n in range_n_clusters_subset:
            try:
                km = KMeans(n_clusters=n, random_state=42, n_init=10)
                labels_k_raw = km.fit_predict(X_subset_scaled_df) # Get raw labels
                unique_labels_km = np.unique(labels_k_raw)
                n_unique_km = len(unique_labels_km)
                sil = np.nan # Default score
                if 1 < n_unique_km < n_samples_subset:
                    try:
                        sil = silhouette_score(X_subset_scaled_df, labels_k_raw)
                        print(f"  k={n}: Sil={sil:.3f}")
                    except Exception as e_score:
                        print(f"  k={n}: Silhouette score calculation failed: {e_score}")
                else:
                    print(f"  k={n}: Invalid labels found (n_clusters={n_unique_km}). Cannot score.")
                kmeans_results[n] = {'l_raw': labels_k_raw, 's': sil} # Store RAW labels
            except MemoryError:
                print(f"  k={n}: KMeans failed - MemoryError."); kmeans_results[n] = {'l_raw': None, 's': np.nan}; continue
            except Exception as e_km:
                print(f"  k={n}: KMeans failed: {e_km}"); kmeans_results[n] = {'l_raw': None, 's': np.nan}; continue

        valid_results = {k: v for k, v in kmeans_results.items() if v.get('l_raw') is not None and pd.notna(v.get('s'))}
        if valid_results:
            best_k = max(valid_results, key=lambda k: valid_results[k]['s'])
            labels_kmeans_raw = kmeans_results[best_k]['l_raw']
            print(f"  Best K (KMeans) for {subset_suffix} based on Silhouette score: {best_k}")
        else:
            best_k_fallback = next((k for k, v in kmeans_results.items() if v.get('l_raw') is not None and len(np.unique(v['l_raw'])) > 1), None)
            if best_k_fallback:
                best_k = best_k_fallback
                labels_kmeans_raw = kmeans_results[best_k_fallback]['l_raw']
                print(f"  Using fallback K (KMeans) = {best_k} for {subset_suffix} (first valid clustering).")
            else:
                best_k = None; labels_kmeans_raw = np.array([]); print(f"  No valid K-Means clustering found or scored for {subset_suffix}.")
    else:
        best_k = None; labels_kmeans_raw = np.array([])


    # --- Hierarchical (Get Raw Labels) ---
    print(f"\n--- Hierarchical ({subset_suffix} - Getting Raw Labels) ---")
    if best_k is not None and 1 < best_k < n_samples_subset:
        print(f"Attempting Hierarchical clustering with k={best_k}...")
        try:
            hc = AgglomerativeClustering(n_clusters=best_k, linkage='ward')
            labels_hier_raw = hc.fit_predict(X_subset_scaled_df) # Get RAW labels
            if 1 < len(set(labels_hier_raw)) < n_samples_subset:
                try: sil_h = silhouette_score(X_subset_scaled_df, labels_hier_raw); print(f"  k={best_k}: Sil={sil_h:.3f}")
                except Exception as e_score_h: print(f"  k={best_k}: Hierarchical score calculation failed: {e_score_h}")
            else: print(f"  k={best_k}: Could not score Hierarchical results (invalid raw labels).")
        except MemoryError: print(f"  k={best_k}: Hierarchical clustering failed - MemoryError."); labels_hier_raw = np.array([])
        except Exception as e_hc: print(f"  k={best_k}: Hierarchical clustering failed ({e_hc})."); labels_hier_raw = np.array([])
    else: print(f"  Skipping Hierarchical (best_k={best_k} invalid)."); labels_hier_raw = np.array([])


    # --- k-distance plot for DBSCAN ---
    print(f"\n--- Generating k-distance plot for DBSCAN eps ({subset_suffix}) ---")
    dbscan_min_samples_for_plot = dbscan_min_samples
    if n_samples_subset > dbscan_min_samples_for_plot:
        print(f"Calculating nearest neighbors for k={dbscan_min_samples_for_plot} on {subset_suffix}...")
        try:
            nbrs = NearestNeighbors(n_neighbors=dbscan_min_samples_for_plot).fit(X_subset_scaled_df)
            distances, _ = nbrs.kneighbors(X_subset_scaled_df)
            k_distances = np.sort(distances[:, dbscan_min_samples_for_plot-1], axis=0)
            fig_kdist, ax_kdist = plt.subplots(figsize=(width, height))
            ax_kdist.plot(k_distances)
            ax_kdist.set_title(f'k-Distance Graph (k={dbscan_min_samples_for_plot}) ({PUBLICATION_NAMES.get(subset_suffix, subset_suffix)})')
            ax_kdist.set_xlabel("Points sorted by distance")
            ax_kdist.set_ylabel(f"{dbscan_min_samples_for_plot}-th NN Distance (eps candidate)")
            ax_kdist.grid(True, linestyle='--', alpha=0.6)
            kdist_filename = f"k_distance_plot_{subset_suffix}_k{dbscan_min_samples_for_plot}_{timestamp_str}.png"
            kdist_save_path = os.path.join(output_dir_path, kdist_filename)
            try:
                plt.savefig(kdist_save_path, dpi=300)
                print(f"k-distance plot (k={dbscan_min_samples_for_plot}, {subset_suffix}) saved to: {kdist_save_path}")
                print(f"----> Examine this plot to MANUALLY set 'dbscan_eps_range' for {subset_suffix} for better results.")
            except Exception as e_save: print(f"Error saving k-distance plot: {e_save}")
            plt.close(fig_kdist)
        except MemoryError: print(f"ERROR: MemoryError during NN calculation ({subset_suffix})."); k_distances = None
        except Exception as e_nn: print(f"ERROR: Could not calculate NN ({subset_suffix}): {e_nn}"); k_distances = None
    else:
        print(f"Skipping k-distance plot ({subset_suffix}) (n_samples={n_samples_subset} <= k={dbscan_min_samples_for_plot}).")
        k_distances = None


    # --- DBSCAN (Get Raw Labels - Automated Search) ---
    print(f"\n--- DBSCAN ({subset_suffix} - Getting Raw Labels - Automated Search) ---")
    current_eps_values_to_try = None
    if isinstance(dbscan_eps_range, str) and dbscan_eps_range == "auto":
        print("NOTE: Using automated percentile method for eps range (visual inspection of k-dist plot recommended).")
        if k_distances is not None and k_distances.size > 0:
            try:
                eps_min_guess = np.percentile(k_distances, 50); eps_max_guess = np.percentile(k_distances, 98)
                eps_min_guess = max(0.05, eps_min_guess * 0.8); eps_max_guess = eps_max_guess * 1.2
                if eps_max_guess > eps_min_guess:
                    current_eps_values_to_try = np.linspace(eps_min_guess, eps_max_guess, 11)
                    print(f"INFO: Auto eps range from k-dist plot: [{eps_min_guess:.2f} - {eps_max_guess:.2f}]")
                else: current_eps_values_to_try = None # Fallback
            except Exception as e_percentile: print(f"WARN: Error calculating eps range from k-distances: {e_percentile}. Using fallback."); current_eps_values_to_try = None
        if current_eps_values_to_try is None:
            current_eps_values_to_try = np.linspace(0.5, 2.5, 11); print(f"WARN: Using fallback eps range for DBSCAN search: {np.round(current_eps_values_to_try, 2)}")
    elif isinstance(dbscan_eps_range, (list, np.ndarray)):
        current_eps_values_to_try = np.array(dbscan_eps_range); print(f"INFO: Using manually provided eps range: {np.round(current_eps_values_to_try, 2)}")
    else:
        print("ERROR: Invalid dbscan_eps_range provided. Must be 'auto' or a list/array."); current_eps_values_to_try = np.linspace(0.5, 2.5, 11); print(f"WARN: Using fallback eps range: {np.round(current_eps_values_to_try, 2)}")

    current_min_samples = dbscan_min_samples
    best_eps = -1; best_score = -np.inf; best_labels_raw = np.array([]); best_n_clusters = 0; best_noise_perc = 1.0

    print(f"Testing {len(current_eps_values_to_try)} eps values: {np.round(current_eps_values_to_try, 2)}")
    print(f"Using min_samples = {current_min_samples} on {subset_suffix} data...")

    if n_samples_subset >= current_min_samples:
        for current_eps in current_eps_values_to_try:
            try:
                db = DBSCAN(eps=current_eps, min_samples=current_min_samples, n_jobs=-1)
                labels_d = db.fit_predict(X_subset_scaled_df)
                n_clusters_ = len(set(labels_d)) - (1 if -1 in labels_d else 0)
                noise_points_ = np.sum(labels_d == -1)
                noise_perc_ = noise_points_ / n_samples_subset if n_samples_subset > 0 else 0
                print(f"  eps={current_eps:.2f}: Found {n_clusters_} clusters, {noise_perc_*100:.1f}% noise", end="")
                score = -np.inf; n_non_noise_points = n_samples_subset - noise_points_
                if 1 < n_clusters_ < n_non_noise_points:
                    try:
                        non_noise_mask = (labels_d != -1)
                        score = silhouette_score(X_subset_scaled_df[non_noise_mask], labels_d[non_noise_mask])
                        print(f", Sil={score:.3f}")
                        if score > best_score:
                            best_score = score; best_eps = current_eps; best_labels_raw = labels_d; best_n_clusters = n_clusters_; best_noise_perc = noise_perc_
                    except ValueError as ve_score: print(f", Sil=N/A (ValueError: {ve_score})")
                    except Exception as e_score: print(f", Score ERR: {e_score}")
                else: print(", Invalid for scoring (Sil)")
            except MemoryError: print(f"\nERROR: MemoryError during DBSCAN fit (eps={current_eps:.2f}, {subset_suffix}). Stopping."); best_eps = -99; break
            except Exception as e_fit: print(f"\nERROR: DBSCAN fit failed (eps={current_eps:.2f}, {subset_suffix}): {e_fit}")

        if best_eps > 0:
            print(f"\n--- Best DBSCAN Raw Result ({subset_suffix} based on Silhouette) ---")
            print(f"  Best eps: {best_eps:.3f}, Score: {best_score:.3f}, Clusters: {best_n_clusters}, Noise: {best_noise_perc*100:.1f}%")
            labels_dbscan_raw = best_labels_raw
            n_clusters_dbscan = best_n_clusters # Store non-noise count
            dbscan_best_eps = best_eps
        elif best_eps == -99: print(f"\n--- DBSCAN failed due to MemoryError ({subset_suffix}) ---"); labels_dbscan_raw = np.array([]); n_clusters_dbscan = 0
        else: print(f"\n--- DBSCAN: No valid clustering found/scored ({subset_suffix}) ---"); labels_dbscan_raw = np.array([]); n_clusters_dbscan = 0
    else: print(f"Skipping DBSCAN loop ({subset_suffix}) (n_samples < min_samples)."); labels_dbscan_raw = np.array([]); n_clusters_dbscan = 0


    # --- Store RAW Labels in DataFrame ---
    raw_labels_generated = {} # Re-initialize here to capture final raw results

    if labels_kmeans_raw.size == len(df_subset_out):
        df_subset_out['kmeans_cluster_raw'] = pd.Series(labels_kmeans_raw, index=X_subset_scaled_df.index).reindex(df_subset_out.index).astype('Int64')
        raw_labels_generated['KMeans'] = labels_kmeans_raw
    elif 'kmeans_cluster_raw' not in df_subset_out.columns: df_subset_out['kmeans_cluster_raw'] = pd.NA; df_subset_out['kmeans_cluster_raw'] = df_subset_out['kmeans_cluster_raw'].astype('Int64')

    if labels_hier_raw.size == len(df_subset_out):
        df_subset_out['hier_cluster_raw'] = pd.Series(labels_hier_raw, index=X_subset_scaled_df.index).reindex(df_subset_out.index).astype('Int64')
        raw_labels_generated['Hierarchical'] = labels_hier_raw
    elif 'hier_cluster_raw' not in df_subset_out.columns: df_subset_out['hier_cluster_raw'] = pd.NA; df_subset_out['hier_cluster_raw'] = df_subset_out['hier_cluster_raw'].astype('Int64')

    if labels_dbscan_raw.size == len(df_subset_out):
        df_subset_out['dbscan_cluster_raw'] = pd.Series(labels_dbscan_raw, index=X_subset_scaled_df.index).reindex(df_subset_out.index).astype('Int64')
        raw_labels_generated['DBSCAN'] = labels_dbscan_raw
    elif 'dbscan_cluster_raw' not in df_subset_out.columns: df_subset_out['dbscan_cluster_raw'] = pd.NA; df_subset_out['dbscan_cluster_raw'] = df_subset_out['dbscan_cluster_raw'].astype('Int64')


    # --- Generate and Save INITIAL Cluster Summary (using RAW labels) ---
    # --- This is USEFUL for the FIRST run to DETERMINE the mapping ---
    print(f"\n--- Generating INITIAL Cluster Summary ({subset_suffix} using RAW labels) ---")
    initial_summary_texts = []
    numeric_summary_cols_init = [c for c in feature_cols if c in df_subset_out.columns]
    industry_available_subset_init = industry_col_available and industry_col_name and industry_col_name in df_subset_out.columns
    categorical_vars_subset_init = [industry_col_name] if industry_available_subset_init else []
    baseline_dist_init = None
    baseline_col_init = industry_col_name if industry_available_subset_init else None
    if baseline_col_init:
        try: baseline_dist_init = df_subset_out[baseline_col_init].value_counts(normalize=True, dropna=False)
        except Exception: baseline_dist_init = None

    # Generate summary text for each algorithm using RAW labels
    for algo_name_key, raw_labels in raw_labels_generated.items():
        # Check if raw_labels is a valid numpy array and has the correct size
        if isinstance(raw_labels, np.ndarray) and raw_labels.size == len(df_subset_out):
             if 'generate_cluster_summary_text' in globals():
                  initial_summary = generate_cluster_summary_text(
                      df_subset_out, raw_labels, # <<< Use raw_labels directly from the loop
                      f"{algo_name_key} (Raw)", numeric_summary_cols_init, categorical_vars_subset_init,
                      target_col=target_col,
                      id_vars=id_vars,                         # Pass id_vars
                      company_static_info_df=company_static_info_df, # Pass static df
                      exit_status_col=exit_status_col,        # Pass exit status col name
                      baseline_col=baseline_col_init, baseline_dist_series=baseline_dist_init
                  )
                  initial_summary_texts.append(initial_summary)
             else: initial_summary_texts.append(f"Summary function missing for {algo_name_key} (Raw)")
        else:
             # Handle cases where raw_labels might be invalid or size mismatch
             labels_size = raw_labels.size if isinstance(raw_labels, np.ndarray) else 'N/A'
             df_size = len(df_subset_out)
             print(f"WARN: Skipping INITIAL summary for {algo_name_key}. Invalid labels (type: {type(raw_labels)}, size: {labels_size}) or size mismatch with DataFrame (size: {df_size}).")
             initial_summary_texts.append(f"No valid raw labels found for {algo_name_key} to summarize.")
        # <<< End CORRECTED BLOCK >>>

    # Save the INITIAL summary
    if initial_summary_texts:
        full_initial_summary = "\n\n".join(s for s in initial_summary_texts if s)
        if full_initial_summary.strip():
            summary_filename_init = f"cluster_summary_INITIAL_{subset_suffix}_{timestamp_str}.txt"
            full_summary_path_init = os.path.join(output_dir_path, summary_filename_init)
            try:
                with open(full_summary_path_init, "w", encoding='utf-8') as f: f.write(full_initial_summary)
                print(f"INITIAL cluster summary text saved to: {full_summary_path_init}")
                print(f"----> INSPECT THIS FILE TO DEFINE/REFINE 'USER_CLUSTER_MAPPINGS' FOR '{subset_suffix}' <----")
            except Exception as e_save_sum: print(f"Error saving INITIAL summary file {full_summary_path_init}: {e_save_sum}")
        else: print("No INITIAL summary text generated to save.")


    # ======================================================================
    # === APPLY USER MAPPING TO CREATE FINAL (String) LABELS ===
    # ======================================================================
    print(f"\n--- Applying User Mapping for Final Labels ({subset_suffix}) ---")
    final_labels_generated = {} # Stores the FINAL mapped label Series for plots/summary

    # --- FIX: Extract base key for mapping lookup ---
    mapping_key = None
    # Determine the correct key to use in USER_CLUSTER_MAPPINGS based on subset_suffix
    if "Zombie_Mfg" in subset_suffix:
        mapping_key = "Mfg" # Use the key defined in your USER_CLUSTER_MAPPINGS dict
    elif "Zombie_NonMfg" in subset_suffix:
        mapping_key = "Non_Mfg" # Use the key defined in your USER_CLUSTER_MAPPINGS dict
    elif "Mfg" in subset_suffix: # Fallback if just "Mfg" is passed
        mapping_key = "Mfg"
    elif "NonMfg" in subset_suffix or "Non_Mfg" in subset_suffix: # Fallback if just "NonMfg"/"Non_Mfg" is passed
        mapping_key = "Non_Mfg"
    # Add elif conditions here if you have other subset suffixes like "All_Full_Fallback"
    # and corresponding keys in your USER_CLUSTER_MAPPINGS dictionary.
    # Example:
    # elif "Fallback" in subset_suffix:
    #     mapping_key = "Fallback_Key_In_Dict" # Replace with your actual dictionary key

    subset_mapping = USER_CLUSTER_MAPPINGS.get(mapping_key) # Use extracted key for lookup
    # --- END FIX ---

    if subset_mapping and isinstance(subset_mapping, dict):
        print(f"Found mapping configuration for key '{mapping_key}' (derived from '{subset_suffix}'). Applying...") # Log using the key found
        # Loop through algorithms to apply mapping
        for algo_name_key, raw_col_name in [('KMeans', 'kmeans_cluster_raw'),
                                            ('Hierarchical', 'hier_cluster_raw'),
                                            ('DBSCAN', 'dbscan_cluster_raw')]:

            final_col_name = raw_col_name.replace('_raw', '') # e.g., 'kmeans_cluster'
            # Ensure final column exists and is string type
            if final_col_name not in df_subset_out.columns: df_subset_out[final_col_name] = pd.NA
            df_subset_out[final_col_name] = df_subset_out[final_col_name].astype('string')

            if raw_col_name not in df_subset_out.columns or df_subset_out[raw_col_name].isna().all():
                print(f"  Skipping mapping for {algo_name_key}: Raw labels column '{raw_col_name}' not available or all NA.")
                final_labels_generated[algo_name_key] = df_subset_out[final_col_name] # Store the existing (likely all NA) series
                continue

            algo_map_dict = subset_mapping.get(algo_name_key)

            if algo_map_dict and isinstance(algo_map_dict, dict):
                print(f"  Applying mapping for {algo_name_key}: {algo_map_dict}")
                try:
                    raw_series_for_map = df_subset_out[raw_col_name]
                    # Map raw integer labels to strings using the defined dictionary
                    mapped_values = raw_series_for_map.map(algo_map_dict)
                    # Assign mapped strings (or NA if unmapped) to the final column
                    df_subset_out[final_col_name] = mapped_values.astype('string')

                    # Check for unmapped raw labels that are not NA
                    unmapped_mask = df_subset_out[final_col_name].isna() & df_subset_out[raw_col_name].notna()
                    unmapped_count = unmapped_mask.sum()
                    if unmapped_count > 0:
                        unmapped_raw_labels = df_subset_out.loc[unmapped_mask, raw_col_name].unique()
                        # Convert potentially mixed types (like Int64) to string for printing
                        unmapped_raw_labels_str = [str(x) if pd.notna(x) else 'NA' for x in unmapped_raw_labels]
                        print(f"  -> Warning: {unmapped_count} observations had raw labels not found in the user map for {algo_name_key}.")
                        print(f"     Unmapped raw labels: {unmapped_raw_labels_str}")
                        print(f"     These are now NA in final column '{final_col_name}'.")

                    # Store the final Series (containing strings or NAs) for plotting
                    if df_subset_out[final_col_name].notna().any():
                        final_labels_generated[algo_name_key] = df_subset_out[final_col_name].copy() # Store a copy
                    else:
                        final_labels_generated[algo_name_key] = None # No valid final labels
                    print(f"  -> Created/updated final mapped column: {final_col_name}")

                except Exception as e_map:
                    print(f"  -> ERROR applying mapping for {algo_name_key}: {e_map}")
                    df_subset_out[final_col_name] = pd.NA # Reset column to NA on error
                    final_labels_generated[algo_name_key] = None
            else:
                print(f"  Skipping mapping for {algo_name_key}: No valid mapping dictionary provided in USER_CLUSTER_MAPPINGS['{mapping_key}']. Defaulting to stringified raw labels.")
                # Fallback: Convert raw numeric labels to strings directly
                if df_subset_out[raw_col_name].notna().any():
                    df_subset_out[final_col_name] = df_subset_out[raw_col_name].astype(str).astype('string')
                    final_labels_generated[algo_name_key] = df_subset_out[final_col_name].copy()
                else:
                    df_subset_out[final_col_name] = pd.NA
                    final_labels_generated[algo_name_key] = None
                print(f"  -> Populated final column '{final_col_name}' with stringified raw labels.")

    else:
        # Fallback if no mapping dictionary found for the subset
        print(f"--- No User Mapping dictionary found or configured for key '{mapping_key}' derived from subset '{subset_suffix}' ---")
        print("Plots and Final Summary will use RAW numerical labels (converted to string).")
        for algo_name_key, raw_col_name in [('KMeans', 'kmeans_cluster_raw'),
                                            ('Hierarchical', 'hier_cluster_raw'),
                                            ('DBSCAN', 'dbscan_cluster_raw')]:
            final_col_name = raw_col_name.replace('_raw', '')
            if raw_col_name in df_subset_out.columns and df_subset_out[raw_col_name].notna().any():
                final_series = df_subset_out[raw_col_name].astype(str).astype('string') # Convert raw Int64 to string
                final_labels_generated[algo_name_key] = final_series.copy()
                if final_col_name not in df_subset_out.columns: df_subset_out[final_col_name] = pd.NA
                df_subset_out[final_col_name] = final_series # Assign stringified raw labels
            else:
                final_labels_generated[algo_name_key] = None
                if final_col_name not in df_subset_out.columns: df_subset_out[final_col_name] = pd.NA
                df_subset_out[final_col_name] = df_subset_out[final_col_name].astype('string') # Ensure string type


    # ======================================================================
    # === Prepare parameters for plotting using FINAL mapped labels     ===
    # ======================================================================
    plot_params_subset = {} # Stores parameters needed for plotting (cluster column name, k/unique_labels)
    labels_plot_dict = {}   # Stores the actual FINAL label Series used for plotting (strings or stringified raw)
    counts_plot_dict = {}   # Stores the number of unique FINAL non-NA labels

    # Use the results from final_labels_generated
    for algo_name_key, final_labels_series in final_labels_generated.items():
        # Check if final_labels_series is a valid Series and contains non-NA values
        if isinstance(final_labels_series, pd.Series) and final_labels_series.notna().any():
            final_col_name = f"{algo_name_key.lower().replace('kmeans','kmeans_cluster').replace('hierarchical','hier_cluster').replace('dbscan','dbscan_cluster')}" # Construct final col name
            # Calculate unique non-NA final labels
            unique_final_labels = final_labels_series.dropna().unique()
            num_unique_final = len(unique_final_labels)

            if num_unique_final > 0:
                # Ensure the column exists in the dataframe before adding to plot params
                if final_col_name in df_subset_out.columns:
                    plot_params_subset[algo_name_key] = {'col': final_col_name, 'k': num_unique_final} # Use final col name, store unique count as 'k'
                    labels_plot_dict[algo_name_key] = final_labels_series # Store the Series (strings or stringified raw)
                    counts_plot_dict[algo_name_key] = num_unique_final
                    print(f"Prepared plotting params for {algo_name_key} using final column '{final_col_name}' ({num_unique_final} unique labels).")
                else:
                    print(f"Skipping plotting params for {algo_name_key}: Final column '{final_col_name}' not found in DataFrame.")
            else:
                print(f"Skipping plotting params for {algo_name_key}: No valid final labels after mapping/fallback (all NA).")
        else:
            print(f"Skipping plotting params for {algo_name_key}: No final labels generated or series is invalid.")

    valid_algos_subset = list(plot_params_subset.keys()) # Get keys from the populated dict
    num_algos = len(valid_algos_subset) # Update num_algos based on actual results
    print(f"\nAlgorithms with valid FINAL results for plotting/summary ({subset_suffix}): {valid_algos_subset}")


    # --- Generate Plots (Using FINAL mapped labels if available) ---
    if num_algos > 0 and not X_subset_scaled_df.empty:
        print(f"\n--- Generating Plots ({subset_suffix} - Using Final Labels) ---")

        # --- Time Evolution Plot (Uses Corrected Helper) ---
        if 'year' in df_subset_out.columns and all(p['col'] in df_subset_out.columns for p in plot_params_subset.values()):
            print(f"\nGenerating Time Evolution plot ({subset_suffix} - Final Labels)...")
            fig_evo_bar, axes_evo_bar = plt.subplots(num_algos, 1, figsize=(width * 1.8, height * num_algos * 0.9), sharex=True, squeeze=False) # Ensure axes_evo_bar is always array
            axes_evo_bar = axes_evo_bar.flatten() # Flatten for easy indexing

            subset_display_name = PUBLICATION_NAMES.get(subset_suffix, subset_suffix)
            #fig_evo_bar.suptitle(f'Annual Cluster Composition ({subset_display_name})')

            for i, algo_name in enumerate(valid_algos_subset): # Iterate through the algorithms that worked
                ax_current = axes_evo_bar[i]
                params = plot_params_subset[algo_name]
                if 'plot_cluster_evolution_bars' in globals():
                    # Call the adapted helper function
                    plot_cluster_evolution_bars(
                        df=df_subset_out, # Pass the df with the FINAL string labels
                        year_col='year',
                        cluster_col=params['col'], # Pass the FINAL column name (e.g., 'kmeans_cluster')
                        k_clusters=params['k'], # Pass the count of unique FINAL labels
                        algo_name=algo_name, # Original algorithm name for title
                        ax=ax_current
                    )
                else:
                    print("WARN: plot_cluster_evolution_bars helper function not found.")
                    ax_current.text(0.5, 0.5, 'Plot Func Err', ha='center', va='center', transform=ax_current.transAxes)

                # Set Y label only for the middle plot
                if i == num_algos // 2: ax_current.set_ylabel(f"{PUBLICATION_NAMES.get('Share', 'Share')} (%)")
                else: ax_current.set_ylabel('')
                # Set X label only for the bottom plot
                if i == num_algos - 1: ax_current.set_xlabel('Year')
                else: ax_current.set_xlabel(''); ax_current.tick_params(axis='x', labelbottom=False)

            try: plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust rect for suptitle
            except Exception as e_tl: print(f"Warning: tight_layout failed for evolution plot ({subset_suffix}): {e_tl}")
            evo_filename = f"cluster_evolution_bars_{subset_suffix}_FINAL_{timestamp_str}.png" # Add FINAL suffix
            evo_save_path = os.path.join(output_dir_path, evo_filename)
            try: plt.savefig(evo_save_path, dpi=600); print(f"Evolution plot saved: {evo_save_path}")
            except Exception as e_save: print(f"Error saving plot {evo_save_path}: {e_save}")
            plt.close(fig_evo_bar)
        else: print("Skipping time evolution plot: 'year' or final cluster columns missing, or no valid algorithms.")


        # --- Visualizations (PCA/t-SNE) (MODIFIED TO HANDLE STRINGS & DBSCAN NOISE) ---
        if n_samples_subset >= 2:
            zombie_mask_subset = None
            # Zombie mask alignment logic remains the same (ensure alignment with X_subset_scaled_df.index)
            if include_zombie_overlay and target_col in df_subset_out.columns:
                if X_subset_scaled_df.index.isin(df_subset_out.index).all():
                    try:
                        aligned_target = df_subset_out.loc[X_subset_scaled_df.index, target_col]
                        zombie_mask_subset = (aligned_target == 1).values
                        if zombie_mask_subset.size != n_samples_subset: zombie_mask_subset = None # Reset if size mismatch
                    except KeyError: zombie_mask_subset = None
                else: zombie_mask_subset = None

# --- PCA ---
            print(f"\nGenerating PCA plot ({subset_suffix} - Final Labels)...")
            X_pca_subset = None # Initialize
            try:
                pca_obj = PCA(n_components=2, random_state=42)
                X_pca_subset = pca_obj.fit_transform(X_subset_scaled_df) # NumPy array, shape (n_samples_subset, 2)
                print(f"PCA explained variance ({subset_suffix}): {pca_obj.explained_variance_ratio_}")
                # --- PCA Loadings Text File ---
                # ... (loadings report code remains the same) ...
                if hasattr(pca_obj, 'components_') and pca_obj.components_.shape == (2, len(clustering_vars)):
                    # ... (Code to generate and save loadings report) ...
                    pass # Placeholder - ensure loading report code is here
                else: print("Could not generate PCA loadings report.")
                # --- End PCA Loadings Text File ---

                # --- PCA Plotting ---
                fig_pca, axes_pca = plt.subplots(1, num_algos, figsize=(width*num_algos*0.9, height*1.1), sharex=True, sharey=True, squeeze=False)
                axes_pca = axes_pca.flatten()

                subset_display_name = PUBLICATION_NAMES.get(subset_suffix, subset_suffix)
                title_pca = f"PCA Projection by Cluster ({subset_display_name})"
                if include_zombie_overlay: title_pca += f" - {PUBLICATION_NAMES.get('Zombie', 'Zombie')}"
                #fig_pca.suptitle(title_pca)

                for i, algo_key_name in enumerate(valid_algos_subset):
                    ax = axes_pca[i]
                    algo_display_name = PUBLICATION_NAMES.get(algo_key_name, algo_key_name)
                    labels_vis_final = labels_plot_dict.get(algo_key_name) # Pandas Series aligned with X_pca_subset

                    if labels_vis_final is None or labels_vis_final.size != X_pca_subset.shape[0]:
                        # ... (error handling) ...
                        continue

                    # --- *** START REVISED MASKING & PLOTTING FOR PCA (INTEGER INDICES) *** ---
                    valid_mask_pca = labels_vis_final.notna() # Pandas bool Series (length N)

                    if not valid_mask_pca.any():
                         # ... (handle no valid labels) ...
                         continue

                    is_dbscan_plot = 'DBSCAN' in algo_key_name

                    # Create masks relative to the original X_pca_subset length (N)
                    noise_mask_pca = pd.Series(False, index=labels_vis_final.index)
                    cluster_mask_pca = pd.Series(False, index=labels_vis_final.index)

                    if is_dbscan_plot:
                        noise_mask_pca = (labels_vis_final == "Noise") & valid_mask_pca # Bool Series, length N
                        cluster_mask_pca = (labels_vis_final != "Noise") & valid_mask_pca # Bool Series, length N
                    else:
                        cluster_mask_pca = valid_mask_pca # Bool Series, length N

                    # --- Get Integer Indices ---
                    noise_indices = np.where(noise_mask_pca.values)[0]   # Integer array
                    cluster_indices = np.where(cluster_mask_pca.values)[0] # Integer array

                    # Plot noise points using integer indices
                    if is_dbscan_plot and noise_indices.size > 0:
                        ax.scatter(X_pca_subset[noise_indices, 0], X_pca_subset[noise_indices, 1],
                                   c='lightgrey', marker='x', s=3, alpha=0.6, label='_nolegend_')

                    # Plot cluster points using integer indices
                    unique_str_labels_clusters_pca = []
                    k_clusters_final_pca = 0
                    if cluster_indices.size > 0:
                        # Get labels only for the cluster points to determine colors
                        labels_clusters_only_pca = labels_vis_final.iloc[cluster_indices] # Use iloc with integer indices
                        codes_clusters_pca, unique_str_labels_clusters_pca = pd.factorize(labels_clusters_only_pca, sort=True)
                        k_clusters_final_pca = len(unique_str_labels_clusters_pca)

                        # Plot using the integer cluster indices
                        scatter_clusters_pca = ax.scatter(X_pca_subset[cluster_indices, 0], X_pca_subset[cluster_indices, 1],
                                                          c=codes_clusters_pca,
                                                          s=1, cmap='viridis', alpha=0.7,
                                                          vmin=0, vmax=max(0, k_clusters_final_pca - 1))

                    # Set title
                    title_suffix = f"(Clusters={k_clusters_final_pca})"
                    t = f"{algo_display_name} {title_suffix}"
                    ax.set_title(t)
                    # --- *** END REVISED MASKING & PLOTTING FOR PCA (INTEGER INDICES) *** ---

                    # --- Zombie Overlay ---
                    zombie_mask_to_plot_indices = [] # Default empty
                    if include_zombie_overlay and zombie_mask_subset is not None:
                        if zombie_mask_subset.size == X_pca_subset.shape[0]:
                            # Combine zombie mask with valid label mask
                            zombie_mask_combined = zombie_mask_subset & valid_mask_pca.values # NumPy bool array, length N
                            # Get integer indices for zombies to plot
                            zombie_mask_to_plot_indices = np.where(zombie_mask_combined)[0]
                            if zombie_mask_to_plot_indices.size > 0:
                                # Index original X_pca_subset with these integer indices
                                ax.scatter(X_pca_subset[zombie_mask_to_plot_indices, 0],
                                           X_pca_subset[zombie_mask_to_plot_indices, 1],
                                           marker='o', s=4, facecolors='none', edgecolors='red', linewidths=0.2, alpha=0.7, label='_nolegend_')
                        else: print(f"WARN: Zombie mask size mismatch in PCA overlay for {algo_key_name}.")

                    # --- Axis Labels and Grid ---
                    # ... (Axis label and grid code remains the same) ...
                    pc1_label = PUBLICATION_NAMES.get("PC1", "PC1")
                    pc2_label = PUBLICATION_NAMES.get("PC2", "PC2")
                    ax.set_xlabel(f"{pc1_label}")
                    ax.grid(True, ls='--', alpha=0.6, lw=0.5)
                    if i == 0: ax.set_ylabel(f"{pc2_label}")
                    else: ax.set_ylabel("")
                    if 'ascii_minus_formatter' in globals():
                        ax.xaxis.set_major_formatter(FuncFormatter(ascii_minus_formatter))
                        ax.yaxis.set_major_formatter(FuncFormatter(ascii_minus_formatter))


                    # --- ADJUSTED LEGEND CREATION (PCA - Handles DBSCAN Noise) ---
                    # (Legend logic remains the same, using k_clusters_final_pca, unique_str_labels_clusters_pca, noise_indices, zombie_mask_to_plot_indices)
                    handles_list_pca = []
                    labels_list_pca = []
                    # Add handles for actual clusters found
                    if k_clusters_final_pca > 0:
                        cmap_func_pca = matplotlib.colormaps['viridis']
                        norm_func_pca = plt.Normalize(vmin=0, vmax=max(0, k_clusters_final_pca - 1))
                        for code_idx, label_str in enumerate(unique_str_labels_clusters_pca):
                            try: color_val = cmap_func_pca(norm_func_pca(code_idx))
                            except ValueError: color_val = 'grey'
                            handles_list_pca.append(mlines.Line2D([],[], color=color_val, marker='o', ls='None', ms=4))
                            labels_list_pca.append(str(label_str))
                    # Add handle for Noise if DBSCAN and noise exists
                    if is_dbscan_plot and noise_indices.size > 0:
                        noise_handle = mlines.Line2D([], [], color='lightgrey', marker='x', linestyle='None', markersize=3)
                        handles_list_pca.append(noise_handle)
                        labels_list_pca.append("Noise")
                    # Add Zombie handle if needed
                    if include_zombie_overlay and zombie_mask_subset is not None and zombie_mask_to_plot_indices.size > 0:
                         zombie_handle = mlines.Line2D([], [], color='red', marker='o', ls='None', markersize=3, markerfacecolor='none', markeredgewidth=0.5)
                         handles_list_pca.append(zombie_handle); labels_list_pca.append(PUBLICATION_NAMES.get("Zombie", "Zombie"))
                    # Create legend
                    if handles_list_pca:
                        legend_title = "Clusters" if not is_dbscan_plot else "Clusters & Noise"
                        legend_obj_pca = ax.legend(handles=handles_list_pca, labels=labels_list_pca, loc='best', title=legend_title, title_fontsize=7, fontsize=6, frameon=True, facecolor='white', framealpha=0.6, edgecolor='black', fancybox=False)
                        if legend_obj_pca:
                            legend_obj_pca.get_frame().set_linewidth(plt.rcParams['axes.linewidth'])
                            if legend_obj_pca.get_title(): legend_obj_pca.get_title().set_horizontalalignment('left')
                    # --- END ADJUSTED LEGEND (PCA) ---

                # --- Finalize and Save PCA plot ---
                # ... (Saving code remains the same) ...
                try: fig_pca.tight_layout(rect=[0, 0.03, 1, 0.93])
                except Exception as e_tl: print(f"Warning: tight_layout failed for PCA plot ({subset_suffix}): {e_tl}")
                pca_filename = f"clusters_{subset_suffix}_pca_FINAL_{timestamp_str}.png"
                pca_save_path = os.path.join(output_dir_path, pca_filename)
                try: plt.savefig(pca_save_path, dpi=600); print(f"PCA viz saved: {pca_save_path}")
                except Exception as e_save: print(f"Error saving PCA plot {pca_save_path}: {e_save}")
                plt.close(fig_pca)


            except Exception as e_pca:
                print("Error during PCA generation")
                import traceback
                print("--- PCA Error Traceback ---")
                print(traceback.format_exc())
                print("---------------------------")
                if 'fig_pca' in locals() and fig_pca in plt.get_fignums(): plt.close(fig_pca)
            # --- End of PCA Section ---
            
            # --- t-SNE ---
            if perform_tsne:
                print(f"\nGenerating t-SNE plot ({subset_suffix} - Final Labels)...")
                perp_val = max(5, min(30, n_samples_subset - 1)) if n_samples_subset > 1 else 0
                if perp_val > 0:
                    X_tsne_subset = None
                    try:
                        tsne_obj=TSNE(n_components=2, random_state=42, perplexity=perp_val, init='pca', learning_rate='auto', n_iter=1000, n_jobs=-1)
                        X_tsne_subset = tsne_obj.fit_transform(X_subset_scaled_df) # NumPy array, shape (n_samples_subset, 2)
                        # --- t-SNE Inputs Text File ---
                        # ... (t-SNE inputs report code remains the same - ensure it's present once) ...
                        tsne_feature_names = clustering_vars
                        # ... (Code to generate and save t-SNE input report) ...
                        pass # Placeholder

                        # --- t-SNE Plotting ---
                        fig_tsne, axes_tsne = plt.subplots(1, num_algos, figsize=(width*num_algos*0.9, height*1.1), sharex=True, sharey=True, squeeze=False)
                        axes_tsne = axes_tsne.flatten()
                        # ... (Title setup remains the same) ...
                        subset_display_name = PUBLICATION_NAMES.get(subset_suffix, subset_suffix)
                        title_tsne = f"t-SNE Embedding by Cluster ({subset_display_name})"
                        if include_zombie_overlay: title_tsne += f" - {PUBLICATION_NAMES.get('Zombie', 'Zombie')}"
                        #fig_tsne.suptitle(title_tsne)

                        for i, algo_key_name in enumerate(valid_algos_subset):
                            ax=axes_tsne[i]
                            algo_display_name = PUBLICATION_NAMES.get(algo_key_name, algo_key_name)
                            labels_vis_final = labels_plot_dict.get(algo_key_name) # Pandas Series aligned with X_tsne_subset

                            if labels_vis_final is None or labels_vis_final.size != X_tsne_subset.shape[0]:
                               # ... (error handling) ...
                               continue

                            # --- *** START REVISED MASKING & PLOTTING FOR T-SNE (INTEGER INDICES) *** ---
                            valid_mask_tsne = labels_vis_final.notna() # Pandas bool Series (length N)

                            if not valid_mask_tsne.any():
                                 # ... (handle no valid labels) ...
                                 continue

                            is_dbscan_plot = 'DBSCAN' in algo_key_name

                            # Create masks relative to the original X_tsne_subset length (N)
                            noise_mask_tsne = pd.Series(False, index=labels_vis_final.index)
                            cluster_mask_tsne = pd.Series(False, index=labels_vis_final.index)

                            if is_dbscan_plot:
                                noise_mask_tsne = (labels_vis_final == "Noise") & valid_mask_tsne # Bool Series, length N
                                cluster_mask_tsne = (labels_vis_final != "Noise") & valid_mask_tsne # Bool Series, length N
                            else:
                                cluster_mask_tsne = valid_mask_tsne # Bool Series, length N

                            # --- Get Integer Indices ---
                            noise_indices = np.where(noise_mask_tsne.values)[0]   # Integer array
                            cluster_indices = np.where(cluster_mask_tsne.values)[0] # Integer array

                            # Plot noise points using integer indices
                            if is_dbscan_plot and noise_indices.size > 0:
                                ax.scatter(X_tsne_subset[noise_indices, 0], X_tsne_subset[noise_indices, 1],
                                           c='lightgrey', marker='x', s=3, alpha=0.7, label='_nolegend_')

                            # Plot cluster points using integer indices
                            unique_str_labels_clusters_tsne = []
                            k_clusters_final_tsne = 0
                            if cluster_indices.size > 0:
                                # Get labels only for the cluster points to determine colors
                                labels_clusters_only_tsne = labels_vis_final.iloc[cluster_indices] # Use iloc with integer indices
                                codes_clusters_tsne, unique_str_labels_clusters_tsne = pd.factorize(labels_clusters_only_tsne, sort=True)
                                k_clusters_final_tsne = len(unique_str_labels_clusters_tsne)

                                # Plot using the integer cluster indices
                                scatter_clusters_tsne = ax.scatter(X_tsne_subset[cluster_indices, 0], X_tsne_subset[cluster_indices, 1],
                                                                  c=codes_clusters_tsne,
                                                                  s=1, cmap='viridis', alpha=0.7,
                                                                  vmin=0, vmax=max(0, k_clusters_final_tsne - 1))

                            # Set title
                            title_suffix = f"(Clusters={k_clusters_final_tsne})"
                            t = f"{algo_display_name} {title_suffix}"
                            ax.set_title(t)
                            # --- *** END REVISED MASKING & PLOTTING FOR T-SNE (INTEGER INDICES) *** ---

                            # --- Zombie Overlay ---
                            zombie_mask_to_plot_indices = [] # Default empty
                            if include_zombie_overlay and zombie_mask_subset is not None:
                                if zombie_mask_subset.size == X_tsne_subset.shape[0]:
                                    # Combine zombie mask with valid label mask
                                    zombie_mask_combined = zombie_mask_subset & valid_mask_tsne.values # NumPy bool array, length N
                                    # Get integer indices for zombies to plot
                                    zombie_mask_to_plot_indices = np.where(zombie_mask_combined)[0]
                                    if zombie_mask_to_plot_indices.size > 0:
                                        # Index original X_tsne_subset with these integer indices
                                        ax.scatter(X_tsne_subset[zombie_mask_to_plot_indices, 0],
                                                   X_tsne_subset[zombie_mask_to_plot_indices, 1],
                                                   marker='o', s=3, facecolors='none', edgecolors='red', linewidths=0.2, alpha=0.7, label='_nolegend_')
                                else: print(f"WARN: Zombie mask size mismatch in t-SNE overlay for {algo_key_name}.")


                            # --- Axis Labels and Grid --- (Remain the same)
                            # ... (Axis label and grid code remains the same) ...
                            ax.set_xlabel(PUBLICATION_NAMES.get("t-SNE1", "t-SNE1")); ax.grid(True,ls='--',alpha=0.6,lw=0.5)
                            if i==0: ax.set_ylabel(PUBLICATION_NAMES.get("t-SNE2", "t-SNE2"))
                            else: ax.set_ylabel("")
                            if 'ascii_minus_formatter' in globals(): ax.xaxis.set_major_formatter(FuncFormatter(ascii_minus_formatter)); ax.yaxis.set_major_formatter(FuncFormatter(ascii_minus_formatter))


                            # --- ADJUSTED LEGEND CREATION (t-SNE - Handles DBSCAN Noise) ---
                            # (Legend logic remains the same, using k_clusters_final_tsne, unique_str_labels_clusters_tsne, noise_indices, zombie_mask_to_plot_indices)
                            handles_list_tsne = []; labels_list_tsne = []
                            # Add handles for actual clusters found
                            if k_clusters_final_tsne > 0:
                                cmap_func_tsne = matplotlib.colormaps['viridis']
                                norm_func_tsne = plt.Normalize(vmin=0, vmax=max(0, k_clusters_final_tsne - 1))
                                for code_idx, label_str in enumerate(unique_str_labels_clusters_tsne): # Use cluster labels found above
                                    try: color_val = cmap_func_tsne(norm_func_tsne(code_idx))
                                    except ValueError: color_val = 'grey'
                                    handles_list_tsne.append(mlines.Line2D([],[], color=color_val, marker='o', ls='None', ms=4))
                                    labels_list_tsne.append(str(label_str))
                            # Add handle for Noise if DBSCAN and noise exists
                            if is_dbscan_plot and noise_indices.size > 0:
                                noise_handle = mlines.Line2D([], [], color='lightgrey', marker='x', linestyle='None', markersize=3)
                                handles_list_tsne.append(noise_handle)
                                labels_list_tsne.append("Noise")
                            # Add Zombie handle if needed
                            if include_zombie_overlay and zombie_mask_subset is not None and zombie_mask_to_plot_indices.size > 0:
                                zombie_handle = mlines.Line2D([], [], color='red', marker='o', ls='None', markersize=3, markerfacecolor='none', markeredgewidth=0.5)
                                handles_list_tsne.append(zombie_handle); labels_list_tsne.append(PUBLICATION_NAMES.get("Zombie", "Zombie"))
                            # Create legend
                            if handles_list_tsne:
                                legend_title = "Clusters" if not is_dbscan_plot else "Clusters & Noise"
                                legend_obj_tsne = ax.legend(handles=handles_list_tsne, labels=labels_list_tsne, loc='best', title=legend_title, title_fontsize=7, fontsize=6, frameon=True, facecolor='white', framealpha=0.6, edgecolor='black', fancybox=False)
                                if legend_obj_tsne: legend_obj_tsne.get_frame().set_linewidth(plt.rcParams['axes.linewidth']); legend_obj_tsne.get_title().set_horizontalalignment('left')
                            # --- END ADJUSTED LEGEND (t-SNE) ---

                        # --- Finalize and Save t-SNE plot ---
                        # ... (Saving code remains the same) ...
                        try: fig_tsne.tight_layout(rect=[0, 0.03, 1, 0.93])
                        except Exception as e_tl: print(f"Warning: tight_layout failed for t-SNE plot ({subset_suffix}): {e_tl}")
                        tsne_filename = f"clusters_{subset_suffix}_tsne_FINAL_{timestamp_str}.png"
                        tsne_save_path = os.path.join(output_dir_path, tsne_filename)
                        try: plt.savefig(tsne_save_path, dpi=600); print(f"t-SNE viz saved: {tsne_save_path}")
                        except Exception as e_save: print(f"Error saving t-SNE plot {tsne_save_path}: {e_save}")
                        plt.close(fig_tsne)


                    except MemoryError: print("ERROR: MemoryError during t-SNE."); plt.close(fig_tsne) if 'fig_tsne' in locals() else None
                    except Exception as e:
                        print("Error during t-SNE generation")
                        import traceback
                        print("--- t-SNE Error Traceback ---")
                        print(traceback.format_exc())
                        print("---------------------------")
                        if 'fig_tsne' in locals() and fig_tsne in plt.get_fignums(): plt.close(fig_tsne)
                else: print(f"Skipping t-SNE plot ({subset_suffix}): Not enough samples for perplexity ({n_samples_subset}).")
            else: print(f"Skipping t-SNE plot ({subset_suffix}) as per request.")
            # --- End of t-SNE Section ---

        else: # Corresponds to 'if n_samples_subset >= 2:'
            print(f"Skipping PCA/t-SNE viz ({subset_suffix}) (<2 samples).")
            print(f"Skipping PCA/t-SNE text report generation ({subset_suffix}) (<2 samples).")


        # --- Box Plots (Calls the grid function - Uses Corrected Helper) ---
        print(f"\n--- Generating Box Plot Grid ({subset_suffix} - Final Labels) ---")
        if not X_subset.empty and num_algos > 0:
            if 'plot_subset_boxplot_grid' in globals():
                # Pass the dictionary containing the FINAL STRING label Series
                plot_subset_boxplot_grid(
                    X_data_subset=X_subset, # Still plot original scale features
                    labels_dict=labels_plot_dict, # Pass dict with FINAL STRING label Series
                    cluster_counts=counts_plot_dict, # Pass dict with unique FINAL label counts
                    clustering_vars=clustering_vars,
                    subset_suffix=subset_suffix,
                    fig_width=width, fig_height=height,
                    output_dir_path=output_dir_path, timestamp_str=timestamp_str
                )
            else: print("WARN: Helper function 'plot_subset_boxplot_grid' not found.")
        elif X_subset.empty: print("Skipping box plot grid: Input X_subset is empty.")
        else: print("Skipping box plot grid: No valid clustering algorithms found.")
        # --- End Box Plots ---

    else: # Corresponds to 'if num_algos > 0 and not X_subset_scaled_df.empty:'
        print(f"Skipping plot generation for {subset_suffix} (no valid final clustering results or scaled data).")



    # --- Generate and Save FINAL Detailed Cluster Summaries (Using FINAL Labels) ---
    print(f"\n--- Generating FINAL Detailed Cluster Summaries ({subset_suffix} - Using Final Labels) ---")
    if not valid_algos_subset:
        print("Skipping FINAL detailed summaries (no valid final cluster results).")
        # Still return the df with raw labels even if final summary fails
        return df_subset_out

    summary_texts_final = []
    numeric_summary_cols_final = [c for c in feature_cols if c in df_subset_out.columns]
    industry_available_subset_final = industry_col_available and industry_col_name and industry_col_name in df_subset_out.columns
    categorical_vars_subset_final = [industry_col_name] if industry_available_subset_final else []
    baseline_dist_final = None
    baseline_col_final = industry_col_name if industry_available_subset_final else None
    if baseline_col_final:
        try: baseline_dist_final = df_subset_out[baseline_col_final].value_counts(normalize=True, dropna=False)
        except Exception: baseline_dist_final = None

    # Generate summary text for each algorithm using FINAL labels
    for algo_name_key in valid_algos_subset:
        # Use the FINAL string labels Series from labels_plot_dict
        final_labels_series = labels_plot_dict.get(algo_name_key)

        # Check if the series exists and has non-NA values
        if isinstance(final_labels_series, pd.Series) and final_labels_series.notna().any():
            # Ensure labels array passed to summary func has same length as df_subset_out
            if final_labels_series.size == len(df_subset_out):
                if 'generate_cluster_summary_text' in globals():                
                    final_summary = generate_cluster_summary_text(
                       df_subset_out, final_labels_series, # Pass df and FINAL labels series
                       f"{algo_name_key} (Final Mapped)", numeric_summary_cols_final, categorical_vars_subset_final,
                       target_col=target_col,
                       id_vars=id_vars,                         # <<< PASS id_vars >>>
                       company_static_info_df=company_static_info_df, # <<< PASS static df >>>
                       exit_status_col=exit_status_col,        # <<< PASS exit status col name >>>
                       baseline_col=baseline_col_final, baseline_dist_series=baseline_dist_final
                       )
                    
                    summary_texts_final.append(final_summary)
                else: summary_texts_final.append(f"Summary function missing for {algo_name_key} (Final)")
            else:
                summary_texts_final.append(f"Label size mismatch for {algo_name_key} (Final).")
                print(f"WARN: Final labels size ({final_labels_series.size}) mismatch with DataFrame ({len(df_subset_out)}) for {algo_name_key}. Skipping summary.")
        else: summary_texts_final.append(f"No valid final labels for {algo_name_key} to summarize.")

    # Save the FINAL summary
    if summary_texts_final:
        full_final_summary = "\n\n".join(s for s in summary_texts_final if s)
        if full_final_summary.strip():
            summary_filename_final = f"cluster_summary_FINAL_{subset_suffix}_{timestamp_str}.txt"
            full_summary_path_final = os.path.join(output_dir_path, summary_filename_final)
            try:
                with open(full_summary_path_final, "w", encoding='utf-8') as f: f.write(full_final_summary)
                print(f"FINAL cluster summary text saved to: {full_summary_path_final}")
            except Exception as e_save_sum: print(f"Error saving FINAL summary file {full_summary_path_final}: {e_save_sum}")
        else: print("No FINAL summary text generated to save.")

    # Return the dataframe with added raw AND final cluster labels
    print(f"====== Finished Clustering for Subset: {subset_suffix} ======")
    return df_subset_out
# --- End of run_clustering_analysis Helper Function ---

# <<< --- START: MODIFIED HELPER FUNCTION FOR SANKEY --- >>>

# def prepare_sankey_data(df_transitions, value_col='count'): # <<< OLD SIGNATURE
def prepare_sankey_data(df_transitions, cluster_col_name, value_col='count'): # <<< NEW SIGNATURE (added cluster_col_name)
    """
    Prepares data for a Plotly Sankey diagram from a transition DataFrame.

    Args:
        df_transitions (pd.DataFrame): DataFrame with columns 'year',
                                       'prev_cluster', cluster_col_name, and value_col
                                       representing the count/value of each transition.
                                       'year' represents the END year of the transition.
        cluster_col_name (str): The actual name of the column containing the
                                current cluster assignment (e.g., 'kmeans_cluster'). # <<< ADDED DOC
        value_col (str): Name of the column holding the flow value (count).

    Returns:
        tuple: (nodes, links)
               - nodes (dict): Dictionary containing 'label' list for Sankey nodes.
               - links (dict): Dictionary containing 'source', 'target', 'value' lists.
               Returns (None, None) if data preparation fails.
    """
    # <<< --- MODIFIED CHECK --- >>>
    required_cols = ['year', 'prev_cluster', cluster_col_name, value_col]
    if not all(col in df_transitions.columns for col in required_cols):
        print(f"ERROR (prepare_sankey_data): Input DataFrame missing required columns. Expected: {required_cols}, Found: {list(df_transitions.columns)}")
        return None, None
    # <<< --- END MODIFIED CHECK --- >>>

    if df_transitions.empty:
        print("WARN (prepare_sankey_data): Input DataFrame is empty.")
        return None, None

    # Create labels for nodes (e.g., "Type1_2008")
    df_transitions['source_label'] = df_transitions['prev_cluster'].astype(str) + '_' + (df_transitions['year'] - 1).astype(str)
    # <<< --- MODIFIED TARGET LABEL CREATION --- >>>
    df_transitions['target_label'] = df_transitions[cluster_col_name].astype(str) + '_' + df_transitions['year'].astype(str)
    # <<< --- END MODIFIED TARGET LABEL CREATION --- >>>


    # Get unique node labels that actually appear in the transitions
    all_node_labels = pd.unique(df_transitions[['source_label', 'target_label']].values.ravel('K'))
    all_node_labels = [label for label in all_node_labels if pd.notna(label)] # Remove potential NaNs

    if not all_node_labels:
        print("WARN (prepare_sankey_data): No valid node labels found.")
        return None, None

    # Create mapping from label to index
    label_to_id = {label: i for i, label in enumerate(all_node_labels)}

    # Map labels to source/target integer indices
    links_source = df_transitions['source_label'].map(label_to_id)
    links_target = df_transitions['target_label'].map(label_to_id)
    links_value = df_transitions[value_col]

    # Remove transitions where mapping failed (shouldn't happen if all_node_labels is correct)
    valid_links_mask = links_source.notna() & links_target.notna()
    if not valid_links_mask.all():
        print(f"WARN (prepare_sankey_data): Dropping {len(valid_links_mask) - valid_links_mask.sum()} links due to mapping issues.")
        links_source = links_source[valid_links_mask]
        links_target = links_target[valid_links_mask]
        links_value = links_value[valid_links_mask]

    nodes = {'label': all_node_labels}
    links = {
        'source': links_source.tolist(),
        'target': links_target.tolist(),
        'value': links_value.tolist()
    }

    # Optional: Add color mapping based on cluster type here if desired
    # E.g., extract type from label, assign color from a predefined dict

    return nodes, links

# ==============================================================
# --- Data Definition for Interest Rates ONLY ---
# ==============================================================
# Note: Values are converted from percentages to decimals (e.g., 5.228% -> 0.05228)

interest_rate_data_dict = {
    2000: 0.05228, 2001: 0.05247, 2002: 0.04568, 2003: 0.03798, 2004: 0.03452,
    2005: 0.03408, 2006: 0.04123, 2007: 0.04910, 2008: 0.05393, 2009: 0.02762,
    2010: 0.02323, 2011: 0.03285, 2012: 0.03626, 2013: 0.03485, 2014: 0.03058,
    2015: 0.02173, 2016: 0.01736, 2017: 0.01541, 2018: 0.01493, 2019: 0.01376,
    2020: 0.01220, 2021: 0.01145, 2022: 0.01762, 2023: 0.04824, 2024: 0.05080
}

# --- Create DataFrame for Interest Rates ---
try:
    df_interest = pd.DataFrame(list(interest_rate_data_dict.items()), columns=['year', 'corp_loan_rate'])
    # We only have interest rate data now
    external_data_available = True # Set to True if interest rate data is present
    print("Created DataFrame for Interest Rate data from dictionary.")
except Exception as e_create_df:
    print(f"ERROR creating DataFrame from interest rate dictionary: {e_create_df}. Skipping overlay.")
    df_interest = pd.DataFrame(columns=['year', 'corp_loan_rate']) # Ensure empty DF exists
    external_data_available = False

###############################################################################
# PART B: PANEL LOGISTIC REGRESSION (FE/RE)
###############################################################################

# Check if data preparation was successful before attempting models
if panel_data_available and df_analysis_panel is not None:

# --- Population-Averaged Model: GEE Logit ---
    print("\n=== Fitting Population-Averaged Logit Model (GEE) ===")
    try:
        # Define model components
        family = Binomial()
        cov_struct = sm_cov_struct.Independence()
        print(f"Using GEE formula: {gee_panel_formula}") # <<< THIS LINE NOW PRINTS YOUR *NEW* FORMULA
        print(f"Using Covariance Structure: {type(cov_struct).__name__}")
    
        # Define the GEE model (uses the updated formula variable)
        gee_model = gee.GEE.from_formula(gee_panel_formula, # <<< IT USES YOUR NEW FORMULA HERE
                                         groups=df_analysis_panel['firm_id'],
                                         data=df_analysis_panel,
                                         family=family,
                                         cov_struct=cov_struct)
        # Fit the model
        gee_results = gee_model.fit(cov_type='robust', maxiter=1000) # <<< THE FIT USES THE MODEL DEFINED WITH THE NEW FORMULA
        print("\nGEE Logit Results Summary:\n", gee_results.summary()) # <<< THE SUMMARY WILL REFLECT THE NEW MODEL


        # Optional: Save summary to file
        try:
            txt_filename = f"gee_logit_summary_{timestamp_str}.txt"
            full_txt_path = os.path.join(output_dir_path, txt_filename)
            with open(full_txt_path, "w") as f:
                f.write(gee_results.summary().as_text())
            print(f"GEE Logit summary saved to {full_txt_path}")
        except Exception as e_save:
            print(f"Could not save GEE Logit summary: {e_save}")

    # This except block catches errors during GEE fitting
    except Exception as e_gee:
        print(f"Could not fit GEE Logit model: {e_gee}")
        # Add specific check for convergence warning if needed after fit attempt fails
        if 'IterationLimitWarning' in str(e_gee):
             print("  Note: Model likely failed due to not converging. Consider further increasing maxiter or checking data.")

    # --- Fixed Effects (FE) Logit Model ---  
    print("\n=== Fitting Fixed Effects Logit Model (ConditionalLogit) ===")

    # --- ADD CHECK FOR VARIATION ---
    # Check this *before* trying to fit the model
    if panel_data_available and df_analysis_panel is not None and 'firm_id' in df_analysis_panel.columns and target_col in df_analysis_panel.columns:
        print("   Checking within-group variance before FE model fit...")
        try: # Add try-except around check in case of unexpected error during check
            variation_check = df_analysis_panel.groupby('firm_id')[target_col].nunique()
            firms_with_variation = (variation_check > 1).sum()
            total_firms_in_panel_df = variation_check.shape[0]
            print(f"   Total firms in df_analysis_panel for FE: {total_firms_in_panel_df}")
            print(f"   Firms with within-group variation in '{target_col}': {firms_with_variation}")
            if firms_with_variation == 0:
                 print("   ERROR: No firms remaining have within-group variation in the target. FE model cannot be estimated.")
                 # You could add: panel_data_available = False here if you want subsequent steps to know FE failed definitively
        except Exception as e_check:
            print(f"   Warning: Could not perform within-group variance check: {e_check}")
    else:
        print("   Skipping within-group variance check (panel data or columns missing).")
    # --- END CHECK ---

    # --- Try Fitting the Model ---
    # This entire try...except block replaces the original FE try...except block
    try:
        # Ensure panel data is still considered available before proceeding inside try
        if not panel_data_available or df_analysis_panel is None:
             raise RuntimeError("Panel data not available for FE model fitting.")

        # Ensure firms_with_variation exists and is > 0 before proceeding (optional extra check)
        if 'firms_with_variation' not in locals() or firms_with_variation <= 0:
            print("   Skipping FE fit: No firms with within-group variation identified.")
            raise RuntimeError("Cannot fit FE model with no within-group variation.") # Stop this try block

        # FE formula uses unscaled features and excludes intercept (' - 1')
        print(f"Using FE formula (no intercept): {fe_panel_formula}")

        # Use ConditionalLogit class directly
        fe_model = ConditionalLogit.from_formula(fe_panel_formula,
                                                 data=df_analysis_panel, # Uses renamed, unscaled data
                                                 groups=df_analysis_panel['firm_id'])

        # --- MODIFIED FIT PROCESS ---
        print("Attempting FE Logit fit WITHOUT robust standard errors...")
        # Fit the model (remove cov_type='robust')
        fe_results = fe_model.fit(method='newton') # <<< Fit without robust SEs first
        print("\nFixed Effects Logit Results Summary (Non-Robust SEs):\n", fe_results.summary())
        # Optionally save non-robust results if needed (ensure f-string)
        try:
            txt_filename_nr = f"fe_logit_summary_nonrobust_{timestamp_str}.txt"
            full_txt_path_nr = os.path.join(output_dir_path, txt_filename_nr)
            with open(full_txt_path_nr, "w") as f:
                f.write(fe_results.summary().as_text())
            print(f"FE Logit summary (Non-Robust) saved to {full_txt_path_nr}")
        except Exception as e_save:
            print(f"Could not save FE Logit summary (Non-Robust): {e_save}")


        # Optionally, try robust SEs again separately if the first fit succeeds
        print("\nAttempting robust standard errors separately...")
        fe_results_robust = fe_model.fit(method='newton', cov_type='robust') # <<< Now try with robust SEs
        print("\nFixed Effects Logit Results Summary (Robust SEs):\n", fe_results_robust.summary())

        # Save the robust summary if successful (ensure f-string)
        try:
           txt_filename_r = f"fe_logit_summary_robust_{timestamp_str}.txt"
           full_txt_path_r = os.path.join(output_dir_path, txt_filename_r)
           with open(full_txt_path_r, "w") as f:
               f.write(fe_results_robust.summary().as_text())
           print(f"FE Logit summary (Robust) saved to {full_txt_path_r}")
        except Exception as e_save:
            print(f"Could not save FE Logit summary (Robust): {e_save}")
        # --- END MODIFIED FIT PROCESS ---

    except NameError: # Catch specific error if ConditionalLogit class wasn't imported
         print("ERROR: ConditionalLogit class not found. Check import statement.")
         print("        It should be: from statsmodels.discrete.conditional_models import ConditionalLogit")
         # No traceback needed here, it's an import error
    except Exception as e_fe: # Catch all other errors during model definition or fitting
         print(f"Could not fit Fixed Effects Logit model: {e_fe}")
         print("  (Note: This often fails if there's insufficient within-group variation for some firms or issues with robust SE calculation)")
         # --- ADD TRACEBACK PRINT HERE ---
         import traceback
         print("\nFull Traceback for FE fit error:")
         print(traceback.format_exc()) # Print detailed error
         # --- END TRACEBACK PRINT ---
     # <<< END REPLACEMENT HERE >>>

# This else corresponds to the 'if panel_data_available...' at the start of PART B
else:
    print("\nSkipping FE/RE Panel Logistic Regression models (panel data not available).")

# --- End of PART B ---

###############################################################################
# PART C: PIPELINE + GRIDSEARCHCV + ROC PLOT (Scikit-learn)
###############################################################################
print("\n=== Fitting Logit with L1 via GridSearchCV (Scikit-learn) ===")
# Use X and y which should be the final, potentially trimmed versions
if 'X' in locals() and not X.empty and 'y' in locals() and not y.empty and final_rows > 0:

    # <<< --- START: MODIFICATIONS FOR LASSO --- >>>
    print("Preparing features for Lasso (incl. dummy and interactions)...")

    # 1. Define columns for Lasso: Base features + Dummy
    lasso_feature_cols = feature_cols + ['Is_Manufacturing']
    # Ensure all these columns actually exist in df_analysis before selecting
    lasso_feature_cols = [col for col in lasso_feature_cols if col in df_analysis.columns]

    if 'Is_Manufacturing' not in lasso_feature_cols:
         print("WARNING: 'Is_Manufacturing' column not found in df_analysis for Part C. Lasso will not include it.")
         X_lasso = X[feature_cols].copy() # Fallback to original features
    else:
         X_lasso = df_analysis[lasso_feature_cols].copy() # Select features AND dummy

    # 2. (Optional) Create Interaction Terms Manually in X_lasso
    if 'Is_Manufacturing' in X_lasso.columns: # Only proceed if dummy is present
        # --- Example Interactions ---
        features_to_interact_c = ['Log_Total_Assets', 'Total_Leverage'] # Choose features from X_lasso.columns
        print("  Creating interaction terms for Part C...")
        for feature in features_to_interact_c:
            if feature in X_lasso.columns:
                interaction_col_name = f"{feature}_x_Mfg"
                X_lasso[interaction_col_name] = X_lasso[feature] * X_lasso['Is_Manufacturing']
                print(f"    Created interaction: {interaction_col_name}")
            else:
                print(f"    Warning: Feature '{feature}' not in X_lasso for interaction.")
        # --- End Example Interactions ---
        print(f"  Final columns for Lasso (Part C): {list(X_lasso.columns)}")
    # <<< --- END: MODIFICATIONS FOR LASSO --- >>>

    # Define pipeline (The scaler step will now process X_lasso)
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('logreg', LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42))
    ])
    # Define parameter grid for C (inverse of regularization strength)
    param_grid = {'logreg__C': np.logspace(-3, 2, 6)} # Test 0.001, 0.01, ..., 100
    # Use Stratified K-Fold for cross-validation
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # Consistent folds
    # Perform Grid Search
    grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='roc_auc', n_jobs=-1, error_score='raise') # Raise error if CV fails

    try:
        grid_search.fit(X, y) # Fit on final X, y
        print(f"\nBest params (GridSearchCV): {grid_search.best_params_}")
        print(f"Best CV ROC AUC score: {grid_search.best_score_:.4f}")

        # Evaluate best model on the whole training data (for illustration)
        best_model_pipeline = grid_search.best_estimator_
        y_pred = best_model_pipeline.predict(X)
        y_prob = best_model_pipeline.predict_proba(X)[:, 1]

        roc_auc = roc_auc_score(y, y_prob)
        precision = precision_score(y, y_pred, zero_division=0)
        recall = recall_score(y, y_pred, zero_division=0)
        f1 = f1_score(y, y_pred, zero_division=0)
        cm = confusion_matrix(y, y_pred)

        print("\nMetrics (Best Model on training data):")
        print(f"  ROC AUC: {roc_auc:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print(f"  F1-Score: {f1:.4f}")
        print("Confusion Matrix:\n", cm)

        # --- ROC Curve Plot ---
        print("\nGenerating ROC Curve plot...")
        # Create Figure and Axes using predefined width/height
        fig_roc, ax_roc = plt.subplots(figsize=(width, height))

        # Calculate ROC curve values
        fpr, tpr, _ = roc_curve(y, y_prob)
        # Plot the ROC curve, assign to variable for legend handle
        line1, = ax_roc.plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.2f})")

        # Plot the baseline random guess line, assign to variable
        line2, = ax_roc.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Random Guess (AUC = 0.50)', alpha=0.8)

        # --- Styling ---
        # Rely on global rcParams for spines, tick directions, base font sizes etc.
        # Explicitly turn grid off if not default
        ax_roc.grid(False)
        # Set labels and title (font sizes will come from rcParams or style)
        ax_roc.set_xlabel("False Positive Rate")
        ax_roc.set_ylabel("True Positive Rate")
        ax_roc.set_title(f"{PUBLICATION_NAMES['ROC']} Curve")

        # --- Create Legend with Modifications ---
        ax_roc.legend(
            handles=[line1, line2], # Use handles for robustness
            loc="lower right",   # Location
            frameon=True,        # Draw the box/frame (can be overridden by style)
            edgecolor='black',   # Set box outline color to black (if frameon=True)
            fancybox=False,      # Use square corners for the box
            fontsize=6,          # Set a smaller font size (adjust '6' if needed)
            #linewidth=0.5 # Might be controlled by style
        )
        # --- End Legend ---

        # Adjust layout to prevent overlap
        plt.tight_layout()

        # Save the figure
        try:
            png_filename = f"roc_curve_{timestamp_str}.png"
            full_png_path = os.path.join(output_dir_path, png_filename)
            plt.savefig(full_png_path, dpi=600)
            print(f"ROC curve saved to {full_png_path}")
        except Exception as e_save:
            print(f"Error saving ROC plot: {e_save}")

        # Close the figure to free memory
        plt.close(fig_roc)
        # --- End ROC Curve Plot ---

    except Exception as e_grid:
        print(f"Could not perform GridSearchCV: {e_grid}")
else:
    print("Skipping Sklearn Pipeline/GridSearchCV (X/y empty or no valid rows).")
    
###############################################################################
# PART D: LASSO REGULARIZATION (LOGISTICREGRESSIONCV)
###############################################################################
print("\n=== Fitting Logit with L1 via LogisticRegressionCV ===")

# Use final X, y (potentially winsorized)
if 'X' in locals() and not X.empty and 'y' in locals() and not y.empty and final_rows > 0:

    # <<< --- START: MODIFICATIONS FOR LASSO --- >>>
    print("Preparing features for Lasso CV (incl. dummy and interactions)...")

    # 1. Define columns for Lasso: Base features + Dummy
    lasso_feature_cols_d = feature_cols + ['Is_Manufacturing']
    # Ensure all these columns actually exist in df_analysis before selecting
    lasso_feature_cols_d = [col for col in lasso_feature_cols_d if col in df_analysis.columns]

    if 'Is_Manufacturing' not in lasso_feature_cols_d:
         print("WARNING: 'Is_Manufacturing' column not found in df_analysis for Part D. Lasso CV will not include it.")
         X_lasso_cv = X[feature_cols].copy() # Fallback to original features
    else:
         X_lasso_cv = df_analysis[lasso_feature_cols_d].copy() # Select features AND dummy

    # 2. (Optional) Create Interaction Terms Manually in X_lasso_cv
    if 'Is_Manufacturing' in X_lasso_cv.columns: # Only proceed if dummy is present
        # --- Example Interactions ---
        features_to_interact_d = ['Log_Total_Assets', 'Total_Leverage'] # Choose features from X_lasso_cv.columns
        print("  Creating interaction terms for Part D...")
        for feature in features_to_interact_d:
            if feature in X_lasso_cv.columns:
                interaction_col_name = f"{feature}_x_Mfg"
                X_lasso_cv[interaction_col_name] = X_lasso_cv[feature] * X_lasso_cv['Is_Manufacturing']
                print(f"    Created interaction: {interaction_col_name}")
            else:
                print(f"    Warning: Feature '{feature}' not in X_lasso_cv for interaction.")
        # --- End Example Interactions ---
        print(f"  Final columns for Lasso CV (Part D): {list(X_lasso_cv.columns)}")
    # <<< --- END: MODIFICATIONS FOR LASSO --- >>>


    scaler_lasso = StandardScaler() # Use standard scaler for consistency
    X_scaled_lasso = scaler_lasso.fit_transform(X)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # Consistent CV

    # LogisticRegressionCV automatically tunes C
    lasso_cv_model = LogisticRegressionCV(
        Cs=10, # Number of C values to test on a logarithmic scale
        cv=skf,
        penalty='l1',
        solver='liblinear',
        scoring='roc_auc',
        max_iter=1000,
        random_state=42,
        n_jobs=-1,
        # Added options for robustness
        tol=1e-4, # Default tolerance
        # class_weight='balanced' # Consider if classes are imbalanced
    )
    try:
        lasso_cv_model.fit(X_scaled_lasso, y)
        print(f"\nBest C chosen by LogisticRegressionCV: {lasso_cv_model.C_[0]:.4f}")

        # Accessing scores correctly - scores_ gives dict {class: fold_scores}
        # Assuming binary classification, class 1 is usually the positive class
        if 1 in lasso_cv_model.scores_:
             mean_score = np.mean(lasso_cv_model.scores_[1]) # Mean score for class 1 across folds for the best C
             print(f"Mean ROC AUC across folds for best C (for class 1): {mean_score:.4f}")
        else:
             print("Could not retrieve scores for class 1.")

        # Coefficients correspond to the scaled data
        lasso_coef = pd.Series(lasso_cv_model.coef_[0], index=X.columns)
        lasso_results = pd.DataFrame(lasso_coef, columns=['Coefficient (Scaled Data)'])
        print("\nLASSO Coefficients (from LogisticRegressionCV):\n", lasso_results)

        excel_filename = f"lasso_cv_coefficients_{timestamp_str}.xlsx"
        full_excel_path = os.path.join(output_dir_path, excel_filename)
        lasso_results.to_excel(full_excel_path)
        print(f"\nLASSO coefficients saved to {full_excel_path}")
    except Exception as e_lasso_cv:
        print(f"Could not fit LogisticRegressionCV: {e_lasso_cv}")
else:
    print("Skipping LogisticRegressionCV (X/y empty or no valid rows).")

# --- Block To Add ---

# ==============================================================
# === USER-DEFINED CLUSTER MAPPINGS (UNIVERSAL TYPES) ===
# ==============================================================
# Mapping raw cluster IDs to consistent "Type" labels across sectors
# and algorithms (KMeans/Hierarchical).
#
# !!! IMPORTANT !!! You MUST define these Types clearly in your paper's
# text or figure captions based on the characteristics observed in the
# INITIAL cluster summary files. Ensure the definitions match the data.

# --- Universal Type Definitions (KMeans / Hierarchical) ---
# Type 1: Low-Leverage Zombies (LLZ)
# Type 2: Deep Operational Loss Zombies (DOLZ)
# Type 3: Highly Leveraged Zombies (HLZ) - Primarily Mfg
# Type 4: Extreme Leverage Zombies (ELZ)
# Type 5: High Liquidity Zombies (HLiZ) - Non-Mfg Only
# Type 6: High EBITDA Zombies (HEZ) - Non-Mfg Only
# Note: Not all types may appear in both sectors.

# --- DBSCAN Type Definitions (Separate due to different structure) ---
# Type A (DBSCAN): Core group (verify characteristics, e.g., mostly LLZ?)
# Type B (DBSCAN): Secondary group (verify characteristics)
# Noise (DBSCAN): Outliers / Non-Clustered Points

USER_CLUSTER_MAPPINGS = {
    "Mfg": {
        # KMEANS MAPPING TO UNIVERSAL TYPES (Reference):
        # Updated based on requested changes
        "KMeans":        {
                            # Raw ID 2 was Type 1, now maps to Type 3
                            2: "Type 3",
                            # Raw ID 3 was Type 2, stays maps to Type 2
                            3: "Type 2",
                            # Raw ID 0 was Type 3, now maps to Type 4
                            0: "Type 4",
                            # Raw ID 1 was Type 4, now maps to Type 1
                            1: "Type 1"
                         },
        # HIERARCHICAL MAPPING TO UNIVERSAL TYPES (Updated based on similarity to KMeans):
        # Mapping based on rule: Hierarchical Cluster with current Type X now maps to New Type Y
        "Hierarchical": {
                            # Hierarchical Raw ID 3 was Type 1, now maps to Type 3
                            3: "Type 3",
                            # Hierarchical Raw ID 1 was Type 2, stays maps to Type 2
                            1: "Type 2",
                            # Hierarchical Raw ID 0 was Type 3, now maps to Type 4
                            0: "Type 4",
                            # Hierarchical Raw ID 2 was Type 4, now maps to Type 1
                            2: "Type 1"
                         },
        # DBSCAN mapping remains distinct
        "DBSCAN":        {
                            0: "Type A", # Core cluster(s) - may align with one or more KMeans/Hierarchical types
                            1: "Type B", # Specific outlier group(s) found by DBSCAN
                            -1: "Noise"  # Noise points
                         }
    },
    "Non_Mfg": {
        # KMEANS MAPPING (Reference):
        # Updated based on requested changes
        "KMeans": {
            # Raw ID 0 was Type 5, now maps to Type 6
            0: "Type 6",
            # Raw ID 4 was Type 6, now maps to Type 8
            4: "Type 8",
            # Raw ID 1 was Type 7, stays maps to Type 7
            1: "Type 7",
            # Raw ID 2 was Type 8, now maps to Type 5
            2: "Type 5",
            # Raw ID 3 was Type 9, stays maps to Type 9
            3: "Type 9"
        },
        # HIERARCHICAL MAPPING (Updated based on requested changes):
        # Mapping based on rule: Hierarchical Cluster with current Type X now maps to New Type Y
        "Hierarchical": {
             # Hierarchical Raw ID 2 was Type 5, now maps to Type 6
            2: "Type 6",
             # Hierarchical Raw ID 3 is similar to KMeans Type 2 (Raw 4), was Type 6, now maps to Type 8
            3: "Type 8",
              # Hierarchical Raw ID 0 is similar to KMeans Type 4 (Raw 1), was Type 7, stays maps to Type 7
            0: "Type 7",
             # Hierarchical Raw ID 1 is similar to KMeans Type 5 (Raw 2), was Type 8, now maps to Type 5
            1: "Type 5",
             # Hierarchical Raw ID 4 is similar to KMeans Type 6 (Raw 3), was Type 9, stays maps to Type 9
            4: "Type 9"
        },
        # DBSCAN MAPPING (Remains distinct)
        "DBSCAN": {
            0: "Type C", # Core cluster(s)
            1: "Type D", # Specific outlier group(s) found by DBSCAN
            -1: "Noise"  # Noise points
        }
    }
    # Add mappings for other potential subsets if needed, following a similar comparison logic.
}


print("USER_CLUSTER_MAPPINGS dictionary defined (using UNIVERSAL Type numbering for KMeans/Hierarchical).")
print("!!! REMINDER: Define Type 1-6, Type A, Type B, Noise clearly in your text/captions based on INITIAL summaries !!!")


###############################################################################
# PART E: CLUSTERING ANALYSIS (REFACTORED FOR SECTOR SPLIT)
###############################################################################
print("\n\n====== PART E: Clustering Analysis (Zombie Firms Only - Split by Sector) ======")

# --- Define clustering variables based on final features from Part A --- ### <--- FIX: DEFINE HERE ###
# Ensure feature_cols exists from Part A
if 'feature_cols' in locals() and isinstance(feature_cols, list):
    clustering_vars = feature_cols
    print(f"\nUsing clustering variables for Part E: {clustering_vars}")
else:
    print("CRITICAL ERROR: 'feature_cols' not defined before Part E. Cannot define 'clustering_vars'.")
    # Handle error appropriately - maybe define a default list or exit
    # Example: feature_cols = ["Log_Age_Plus_1", "Log_Total_Assets", ...] # Define fallback
    sys.exit("Stopping script because feature_cols is missing.")

# --- Create Zombie Subset ---
# Uses df_analysis which should exist from Part A
df_zombie_subset = None # Initialize
X_zombie = None         # Initialize

if 'df_analysis' in locals() and not df_analysis.empty and target_col in df_analysis.columns and final_rows > 0:
    # Ensure X is aligned with the potentially trimmed df_analysis index
    if 'X' in locals() and not X.empty and X.index.equals(df_analysis.index):
        df_analysis_zombies_only = df_analysis[df_analysis[target_col] == 1].copy()
        if not df_analysis_zombies_only.empty:
            print(f"Found {len(df_analysis_zombies_only)} zombie firm-year observations.")
            # Select corresponding feature rows using the index from the subset
            df_zombie_subset = df_analysis_zombies_only # Already has correct index
            # --- FIX: Ensure X_zombie uses the correct `clustering_vars` ---
            if all(v in X.columns for v in clustering_vars):
                 X_zombie = X.loc[df_zombie_subset.index, clustering_vars] # Select matching rows AND correct columns
                 print(f"Aligned X_zombie shape: {X_zombie.shape}")
            else:
                 missing_for_zombie = [v for v in clustering_vars if v not in X.columns]
                 print(f"ERROR: Cannot create X_zombie. Clustering variables missing from X: {missing_for_zombie}")
                 X_zombie = pd.DataFrame(columns=clustering_vars) # Assign empty df
        else:
            print("No zombie firms found in df_analysis.")
            X_zombie = pd.DataFrame(columns=clustering_vars) # Assign empty df
    else:
        print("Warning: Feature matrix X is missing, empty, or not aligned with df_analysis.")
        X_zombie = pd.DataFrame(columns=clustering_vars) # Assign empty df
else:
    print("Skipping Zombie Analysis Prep: df_analysis missing, empty, target missing, or no final rows.")
    X_zombie = pd.DataFrame(columns=clustering_vars if 'clustering_vars' in locals() else feature_cols) # Assign empty df

# Ensure dataframes are properly initialized if subset creation failed
if df_zombie_subset is None: df_zombie_subset = pd.DataFrame()
if X_zombie is None: X_zombie = pd.DataFrame(columns=clustering_vars if 'clustering_vars' in locals() else feature_cols)
# --- End Subset Creation ---


# --- Start Main Block for Zombie Clustering (if data exists) ---
if X_zombie.empty:
    print("Skipping clustering (Zombie Firms) - No zombie data.")
# Check if sample size is sufficient *before* trying to split
elif X_zombie.shape[0] < 3: # Need at least 3 samples for meaningful clustering
    print(f"Skipping clustering (Zombie Firms) - Only {X_zombie.shape[0]} sample(s).")
else:
    # --- Initialize Output DataFrames ---
    # These will store the results if the split is successful
    df_zombie_mfg = pd.DataFrame()
    df_zombie_nonmfg = pd.DataFrame()

    # Check if industry analysis is possible FOR ZOMBIES
    if industry_col_available and industry_col_name in df_zombie_subset.columns and df_zombie_subset[industry_col_name].notna().any():
        print(f"\nIndustry column '{industry_col_name}' found in zombie subset. Splitting zombie clustering analysis.")

        # --- Create Zombie Subsets by Sector ---
        # Ensure 'Sector Type' column exists (might have been added in E.1)
        if 'Sector Type' not in df_zombie_subset.columns:
            print("WARN: 'Sector Type' column missing in zombie subset, attempting to add.")
            # Use the lists defined in Part A
            df_zombie_subset['Sector Type'] = df_zombie_subset[industry_col_name].apply(
                    lambda x: 'Mfg' if x in mfg_sectors else ('NonMfg' if x in nonmfg_sectors else 'Other')
            )
        print("\nSector Type Counts in df_zombie_subset:")
        print(df_zombie_subset['Sector Type'].value_counts())

        df_zombie_mfg_tmp = df_zombie_subset[df_zombie_subset['Sector Type'] == 'Mfg']
        df_zombie_nonmfg_tmp = df_zombie_subset[df_zombie_subset['Sector Type'] == 'NonMfg']
        # --- FIX: Get corresponding feature matrices using clustering_vars ---
        X_zombie_mfg = X_zombie.loc[X_zombie.index.intersection(df_zombie_mfg_tmp.index), clustering_vars]
        X_zombie_nonmfg = X_zombie.loc[X_zombie.index.intersection(df_zombie_nonmfg_tmp.index), clustering_vars]
        print(f"Zombie Mfg subset shape for clustering: {X_zombie_mfg.shape}")
        print(f"Zombie NonMfg subset shape for clustering: {X_zombie_nonmfg.shape}")


        # --- Run for Zombie Manufacturing Subset ---
        print("\n" + "*"*63)
        print("* Running Clustering for ZOMBIE MANUFACTURING Subset         *")
        print("* NOTE: Examine k-distance plot saved for 'Zombie_Mfg'       *")
        print("* and MANUALLY set 'dbscan_eps_range' below for best results *")
        print("*"*63)

        # --- >>> MANUALLY ADJUST DBSCAN PARAMS FOR ZOMBIE MFG HERE <<< ---
        zombie_mfg_eps_range = np.linspace(1.7, 2.7, 12) # Test 12 eps values from 1.7 to 2.7
        print(f"INFO: Manually set eps range for Zombie_Mfg: {np.round(zombie_mfg_eps_range, 2)}")
        zombie_mfg_min_samples = 10
        print(f"INFO: Using min_samples = {zombie_mfg_min_samples} for Zombie_Mfg")
        # --- END MANUAL ADJUSTMENT ---

        # --- Now call the function with the manually set range ---
        # <<< The call below should now work as clustering_vars is defined >>>
        df_zombie_mfg = run_clustering_analysis( # Assign result back
                df_subset=df_zombie_mfg_tmp.copy(),
                X_subset=X_zombie_mfg,
                subset_suffix="Zombie_Mfg",
                clustering_vars=clustering_vars,
                feature_cols=feature_cols,
                target_col=target_col,
                industry_col_name=industry_col_name,
                industry_col_available=True,
                id_vars=id_vars,                         # <<< ADD THIS LINE
                company_static_info_df=df_static_info_for_merge, # <<< ADD THIS LINE (using the df created in Part A)
                output_dir_path=output_dir_path,
                timestamp_str=timestamp_str,
                width=width,
                height=height,
                dbscan_eps_range=zombie_mfg_eps_range,
                dbscan_min_samples=zombie_mfg_min_samples,
                perform_tsne=True,
                include_zombie_overlay=False,
                kmeans_k_min=3,
                kmeans_k_max=8
                # exit_status_col='ExitStatus' # Optional: can add if you renamed it, otherwise defaults
                )
            # --- End Function Call for Zombie Mfg ---

        # --- Run for Zombie Non-Manufacturing Subset ---
        print("\n" + "*"*63)
        print("* Running Clustering for ZOMBIE NON-MANUFACTURING Subset    *")
        print("* NOTE: Examine k-distance plot saved for 'Zombie_NonMfg'   *")
        print("* and MANUALLY set 'dbscan_eps_range' below for best results *")
        print("*"*63)

        # --- >>> MANUALLY ADJUST DBSCAN PARAMS FOR ZOMBIE NONMFG HERE <<< ---
        zombie_nonmfg_eps_range = np.linspace(1.7, 3.0, 13) # Example: Test 13 eps values from 1.7 to 3.0
        print(f"INFO: Manually set eps range for Zombie_NonMfg: {np.round(zombie_nonmfg_eps_range, 2)}")
        zombie_nonmfg_min_samples = 10  # Or try 8, 5, etc.
        print(f"INFO: Using min_samples = {zombie_nonmfg_min_samples} for Zombie_NonMfg")
        # --- END MANUAL ADJUSTMENT ---

        # --- Run for Zombie Non-Manufacturing Subset ---
        # <<< The call below should now work as clustering_vars is defined >>>
        df_zombie_nonmfg = run_clustering_analysis( # Assign result back
            df_subset=df_zombie_nonmfg_tmp.copy(),
            X_subset=X_zombie_nonmfg,
            subset_suffix="Zombie_NonMfg",
            clustering_vars=clustering_vars,
            feature_cols=feature_cols,
            target_col=target_col,
            industry_col_name=industry_col_name,
            industry_col_available=True,
            id_vars=id_vars,                         # <<< ADD THIS LINE
            company_static_info_df=df_static_info_for_merge, # <<< ADD THIS LINE
            output_dir_path=output_dir_path,
            timestamp_str=timestamp_str,
            width=width,
            height=height,
            dbscan_eps_range=zombie_nonmfg_eps_range,
            dbscan_min_samples=zombie_nonmfg_min_samples,
            perform_tsne=True,
            include_zombie_overlay=False,
            kmeans_k_min=3,
            kmeans_k_max=8
            # exit_status_col='ExitStatus' # Optional
            )
            # --- End Function Call ---

        # --- Combine Zombie Cluster Results (Optional) ---
        # ... (rest of the combination logic remains the same) ...
        print("\nCombining Zombie Cluster Results...")
        dfs_zombie_concat = []
        if not df_zombie_mfg.empty: dfs_zombie_concat.append(df_zombie_mfg)
        if not df_zombie_nonmfg.empty: dfs_zombie_concat.append(df_zombie_nonmfg)

        if 'Sector Type' in df_zombie_subset.columns:
            df_zombie_other = df_zombie_subset[df_zombie_subset['Sector Type'] == 'Other'].copy()
            if not df_zombie_other.empty:
                for col in ['kmeans_cluster', 'hier_cluster', 'dbscan_cluster', 'kmeans_cluster_raw', 'hier_cluster_raw', 'dbscan_cluster_raw']:
                    if col not in df_zombie_other.columns:
                        if '_raw' in col:
                             df_zombie_other[col] = pd.NA
                             df_zombie_other[col] = df_zombie_other[col].astype('Int64')
                        else:
                             df_zombie_other[col] = pd.NA
                             df_zombie_other[col] = df_zombie_other[col].astype('string')
                dfs_zombie_concat.append(df_zombie_other)
        else:
            print("WARN: 'Sector Type' column not found in df_zombie_subset; cannot isolate 'Other' sector zombies.")

        if dfs_zombie_concat:
             try:
                 df_zombie_clustered_combined = pd.concat(dfs_zombie_concat, axis=0).sort_index()
                 print("Created combined DataFrame 'df_zombie_clustered_combined' with zombie cluster results.")
             except Exception as e_concat_save:
                 print(f"Error combining or saving zombie cluster results: {e_concat_save}")
                 df_zombie_clustered_combined = pd.DataFrame()
        else:
            print("No cluster results from zombie subsets (Mfg/NonMfg/Other) to combine.")
            df_zombie_clustered_combined = pd.DataFrame()

    else: # Fallback if industry split is not possible
        print("\nIndustry column not available or unusable in zombie subset.")
        print("Skipping Manufacturing/Non-Manufacturing split for zombie clustering.")
        print("\nRunning clustering on FULL ZOMBIE dataset as fallback...")
        # --- >>> MANUALLY ADJUST DBSCAN PARAMS FOR ZOMBIE FALLBACK HERE <<< ---
        zombie_fallback_eps_range = "auto"
        zombie_fallback_min_samples = 16
        # ---
        # <<< The call below should now work as clustering_vars is defined >>>
        df_zombie_subset = run_clustering_analysis(
            df_subset=df_zombie_subset,
            X_subset=X_zombie,
            subset_suffix="Zombie_Full_Fallback",
            clustering_vars=clustering_vars,
            feature_cols=feature_cols,
            target_col=target_col,
            industry_col_name=industry_col_name,
            industry_col_available=False, # Set correctly for fallback
            id_vars=id_vars,                         # <<< ADD THIS LINE
            company_static_info_df=df_static_info_for_merge, # <<< ADD THIS LINE
            output_dir_path=output_dir_path,
            timestamp_str=timestamp_str,
            width=width,
            height=height,
            dbscan_eps_range=zombie_fallback_eps_range,
            dbscan_min_samples=zombie_fallback_min_samples,
            perform_tsne=True,
            include_zombie_overlay=False,
            kmeans_k_min=3,
            kmeans_k_max=8
            # exit_status_col='ExitStatus' # Optional
            )
# --- End of main block for Zombie Clustering ---

# --- End of PART E ---

###############################################################################
# PART H: CLUSTER TRANSITION ANALYSIS (Zombie Firms - Split by Sector - Probabilities Only)
###############################################################################
print("\n\n====== PART H: Cluster Transition Analysis (Zombie Firms - Split by Sector - Probabilities Only) ======")

# Check if the combined zombie cluster results DataFrame exists and has needed columns
required_cols_base = ['firm_id', 'year'] # Base requirements for transitions
if ('df_zombie_clustered_combined' not in locals() or
    not isinstance(df_zombie_clustered_combined, pd.DataFrame) or
    df_zombie_clustered_combined.empty or
    not all(col in df_zombie_clustered_combined.columns for col in required_cols_base)):
    print("Skipping Transition Analysis: `df_zombie_clustered_combined` DataFrame not found, empty, or missing 'firm_id'/'year'.")
    print("  Ensure Part E ran successfully and generated results for zombie firms.")
else:
    # Check for Sector Type column needed for splitting
    if 'Sector Type' not in df_zombie_clustered_combined.columns:
        print("Skipping Sector Split in Transition Analysis: 'Sector Type' column missing.")
        print("  Ensure 'Sector Type' was correctly added in Part E.")
    else:
        print("Found 'Sector Type' column. Proceeding with sector-split transition analysis (Probabilities Only).")

    # --- Define Algorithms to Analyze Transitions For ---
    algorithms_to_analyze = ['KMeans', 'Hierarchical'] # Algorithms to plot side-by-side

    # --- Define Sectors to Analyze ---
    sectors_to_analyze = ['Mfg', 'NonMfg']

    # --- Loop through each sector ---
    for sector_name in sectors_to_analyze:
        print(f"\n\n{'='*15} Analyzing Transitions for Sector: {sector_name} {'='*15}")

        # 1. Filter data for the current sector
        df_sector_zombies = df_zombie_clustered_combined[
            df_zombie_clustered_combined['Sector Type'] == sector_name
        ].copy()

        if df_sector_zombies.empty:
            print(f"No zombie firm data found for sector '{sector_name}'. Skipping.")
            continue
        else:
            print(f"Found {len(df_sector_zombies)} zombie firm-year observations for sector '{sector_name}'.")

        # --- Store results for this sector ---
        # We only need the probability matrices now
        transition_matrices_prob = {}
        all_types_sector = {} # Still need to store unique types found per algo

        # --- Loop through algorithms to calculate matrices ---
        valid_algos_in_sector = []
        for algo_name in algorithms_to_analyze:
            print(f"\n--- Processing Algorithm: {algo_name} for Sector: {sector_name} ---")
            cluster_col = algo_name.lower().replace('kmeans','kmeans_cluster').replace('hierarchical','hier_cluster')

            if cluster_col not in df_sector_zombies.columns:
                print(f"Skipping {algo_name} for {sector_name}: Final cluster column '{cluster_col}' not found.")
                continue

            # Prepare data for this algo/sector
            transition_data_algo = df_sector_zombies[['firm_id', 'year', cluster_col]].dropna(subset=[cluster_col]).copy()
            transition_data_algo[cluster_col] = transition_data_algo[cluster_col].astype(str)

            if transition_data_algo.empty:
                print(f"Skipping {algo_name} for {sector_name}: No valid cluster data after dropping NAs.")
                continue

            transition_data_algo.sort_values(['firm_id', 'year'], inplace=True)
            transition_data_algo['prev_cluster'] = transition_data_algo.groupby('firm_id')[cluster_col].shift(1)
            transitions_algo = transition_data_algo.dropna(subset=['prev_cluster'])
            print(f"\nDEBUG: Transitions Data for {algo_name} / {sector_name}:\n{transitions_algo.head()}") # Add this line
            print(f"Unique previous clusters: {transitions_algo['prev_cluster'].unique()}") # Add this line
            print(f"Unique current clusters: {transitions_algo[cluster_col].unique()}") # Add this line

            if transitions_algo.empty:
                print(f"Skipping {algo_name} for {sector_name}: No consecutive year data found for transitions.")
                continue

            print(f"Found {len(transitions_algo)} potential transitions for {algo_name} in {sector_name}.")
            valid_algos_in_sector.append(algo_name) # Mark this algo as having data for this sector

            try:
                # Calculate Counts Matrix (still needed as intermediate step)
                current_all_types = sorted(list(
                    set(transitions_algo['prev_cluster'].unique()) | set(transitions_algo[cluster_col].unique())
                ))
                all_types_sector[algo_name] = current_all_types # Store types for this algo/sector
                print(f"Unique cluster types for {algo_name} in {sector_name}: {current_all_types}")

                counts_matrix = pd.crosstab(
                    transitions_algo['prev_cluster'], transitions_algo[cluster_col],
                    rownames=['From Cluster Type'], colnames=['To Cluster Type']
                )
                counts_matrix = counts_matrix.reindex(index=current_all_types, columns=current_all_types, fill_value=0)
                print(f"\nDEBUG: Counts Matrix for {algo_name} / {sector_name}:\n{counts_matrix}") # Add this line

                # Calculate Probability Matrix
                row_sums = counts_matrix.sum(axis=1)
                prob_matrix = counts_matrix.divide(row_sums, axis=0).fillna(0)
                transition_matrices_prob[algo_name] = prob_matrix # Store the probability matrix
                print(f"\nDEBUG: Probability Matrix for {algo_name} / {sector_name}:\n{prob_matrix}") # Add this line

                # --- Save Probability Matrix to Excel ---
                print(f"Saving probability matrix for {algo_name} / {sector_name}...")
                excel_filename_h_prob = f"transition_matrix_PROB_{sector_name}_{algo_name}_{timestamp_str}.xlsx" # Naming per algo
                full_excel_path_h_prob = os.path.join(output_dir_path, excel_filename_h_prob)
                try:
                    with pd.ExcelWriter(full_excel_path_h_prob) as writer:
                        prob_matrix.to_excel(writer, sheet_name=f'{algo_name}_Transition_Probs', float_format="%.4f")
                    print(f"  -> Transition matrix (Probabilities) for {algo_name}/{sector_name} saved to Excel: {full_excel_path_h_prob}")
                except Exception as e_save_excel_p:
                    print(f"  -> Error saving transition matrix (Probabilities) to Excel for {algo_name}/{sector_name}: {e_save_excel_p}")


            except Exception as e_matrix_calc:
                print(f"ERROR calculating transition matrices for {algo_name} in {sector_name}: {e_matrix_calc}")
                import traceback
                print(traceback.format_exc())
                if algo_name in valid_algos_in_sector: valid_algos_in_sector.remove(algo_name)

        # --- End Loop Through Algorithms for Matrix Calculation ---

        # --- Plotting Probabilities (Combined Plots - Corrected Heatmap Annotation) ---
        if not valid_algos_in_sector:
             print(f"\nNo valid transition data found for any specified algorithm in sector '{sector_name}'. Skipping probability plot.")
             # continue # Already at the end of the loop for this sector
        elif not any(transition_matrices_prob.get(algo) is not None and not transition_matrices_prob.get(algo).empty for algo in valid_algos_in_sector):
             print(f"\nNo non-empty probability matrices found for sector '{sector_name}'. Skipping probability plot.")
             # continue # Already at the end of the loop for this sector
        else: # Proceed with plotting if at least one valid, non-empty matrix exists
             num_valid_algos = len(valid_algos_in_sector)
             print(f"\n--- Generating Combined Probability Heatmaps for Sector: {sector_name} ---")
             print(f"Algorithms to plot: {valid_algos_in_sector}")

             fig_prob = None # Initialize figure variable outside try
             try:
                 # Estimate plot dimensions dynamically
                 first_algo_types = all_types_sector.get(valid_algos_in_sector[0], [])
                 n_types = len(first_algo_types) if first_algo_types else 5 # Default size if no types found
                 base_width_per_plot = max(5, n_types * 0.8)  # Adjust base width based on number of types
                 base_height = max(4, n_types * 0.7)          # Adjust base height based on number of types

                 fig_prob, axes_prob = plt.subplots(1, num_valid_algos,
                                                    figsize=(base_width_per_plot * num_valid_algos * 1.1, base_height * 1.1), # Add padding factor
                                                    squeeze=False) # Ensure axes_prob is always 2D
                 axes_prob = axes_prob.flatten() # Flatten for easy indexing

                 sector_display_name = PUBLICATION_NAMES.get(sector_name, sector_name)
                 #fig_prob.suptitle(f"Zombie Cluster Transition Probabilities ({sector_display_name})",
                                   #fontsize=matplotlib.rcParams['figure.titlesize'], y=1.02) # Adjust y for suptitle

                 for i, algo_name in enumerate(valid_algos_in_sector):
                     ax = axes_prob[i]
                     prob_matrix = transition_matrices_prob.get(algo_name)
                     algo_display_name = PUBLICATION_NAMES.get(algo_name, algo_name)
                     current_cluster_types = all_types_sector.get(algo_name, []) # Get types for this specific plot

                     if prob_matrix is None or prob_matrix.empty:
                         ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)
                         ax.set_title(f"{algo_display_name}\n(No Data)")
                         continue

                     # <<< --- MODIFIED HEATMAP CALL --- >>>
                     # Determine if this is the last plot to show the color bar
                     show_cbar = (i == num_valid_algos - 1)

                     sns.heatmap(prob_matrix,
                                 annot=prob_matrix,
                                 fmt=".0%",
                                 cmap="Blues",
                                 linewidths=.3,
                                 linecolor='lightgrey',
                                 ax=ax,
                                 cbar=show_cbar, # <<< Only show cbar for the last plot >>>
                                 square=True,
                                 vmin=0, vmax=1, # Keep vmin/vmax consistent for shared scale
                                 # Apply cbar_kws only when cbar is True
                                 cbar_kws={'shrink': 0.75, # Adjusted shrink factor slightly
                                           'format': FuncFormatter(lambda x, pos: f'{x:.0%}')} if show_cbar else None,
                                 annot_kws={"size": max(5, 8 - n_types // 2),
                                            "color": "black"} # Keep annotation color consistent
                                )
                     # <<< --- END OF HEATMAP CALL MODIFICATION --- >>>

                     # --- Set Titles and Labels ---
                     ax.set_title(f"{algo_display_name}", fontsize=matplotlib.rcParams['axes.titlesize'])
                     ax.set_xticks(np.arange(len(current_cluster_types)) + 0.5)
                     ax.set_yticks(np.arange(len(current_cluster_types)) + 0.5)
                     ax.set_xticklabels(current_cluster_types, rotation=45, ha='right', fontsize=matplotlib.rcParams['xtick.labelsize'] * 0.9)
                     ax.set_yticklabels(current_cluster_types, rotation=0, fontsize=matplotlib.rcParams['ytick.labelsize'] * 0.9)

                     ax.set_xlabel("To Cluster Type", fontsize=matplotlib.rcParams['axes.labelsize'])
                     if i == 0:
                         ax.set_ylabel("From Cluster Type", fontsize=matplotlib.rcParams['axes.labelsize'])
                     else:
                         ax.set_ylabel("")
                         plt.setp(ax.get_yticklabels(), visible=False) # Hide y-labels on second plot for compactness
                 # --- End inner loop ---


                 # --- Layout and Save ---
                 # <<< --- MODIFIED LAYOUT ADJUSTMENT --- >>>
                 try:
                     # Adjust subplot parameters for closer spacing and accommodate single colorbar
                     # Reduce wspace significantly. May need small adjustments to left/right/top/bottom
                     fig_prob.subplots_adjust(left=0.08, right=0.90, bottom=0.15, top=0.90, wspace=0.08) # Example: small wspace
                     print(f"DEBUG H: Applied subplots_adjust for {sector_name}.")
                 except Exception as e_layout:
                      print(f"Warning: Adjusting subplot layout failed for {sector_name}: {e_layout}")
                 # <<< --- END MODIFIED LAYOUT ADJUSTMENT --- >>>

                 filename_prob = f"transition_heatmap_PROB_{sector_name}_Combined_{timestamp_str}.png"
                 save_path_prob = os.path.join(output_dir_path, filename_prob)
                 plt.savefig(save_path_prob, dpi=300)
                 print(f"Combined Probability heatmap for {sector_name} saved: {save_path_prob}")
                 plt.close(fig_prob) # Close the figure

             except Exception as e_plot_prob:
                 print(f"ERROR generating combined Probability heatmap for {sector_name}: {e_plot_prob}")
                 import traceback
                 print(traceback.format_exc())
                 if fig_prob is not None and isinstance(fig_prob, plt.Figure) and fig_prob in plt.get_fignums():
                     plt.close(fig_prob) # Ensure closure on error

    # --- End Loop Through Sectors ---

# --- End of PART H (Revised: Probabilities Only - Explicit Annot String Attempt) ---


###############################################################################
# PART J: SANKEY DIAGRAM FOR CLUSTER FLOWS (Zombie Firms - Split by Sector)
###############################################################################
print("\n\n====== PART J: Sankey Diagram for Cluster Flows (Zombie Firms - Split by Sector) ======")

# --- Configuration for Sankey ---
# Define the range of *end years* for the transitions you want to plot
# Example: To see flow from 2007->2008, 2008->2009, 2009->2010
#          We need transitions where the END year is 2008, 2009, 2010.
sankey_years_end = range(2002, 2025) # Example: Crisis period flow (2007 -> 2010)
# sankey_years_end = range(2020, 2023) # Example: Covid period flow (2019 -> 2022)

sankey_algorithms = ['KMeans', 'Hierarchical'] # Algorithms to visualize
sankey_sectors = ['Mfg', 'NonMfg']            # Sectors to visualize

# Check if the combined zombie cluster results DataFrame exists
if ('df_zombie_clustered_combined' not in locals() or
    not isinstance(df_zombie_clustered_combined, pd.DataFrame) or
    df_zombie_clustered_combined.empty or
    not all(col in df_zombie_clustered_combined.columns for col in ['firm_id', 'year', 'Sector Type'])):
    print("Skipping Sankey Diagram Analysis: `df_zombie_clustered_combined` DataFrame not found, empty, or missing key columns.")
    print("  Ensure Part E ran successfully and generated results for zombie firms.")
else:
    print(f"Preparing Sankey diagrams for final years: {list(sankey_years_end)}")

    for sector_name in sankey_sectors:
        print(f"\n--- Generating Sankey for Sector: {sector_name} ---")

        # 1. Filter data for the current sector
        df_sector_zombies_sankey = df_zombie_clustered_combined[
            df_zombie_clustered_combined['Sector Type'] == sector_name
        ].copy()

        if df_sector_zombies_sankey.empty:
            print(f"No zombie firm data found for sector '{sector_name}'. Skipping Sankey.")
            continue

        for algo_name in sankey_algorithms:
            print(f"\n  -- Algorithm: {algo_name} --")
            # Derive the final cluster column name (e.g., 'kmeans_cluster')
            cluster_col_final = algo_name.lower().replace('kmeans','kmeans_cluster').replace('hierarchical','hier_cluster')

            if cluster_col_final not in df_sector_zombies_sankey.columns:
                print(f"  Skipping {algo_name}: Final cluster column '{cluster_col_final}' not found.")
                continue

            # 2. Prepare transition data for this algo/sector
            sankey_data_algo = df_sector_zombies_sankey[['firm_id', 'year', cluster_col_final]].dropna(subset=[cluster_col_final]).copy()
            sankey_data_algo[cluster_col_final] = sankey_data_algo[cluster_col_final].astype(str) # Use string types

            if sankey_data_algo.empty:
                print(f"  Skipping {algo_name}: No valid cluster data after dropping NAs.")
                continue

            # Create previous cluster link
            sankey_data_algo.sort_values(['firm_id', 'year'], inplace=True)
            sankey_data_algo['prev_cluster'] = sankey_data_algo.groupby('firm_id')[cluster_col_final].shift(1)

            # Keep only rows representing a valid transition within the desired years
            transitions_sankey = sankey_data_algo.dropna(subset=['prev_cluster'])
            transitions_sankey = transitions_sankey[transitions_sankey['year'].isin(sankey_years_end)]

            if transitions_sankey.empty:
                print(f"  Skipping {algo_name}: No transitions found within the specified year range {list(sankey_years_end)}.")
                continue

            # 3. Aggregate flows
            flow_counts = transitions_sankey.groupby(
                ['year', 'prev_cluster', cluster_col_final]
            ).size().reset_index(name='count')

            if flow_counts.empty:
                 print(f"  Skipping {algo_name}: No aggregated flows found within the year range.")
                 continue
            else:
                 print(f"  Aggregated {len(flow_counts)} unique transition flows for {algo_name} / {sector_name} / Years {min(sankey_years_end)}-{max(sankey_years_end)}.")

            # 4. Prepare data for Plotly using helper
            sankey_nodes, sankey_links = prepare_sankey_data(
                df_transitions=flow_counts,
                cluster_col_name=cluster_col_final,
                value_col='count'
            )

            # 5. Process and Save Flow Data to TXT (Instead of Plotting)
            if sankey_nodes and sankey_links:
                print(f"  Processing Sankey data for {algo_name} / {sector_name}...")
                try:
                    # Get the node labels corresponding to the source/target IDs
                    node_labels = sankey_nodes.get('label', [])
                    source_ids = sankey_links.get('source', [])
                    target_ids = sankey_links.get('target', [])
                    link_values = sankey_links.get('value', [])

                    if not node_labels or not source_ids or not target_ids or not link_values:
                        print("  ERROR: Invalid nodes or links data received from prepare_sankey_data.")
                        continue

                    # Map IDs back to labels (handle potential index errors)
                    source_labels_list = [node_labels[i] if 0 <= i < len(node_labels) else 'INVALID_ID' for i in source_ids]
                    target_labels_list = [node_labels[i] if 0 <= i < len(node_labels) else 'INVALID_ID' for i in target_ids]

                    # Create a DataFrame for the flow data
                    flow_df = pd.DataFrame({
                        'From_Node': source_labels_list,
                        'To_Node': target_labels_list,
                        'Count': link_values
                    })

                    # Sort for better readability
                    flow_df.sort_values(by=['Count', 'From_Node', 'To_Node'], ascending=[False, True, True], inplace=True)

                    # Define filename and prepare header
                    sankey_data_title = f"Zombie Cluster Flow Data ({PUBLICATION_NAMES.get(algo_name, algo_name)} - {PUBLICATION_NAMES.get(sector_name, sector_name)}: {min(sankey_years_end)-1} to {max(sankey_years_end)})"
                    sankey_filename_base = f"sankey_flow_data_{sector_name}_{algo_name}_{min(sankey_years_end)-1}_{max(sankey_years_end)}_{timestamp_str}"
                    txt_save_path = os.path.join(output_dir_path, f"{sankey_filename_base}.txt")

                    header = f"{sankey_data_title}\n"
                    header += f"Timestamp: {timestamp_str}\n"
                    header += f"Sector: {sector_name}\n"
                    header += f"Algorithm: {algo_name}\n"
                    header += f"Years (End of Transition): {list(sankey_years_end)}\n"
                    header += "=" * 60 + "\n\n"

                    # Save to text file
                    with open(txt_save_path, "w", encoding='utf-8') as f:
                        f.write(header)
                        # Use to_string for better formatting control in text file
                        f.write(flow_df.to_string(index=False, justify='center'))

                    print(f"  Sankey flow data saved to: {txt_save_path}")

                except IndexError as ie:
                     print(f"  ERROR processing Sankey data (IndexError likely mapping IDs to labels): {ie}")
                     print(f"    Node labels count: {len(node_labels)}, Max Source ID: {max(source_ids)}, Max Target ID: {max(target_ids)}")
                except Exception as e_save_txt:
                    print(f"  ERROR saving Sankey data to text file: {e_save_txt}")
                    import traceback
                    print(traceback.format_exc())

            else:
                # This message comes from the prepare_sankey_data failure
                print(f"  Skipping Sankey data saving for {algo_name} / {sector_name}: Data preparation failed.")
# --- End of PART J ---

###############################################################################
# PART F: ZOMBIE ANALYSIS BY INDUSTRY
###############################################################################
print("\n\n====== Zombie Analysis by Industry Sector ======")

# Check if industry data is available... (code remains the same)
if 'industry_col_available' in locals() and industry_col_available and \
   'industry_col_name' in locals() and 'df_analysis' in locals() and \
   industry_col_name in df_analysis.columns and not df_analysis.empty and final_rows > 0:

    print(f"Analyzing using industry column: '{industry_col_name}'")
    df_analysis_industry = df_analysis.dropna(subset=[industry_col_name])
    if len(df_analysis_industry) < len(df_analysis):
        print(f"Note: Dropped {len(df_analysis) - len(df_analysis_industry)} rows with missing industry for Part F analysis.")

    if not df_analysis_industry.empty:

        # --- Calculate Data, Filter Blanks, Add Sector Type, Sort ---
        # (This part remains the same)
        # 1. Proportion
        industry_summary = df_analysis_industry.groupby(industry_col_name).agg(total_firm_years=('year', 'size'), zombie_firm_years=(target_col, lambda x: (x == 1).sum())).reset_index(); industry_summary['zombie_proportion'] = np.where(industry_summary['total_firm_years'] > 0, industry_summary['zombie_firm_years'] / industry_summary['total_firm_years'],0)
        # 2. Distribution
        df_zombie_only = df_analysis_industry[df_analysis_industry[target_col] == 1].copy()
        if not df_zombie_only.empty: zombie_industry_dist_raw = df_zombie_only[industry_col_name].value_counts(normalize=True, dropna=False) * 100; zombie_industry_dist = zombie_industry_dist_raw.reset_index(); zombie_industry_dist.columns = [industry_col_name, 'zombie_distribution']
        else: print("Warning: No zombie firms found, distribution plot will be empty."); zombie_industry_dist = pd.DataFrame(columns=[industry_col_name, 'zombie_distribution'])
        # Filter
        print("\nFiltering blank/whitespace industry names...")
        industry_summary_filt = industry_summary[industry_summary[industry_col_name].astype(str).str.strip().astype(bool)].copy()
        zombie_industry_dist_filt = zombie_industry_dist[zombie_industry_dist[industry_col_name].astype(str).str.strip().astype(bool)].copy()
        # Add Sector Type
        def get_sector_type(sector_name):
            if sector_name in mfg_sectors: return 'Mfg'
            elif sector_name in nonmfg_sectors: return 'NonMfg'
            else: return 'Other'
        if not industry_summary_filt.empty: industry_summary_filt['SectorType'] = industry_summary_filt[industry_col_name].apply(get_sector_type)
        if not zombie_industry_dist_filt.empty: zombie_industry_dist_filt['SectorType'] = zombie_industry_dist_filt[industry_col_name].apply(get_sector_type)
        # Sort
        if not industry_summary_filt.empty: industry_summary_filt.sort_values(by='zombie_proportion', ascending=False, inplace=True); print("\nProportion data sorted descending.")
        if not zombie_industry_dist_filt.empty: zombie_industry_dist_filt.sort_values(by='zombie_distribution', ascending=False, inplace=True); print("Distribution data sorted descending.")


        # <<< --- START: INDEPENDENT PLOTTING BLOCK (Side-by-Side, New Colors) --- >>>
        if not industry_summary_filt.empty or not zombie_industry_dist_filt.empty:
            print("\nGenerating Independent Industry Analysis Plots (Sorted, Side-by-Side, New Colors)...") # Updated print

            num_plots = (1 if not industry_summary_filt.empty else 0) + (1 if not zombie_industry_dist_filt.empty else 0)
            if num_plots == 0:
                print("No data to plot after filtering.")
            else:
                num_industries_prop = len(industry_summary_filt); num_industries_dist = len(zombie_industry_dist_filt)
                fig_height_side = max(height * 1.5, height * max(num_industries_prop, num_industries_dist) * 0.19)
                fig_width_side = width * 4.5

                fig_independent_side, axes_independent_side = plt.subplots(1, num_plots, figsize=(fig_width_side, fig_height_side), squeeze=False)
                plot_col_index = 0

                # --- *** Define New Colors based on reference image *** ---
                color_map_ref = {
                    'Mfg': 'sandybrown',    # Use this color for Manufacturing bars
                    'NonMfg': 'steelblue',  # Use this color for Non-Manufacturing bars
                    'Other': 'silver'       # Use this color for Other/unclassified bars
                }
                # --- *** END Define New Colors *** ---

                # --- Plot 1 (Left): Proportion ---
                if not industry_summary_filt.empty:
                    ax1 = axes_independent_side[0, plot_col_index]
                    plot_labels_prop = industry_summary_filt[industry_col_name].astype(str)
                    # Generate color list using the NEW simplified map
                    colors1 = [color_map_ref.get(stype, color_map_ref['Other']) for stype in industry_summary_filt['SectorType']]

                    bars1 = ax1.barh(plot_labels_prop, industry_summary_filt['zombie_proportion'],
                                     color=colors1, # Apply new color list
                                     edgecolor='black', linewidth=0.7)

                    title1 = f"A: {PUBLICATION_NAMES.get('Zombie_Prop', 'Proportion')} within Each Industry"; ax1.set_title(title1)
                    ax1.set_xlabel(f"{PUBLICATION_NAMES.get('Zombie_Prop', 'Zombie Proportion')} (%)"); ax1.set_ylabel(PUBLICATION_NAMES.get('BvD sectors', 'Industry Sector'))
                    ax1.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0)); ax1.xaxis.set_ticks_position('bottom')
                    ax1.tick_params(axis='y', which='both', left=True, right=False, labelsize=9)
                    ax1.grid(False); ax1.invert_yaxis(); ax1.margins(y=0.015)
                    # Add bar labels
                    for patch in bars1.patches:
                        width_val = patch.get_width(); y_pos = patch.get_y() + patch.get_height() / 2
                        if width_val > 0.0001: label_text = f"{width_val:.1%}"; x_offset = ax1.get_xlim()[1] * 0.015; x_pos_text = width_val + x_offset; ax1.text(x=x_pos_text, y=y_pos, s=label_text, va='center', ha='left', fontsize=6)
                    current_xlim_right1 = ax1.get_xlim()[1]; max_labeled_x1 = industry_summary_filt['zombie_proportion'].max() * 1.25; ax1.set_xlim(right=max(current_xlim_right1, max_labeled_x1, 0.01))
                    plot_col_index += 1

                # --- Plot 2 (Right): Distribution ---
                if not zombie_industry_dist_filt.empty:
                    ax2 = axes_independent_side[0, plot_col_index]
                    plot_labels_dist = zombie_industry_dist_filt[industry_col_name].astype(str)
                    # Generate color list using the NEW simplified map
                    colors2 = [color_map_ref.get(stype, color_map_ref['Other']) for stype in zombie_industry_dist_filt['SectorType']]

                    bars2 = ax2.barh(plot_labels_dist, zombie_industry_dist_filt['zombie_distribution'],
                                     color=colors2, # Apply new color list
                                     edgecolor='black', linewidth=0.7)

                    title2 = f"B: Industry Distribution of All {PUBLICATION_NAMES.get('Zombie', 'Zombie')} {PUBLICATION_NAMES.get('Firm-Year', 'Firm-Year')}s"; ax2.set_title(title2)
                    ax2.set_xlabel(f"% of Total {PUBLICATION_NAMES.get('Zombie', 'Zombie')} {PUBLICATION_NAMES.get('Firm-Year', 'Firm-Year')}s"); ax2.set_ylabel("")
                    ax2.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0)); ax2.xaxis.set_ticks_position('bottom')
                    ax2.tick_params(axis='y', which='both', left=True, right=False, labelleft=True, labelsize=9)
                    ax2.grid(False); ax2.invert_yaxis(); ax2.margins(y=0.015)
                    # Add bar labels
                    for patch in bars2.patches:
                        width_val = patch.get_width(); y_pos = patch.get_y() + patch.get_height() / 2
                        if width_val > 0.01: label_text = f"{width_val:.1f}%"; x_offset = ax2.get_xlim()[1] * 0.015; x_pos_text = width_val + x_offset; ax2.text(x=x_pos_text, y=y_pos, s=label_text, va='center', ha='left', fontsize=6)
                    current_xlim_right2 = ax2.get_xlim()[1]; max_labeled_x2 = zombie_industry_dist_filt['zombie_distribution'].max() * 1.25; ax2.set_xlim(right=max(current_xlim_right2, max_labeled_x2, 1.0))
                    plot_col_index += 1

                # --- *** ADJUST MANUAL LEGEND FOR NEW COLORS *** ---
                if num_plots > 0:
                    legend_elements = [
                         matplotlib.patches.Patch(facecolor=color_map_ref['Mfg'], edgecolor='black', label=PUBLICATION_NAMES.get('Mfg','Manufacturing')),
                         matplotlib.patches.Patch(facecolor=color_map_ref['NonMfg'], edgecolor='black', label=PUBLICATION_NAMES.get('NonMfg','Non-Manufacturing')),
                         matplotlib.patches.Patch(facecolor=color_map_ref['Other'], edgecolor='black', label='Other')
                         ]
                    # Position legend below the plots, adjusted font size
                    fig_independent_side.legend(handles=legend_elements, loc='lower center',
                                                bbox_to_anchor=(0.5, 0.01), # Keep position from previous adjustment
                                                ncol=3, # 3 columns for Mfg/NonMfg/Other
                                                fontsize=9, # Keep increased fontsize
                                                frameon=False)
                # --- *** END MANUAL LEGEND ADJUSTMENT *** ---


                # --- ADJUST LAYOUT (wspace might need tuning) ---
                try:
                    # Ensure wspace is large enough to prevent label overlap
                    fig_independent_side.subplots_adjust(left=0.25, right=0.98, bottom=0.15, top=0.92, wspace=0.75) # Keep previous spacing
                except Exception as e_layout:
                    print(f"Warning: Layout adjustment failed for independent side-by-side plots: {e_layout}")
                # --- END ADJUST LAYOUT ---

                # Save the combined figure
                independent_filename_side = f"zombie_industry_analysis_indep_sort_sidebyside_refcolor_{timestamp_str}.png" # New filename
                independent_save_path_side = os.path.join(output_dir_path, independent_filename_side)
                try:
                    plt.savefig(independent_save_path_side, dpi=600, bbox_inches='tight')
                    print(f"Independent industry analysis plots (Sorted, Side-by-Side, Ref Colors) saved: {independent_save_path_side}")
                except Exception as e_save:
                    print(f"Error saving independent side-by-side plots {independent_save_path_side}: {e_save}")
                plt.close(fig_independent_side) # Close the figure

        # <<< --- END: INDEPENDENT PLOTTING BLOCK --- >>>

    else:
        print("Skipping Industry Analysis Plots (no data after dropping missing industries).")
else:
    print("Skipping Zombie Analysis by Industry (industry column not available or no valid data).")

# --- End of PART F ---

###############################################################################
# PART G: ZOMBIE ANALYSIS BY YEAR (SEPARATE PLOTS, Interest Rate ONLY)
###############################################################################
print("\n\n====== Zombie Analysis by Year (Separate Plots: Counts & Proportions + Interest Rate) ======")

# Use df_analysis aligned with final X/y (potentially trimmed)
if 'df_analysis' in locals() and not df_analysis.empty and final_rows > 0:

    # --- Data Preparation for Zombie Stats---
    # (This section remains the same)
    plot_by_sector = False; year_plot_data = pd.DataFrame()
    try:
        year_summary_total = df_analysis.groupby('year').agg(total_firm_years_total=('year', 'size'), zombie_firm_years_total=(target_col, lambda x: (x == 1).sum())).reset_index()
        year_summary_total['zombie_proportion_total'] = np.where(year_summary_total['total_firm_years_total'] > 0, year_summary_total['zombie_firm_years_total'] / year_summary_total['total_firm_years_total'], 0)
        year_plot_data = year_summary_total; print("Calculated overall yearly zombie summary.")
    except Exception as e_total_agg: print(f"Error calculating overall yearly zombie summary: {e_total_agg}"); print("Skipping Part G main analysis."); year_plot_data = pd.DataFrame()

    if not year_plot_data.empty:
        # --- Sector Data Preparation ---
        # (This part remains the same)
        if industry_col_available and industry_col_name in df_analysis.columns and df_analysis[industry_col_name].notna().any():
             print("Industry data available, preparing sector-specific yearly summary.")
             try:
                # (Keep sector data preparation logic as before)
                if 'Sector Type' not in df_analysis.columns: df_analysis['Sector Type'] = df_analysis[industry_col_name].apply(lambda x: 'Mfg' if x in mfg_sectors else ('NonMfg' if x in nonmfg_sectors else 'Other'))
                sector_year_counts = df_analysis.groupby(['year', 'Sector Type']).agg(total_firm_years=('Sector Type', 'size'), zombie_firm_years=(target_col, lambda x: (x == 1).sum())).reset_index()
                sector_pivot_total = sector_year_counts.pivot(index='year', columns='Sector Type', values='total_firm_years').fillna(0); sector_pivot_zombie = sector_year_counts.pivot(index='year', columns='Sector Type', values='zombie_firm_years').fillna(0)
                sector_pivot_total.columns = [f'total_firm_years_{col}' for col in sector_pivot_total.columns]; sector_pivot_zombie.columns = [f'zombie_firm_years_{col}' for col in sector_pivot_zombie.columns]
                year_plot_data = pd.merge(year_plot_data, sector_pivot_total, on='year', how='left'); year_plot_data = pd.merge(year_plot_data, sector_pivot_zombie, on='year', how='left'); year_plot_data.fillna(0, inplace=True)
                if 'total_firm_years_Mfg' in year_plot_data.columns and 'zombie_firm_years_Mfg' in year_plot_data.columns: year_plot_data['zombie_proportion_mfg'] = np.where(year_plot_data['total_firm_years_Mfg'] > 0, year_plot_data['zombie_firm_years_Mfg'] / year_plot_data['total_firm_years_Mfg'], 0)
                else: year_plot_data['zombie_proportion_mfg'] = 0; year_plot_data['total_firm_years_Mfg'] = 0; year_plot_data['zombie_firm_years_Mfg'] = 0
                if 'total_firm_years_NonMfg' in year_plot_data.columns and 'zombie_firm_years_NonMfg' in year_plot_data.columns: year_plot_data['zombie_proportion_nonmfg'] = np.where(year_plot_data['total_firm_years_NonMfg'] > 0, year_plot_data['zombie_firm_years_NonMfg'] / year_plot_data['total_firm_years_NonMfg'], 0)
                else: year_plot_data['zombie_proportion_nonmfg'] = 0; year_plot_data['total_firm_years_NonMfg'] = 0; year_plot_data['zombie_firm_years_NonMfg'] = 0
                if 'total_firm_years_Other' not in year_plot_data.columns: year_plot_data['total_firm_years_Other'] = 0
                if 'zombie_firm_years_Other' not in year_plot_data.columns: year_plot_data['zombie_firm_years_Other'] = 0
                if year_plot_data['total_firm_years_Mfg'].sum() > 0 or year_plot_data['total_firm_years_NonMfg'].sum() > 0: plot_by_sector = True; print("Sector-specific yearly summary prepared.")
                else: print("No Mfg/NonMfg firms found for sector breakdown."); plot_by_sector = False
             except Exception as e_sector_agg: print(f"Error preparing sector data: {e_sector_agg}. Reverting to total plot."); plot_by_sector = False; year_plot_data['zombie_proportion_mfg'] = np.nan; year_plot_data['zombie_proportion_nonmfg'] = np.nan
        else: print("Skipping sector split (industry data not available/usable)."); year_plot_data['zombie_proportion_mfg'] = np.nan; year_plot_data['zombie_proportion_nonmfg'] = np.nan; plot_by_sector = False
        # --- End Sector Data Preparation ---

        # --- Merge External Data (Interest Rate Only) ---
        if external_data_available:
            try:
                # Only merge interest rate data now
                year_plot_data = pd.merge(year_plot_data, df_interest, on='year', how='left')
                print("Merged external Interest Rate data.")
            except Exception as e_merge:
                print(f"Error merging external data: {e_merge}. Proceeding without overlay.")
                external_data_available = False # Disable if merge fails
        # --- End Merge ---


      # <<< --- START: ADD CODE TO SAVE YEARLY COUNTS TO TXT --- >>>
        print("\nSaving yearly zombie counts summary to text file...")
        try:
          # Select relevant columns to save
          cols_to_save = ['year']
          rename_map = {'year': 'Year'}

          # Always include total zombies if column exists
          if 'zombie_firm_years_total' in year_plot_data.columns:
                cols_to_save.append('zombie_firm_years_total')
                # Use publication names for cleaner headers
                rename_map['zombie_firm_years_total'] = f"Total {PUBLICATION_NAMES.get('Zombie_Count','Zombies')}"
          else:
                print("Warning: 'zombie_firm_years_total' column not found for saving.")


          # Include sector columns only if they were successfully calculated and exist
          if plot_by_sector: # plot_by_sector flag is set earlier in Part G
              if 'zombie_firm_years_Mfg' in year_plot_data.columns:
                  cols_to_save.append('zombie_firm_years_Mfg')
                  rename_map['zombie_firm_years_Mfg'] = f"{PUBLICATION_NAMES.get('Mfg','Mfg')} {PUBLICATION_NAMES.get('Zombie_Count','Zombies')}"
              else:
                   print("Warning: 'zombie_firm_years_Mfg' column expected but not found.")

              if 'zombie_firm_years_NonMfg' in year_plot_data.columns:
                  cols_to_save.append('zombie_firm_years_NonMfg')
                  rename_map['zombie_firm_years_NonMfg'] = f"{PUBLICATION_NAMES.get('NonMfg','NonMfg')} {PUBLICATION_NAMES.get('Zombie_Count','Zombies')}"
              else:
                   print("Warning: 'zombie_firm_years_NonMfg' column expected but not found.")
          else:
              print("Info: Sector-specific counts not available or not calculated, saving totals only.")


          # Create a copy with selected columns, ensure they exist before renaming
          existing_cols_to_save = [col for col in cols_to_save if col in year_plot_data.columns]
          if not existing_cols_to_save:
               raise ValueError("No valid columns found to save in yearly summary.")

          yearly_summary_df = year_plot_data[existing_cols_to_save].copy()
          yearly_summary_df.rename(columns=rename_map, inplace=True)

          # Convert DataFrame to a formatted string (adjust options as needed)
          # index=False prevents writing the DataFrame index to the file
          # justify='center' helps align columns
          summary_string = yearly_summary_df.to_string(index=False, justify='center')

          # Define filename and full path
          txt_filename = f"yearly_zombie_counts_{timestamp_str}.txt"
          full_txt_path = os.path.join(output_dir_path, txt_filename)

          # Write the string to the text file
          with open(full_txt_path, "w", encoding='utf-8') as f:
              f.write(f"Yearly Zombie Firm-Year Counts Summary ({timestamp_str})\n")
              f.write("=" * (len(yearly_summary_df.columns)*12) + "\n") # Adjust separator length
              f.write(summary_string)

          print(f"Yearly zombie counts summary saved to: {full_txt_path}")

        except KeyError as ke:
          print(f"Error saving yearly counts: Missing expected column(s) - {ke}. Check data preparation steps in Part G.")
        except Exception as e_save_txt:
          print(f"Error saving yearly zombie counts summary to text file: {e_save_txt}")
      # <<< --- END: ADD CODE TO SAVE YEARLY COUNTS TO TXT --- >>>

        # --- Plotting ---
        print("\nZombie Firms per Year (Summary Data Used for Plot):")
        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000): print(year_plot_data.round(4))

        # --- FIGURE 1: Counts (Bars) ---
        # (This plot remains unchanged)
        print("\nGenerating Yearly Zombie Count Plot...")
        try:
            # ... (Keep code for Figure 1 exactly as before) ...
            fig_counts, ax_counts = plt.subplots(figsize=(width * 1.8, height * 1.1)); handles_counts = []; bar1_c, bar2_c, bars_c = None, None, None
            if plot_by_sector:
                 color_mfg = 'tab:orange'; color_nonmfg = 'tab:blue'; label_mfg = f"{PUBLICATION_NAMES.get('Mfg','Mfg')} {PUBLICATION_NAMES.get('Zombie_Count','Count')}"; label_nonmfg = f"{PUBLICATION_NAMES.get('NonMfg','NonMfg')} {PUBLICATION_NAMES.get('Zombie_Count','Count')}"
                 if year_plot_data.get('zombie_firm_years_Mfg', 0).sum() > 0: bar1_c = ax_counts.bar(year_plot_data['year'], year_plot_data['zombie_firm_years_Mfg'], color=color_mfg, alpha=0.7, label=label_mfg, zorder=3); handles_counts.append(bar1_c)
                 if year_plot_data.get('zombie_firm_years_NonMfg', 0).sum() > 0: bottom_data = year_plot_data['zombie_firm_years_Mfg'] if bar1_c else 0; bar2_c = ax_counts.bar(year_plot_data['year'], year_plot_data['zombie_firm_years_NonMfg'], bottom=bottom_data, color=color_nonmfg, alpha=0.7, label=label_nonmfg, zorder=3); handles_counts.append(bar2_c)
                 ax_counts.set_ylabel(f"{PUBLICATION_NAMES.get('Firm-Year','Firm-Year')}s"); 
                 title_counts = f"Annual {PUBLICATION_NAMES.get('Zombie','Zombie')} {PUBLICATION_NAMES.get('Firm-Year','Firm-Year')} Count by Sector"; 
                 max_count = (year_plot_data.get('zombie_firm_years_Mfg', 0) + year_plot_data.get('zombie_firm_years_NonMfg', 0) + year_plot_data.get('zombie_firm_years_Other', 0)).max()
            else:
                 if not year_plot_data['zombie_firm_years_total'].empty: color1 = 'tab:grey'; label_total = f"Total {PUBLICATION_NAMES.get('Zombie_Count','Count')}"; bars_c = ax_counts.bar(year_plot_data['year'], year_plot_data['zombie_firm_years_total'], color=color1, alpha=0.7, label=label_total, zorder=3); handles_counts.append(bars_c); ax_counts.set_ylabel(f"{PUBLICATION_NAMES.get('Firm-Year','Firm-Year')}s"); ax_counts.tick_params(axis='y', labelcolor=color1); title_counts = f"Total Annual {PUBLICATION_NAMES.get('Zombie','Zombie')} {PUBLICATION_NAMES.get('Firm-Year','Firm-Year')} Count"; max_count = year_plot_data['zombie_firm_years_total'].max()
                 else: max_count = 0
            ax_counts.set_xlabel('Year'); ax_counts.set_ylim(bottom=0, top=max_count * 1.1 if max_count > 0 else 1); ax_counts.grid(True, linestyle='--', alpha=0.6, axis='y', zorder=0); ax_counts.tick_params(axis='x', rotation=45, labelsize=8, which='major'); years_in_summary = year_plot_data['year'].unique();
            if len(years_in_summary) <= 25: ax_counts.set_xticks(years_in_summary)
            else: ax_counts.xaxis.set_major_locator(mtick.MaxNLocator(integer=True, nbins=15))
            plt.setp(ax_counts.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor", fontsize=8)
            if handles_counts: legend_obj_c = ax_counts.legend(handles=handles_counts, loc='best', fontsize=8, frameon=True, facecolor='white', framealpha=0.8, edgecolor='black', fancybox=False); legend_obj_c.get_frame().set_linewidth(plt.rcParams['axes.linewidth'])
            plt.tight_layout(); count_filename = f"zombie_counts_year{'_by_sector' if plot_by_sector else ''}_{timestamp_str}.png"; count_save_path = os.path.join(output_dir_path, count_filename)
            try: plt.savefig(count_save_path, dpi=600, bbox_inches='tight'); print(f"Yearly counts plot saved: {count_save_path}")
            except Exception as e_save_c: print(f"Error saving counts plot {count_save_path}: {e_save_c}")
            plt.close(fig_counts)
        except Exception as e_plot_c: print(f"Error generating counts plot: {e_plot_c}"); plt.close(fig_counts) if 'fig_counts' in locals() else None
        # --- End Figure 1 ---


        # --- FIGURE 2: Proportions (Lines) + Interest Rate ONLY (Styled) ---
        print("\nGenerating Yearly Zombie Proportion Plot with Interest Rate...")
        try:
            fig_props, ax_props = plt.subplots(figsize=(width * 2.0, height * 1.2))
            handles_props = []
            handles_secondary = [] # Only for interest rate now
            line_total_p, line_mfg_p, line_nonmfg_p = None, None, None
            line_int = None # No line_gdp needed

            # Plot Zombie Proportions on Primary Y-Axis (Left)
            ax_props.set_xlabel('Year')
            ax_props.set_ylabel(f"{PUBLICATION_NAMES.get('Zombie_Prop','Proportion')} (%)", color='black')
            ax_props.tick_params(axis='y', labelcolor='black', labelsize=8)
            ax_props.tick_params(axis='x', rotation=45, labelsize=8, which='major')
            ax_props.grid(True, linestyle=':', alpha=0.4, axis='y', color='grey', zorder=0)

            max_prop_val = 0
            color_total_line = 'black'; label_total_p = f"Overall {PUBLICATION_NAMES.get('Zombie_Prop','Proportion')}"
            if not year_plot_data['zombie_proportion_total'].isna().all():
                line_total_p, = ax_props.plot(year_plot_data['year'], year_plot_data['zombie_proportion_total'], color=color_total_line, linestyle='-', marker='o', markersize=3, label=label_total_p, linewidth=1.5, zorder=10)
                if line_total_p: handles_props.append(line_total_p)
                max_prop_val = max(max_prop_val, year_plot_data['zombie_proportion_total'].fillna(0).max())
            if plot_by_sector:
                color_mfg_line = 'dimgrey'; color_nonmfg_line = 'steelblue'; label_mfg_p = f"{PUBLICATION_NAMES.get('Mfg','Mfg')} {PUBLICATION_NAMES.get('Zombie_Prop','Proportion')}"; label_nonmfg_p = f"{PUBLICATION_NAMES.get('NonMfg','NonMfg')} {PUBLICATION_NAMES.get('Zombie_Prop','Proportion')}"
                if 'zombie_proportion_mfg' in year_plot_data.columns and not year_plot_data['zombie_proportion_mfg'].isna().all(): line_mfg_p, = ax_props.plot(year_plot_data['year'], year_plot_data['zombie_proportion_mfg'], color=color_mfg_line, linestyle='--', marker='^', markersize=3, label=label_mfg_p, linewidth=1.0, zorder=10); handles_props.append(line_mfg_p); max_prop_val = max(max_prop_val, year_plot_data['zombie_proportion_mfg'].fillna(0).max())
                if 'zombie_proportion_nonmfg' in year_plot_data.columns and not year_plot_data['zombie_proportion_nonmfg'].isna().all(): line_nonmfg_p, = ax_props.plot(year_plot_data['year'], year_plot_data['zombie_proportion_nonmfg'], color=color_nonmfg_line, linestyle=':', marker='s', markersize=3, label=label_nonmfg_p, linewidth=1.0, zorder=10); handles_props.append(line_nonmfg_p); max_prop_val = max(max_prop_val, year_plot_data['zombie_proportion_nonmfg'].fillna(0).max())
            ax_props.set_ylim(0, max(0.05, max_prop_val * 1.15) if max_prop_val > 0 else 0.05)
            ax_props.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))

            # --- Plot Interest Rate on Secondary Y-Axis (Right) ---
            if external_data_available and 'corp_loan_rate' in year_plot_data.columns: # Check specific column
                ax_secondary = ax_props.twinx()
                color_interest = 'darkgrey'  # Use a single grey color
                zorder_secondary = 1        # Plot behind

                max_ext_val = -np.inf; min_ext_val = np.inf

                if not year_plot_data['corp_loan_rate'].isna().all():
                     label_int = "Corp. Loan Rate (New)" # Or get from PUBLICATION_NAMES
                     line_int, = ax_secondary.plot(year_plot_data['year'], year_plot_data['corp_loan_rate'],
                                                   color=color_interest, linestyle='-.', marker=None, # Dashed grey, no marker
                                                   label=label_int, linewidth=1.2, alpha=0.9, # Slightly thicker maybe
                                                   zorder=zorder_secondary)
                     if line_int: handles_secondary.append(line_int)
                     max_ext_val = max(max_ext_val, year_plot_data['corp_loan_rate'].fillna(-np.inf).max())
                     min_ext_val = min(min_ext_val, year_plot_data['corp_loan_rate'].fillna(np.inf).min())

                # Configure Secondary Axis (Right) - Use grey color
                if handles_secondary:
                     ax_secondary.set_ylabel('Interest Rate (%)', color='dimgrey') # Specific label
                     ax_secondary.tick_params(axis='y', labelcolor='dimgrey', labelsize=8)
                     y_buffer_ext = (max_ext_val - min_ext_val) * 0.10 if max_ext_val > -np.inf else 0.01 # Handle case with only one value
                     y_low_ext = min_ext_val - y_buffer_ext; y_high_ext = max_ext_val + y_buffer_ext
                     if y_high_ext - y_low_ext < 0.01: mid_point = (y_high_ext + y_low_ext) / 2; y_low_ext = mid_point - 0.005; y_high_ext = mid_point + 0.005
                     ax_secondary.set_ylim(y_low_ext, y_high_ext)
                     ax_secondary.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))
                     ax_secondary.grid(False)
                else: # If interest rate line wasn't plotted
                     if 'ax_secondary' in locals(): ax_secondary.set_visible(False)
            else:
                 print("Skipping interest rate overlay (data not available or column missing).")
            # --- End Secondary Axis Plotting ---

            # Configure Title and X-Axis Ticks
            title_props = f"Annual {PUBLICATION_NAMES.get('Zombie','Zombie')} {PUBLICATION_NAMES.get('Firm-Year','Firm-Year')} Proportion"
            if plot_by_sector: title_props += " by Sector"
            if external_data_available and handles_secondary: title_props += " vs Corp. Loan Rate" # Updated title
            #ax_props.set_title(title_props)

            years_in_summary = year_plot_data['year'].unique()
            if len(years_in_summary) <= 25: ax_props.set_xticks(years_in_summary)
            else: ax_props.xaxis.set_major_locator(mtick.MaxNLocator(integer=True, nbins=15))
            plt.setp(ax_props.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor", fontsize=8)

            # --- Combined Legend (Framed) ---
            combined_handles = handles_props + handles_secondary
            if combined_handles:
                # Filter labels corresponding to actual handles plotted
                combined_labels = [h.get_label() for h in combined_handles if h and h.get_label() and not h.get_label().startswith('_')]
                # Filter handles themselves to match labels
                combined_handles = [h for h in combined_handles if h and h.get_label() and not h.get_label().startswith('_')]

                if combined_handles: # Create legend if still have valid items
                    legend_obj_p = ax_props.legend(
                        handles=combined_handles, labels=combined_labels,
                        loc='best', fontsize=7,
                        frameon=True, facecolor='white', framealpha=0.85,
                        edgecolor='black', fancybox=False
                    )
                    legend_obj_p.get_frame().set_linewidth(plt.rcParams['axes.linewidth'])
            # --- End Combined Legend ---

            plt.tight_layout()
            # Save Figure 2
            prop_filename = f"zombie_proportion_year{'_by_sector' if plot_by_sector else ''}{'_vs_loanrate' if external_data_available else ''}_v4_{timestamp_str}.png" # Updated filename v4
            prop_save_path = os.path.join(output_dir_path, prop_filename)
            try:
                plt.savefig(prop_save_path, dpi=600, bbox_inches='tight')
                print(f"Yearly proportion plot saved: {prop_save_path}")
            except Exception as e_save_p: print(f"Error saving proportion plot {prop_save_path}: {e_save_p}")
            plt.close(fig_props)

        except Exception as e_plot_p:
            print(f"Error generating proportion plot: {e_plot_p}")
            if 'fig_props' in locals() and fig_props in plt.get_fignums(): plt.close(fig_props) # Ensure closure on error
        # --- End Figure 2 ---

    else: # Corresponds to 'if not year_plot_data.empty:'
        print("Skipping yearly zombie plot (no summary data generated).")

else: # Corresponds to 'if 'df_analysis' in locals()...'
    print("Skipping zombie analysis by year (df_analysis is empty or no valid rows).")

# --- End of PART G ---

print("\n\n=== SCRIPT FINISHED ===")
# ""End of script""
